{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tqdm_pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from nltk import word_tokenize\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras.backend as K\n",
    "from keras import initializers, regularizers\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Activation, Embedding, RNN, LSTM, LSTMCell, Dense, Dropout, Concatenate\n",
    "from keras.layers import TimeDistributed, Bidirectional, Lambda\n",
    "from keras.layers import concatenate\n",
    "from keras.layers.recurrent import Recurrent\n",
    "from keras.layers.core import Reshape\n",
    "from keras.activations import tanh, softmax\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure gpu is available\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import embedding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# pretrained GloVe embeddings unpacked\n",
    "\n",
    "file = 'glove.6B.300d.txt'\n",
    "embed_dim = 300\n",
    "\n",
    "w2idx = {}\n",
    "w2vec = {}\n",
    "\n",
    "with open(file) as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        line = [part.strip() for part in line.split()]\n",
    "        word = line[0]\n",
    "        vec = np.asarray(line[1 : embed_dim + 1], dtype='float32')\n",
    "        \n",
    "        w2idx[word] = idx\n",
    "        w2vec[word] = vec\n",
    "        \n",
    "# include empty character for padding - put last\n",
    "w2idx[''] = len(w2idx)\n",
    "w2vec[''] = np.zeros(embed_dim)\n",
    "\n",
    "# create embedding matrix\n",
    "embeddings = np.zeros((len(w2idx), embed_dim))\n",
    "\n",
    "for word, idx in w2idx.items():\n",
    "    embeddings[idx] = np.array(w2vec[word])\n",
    "    \n",
    "# save\n",
    "with open('glv_embed_matrix.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "    \n",
    "with open('glv_w2idx.pkl', 'wb') as f:\n",
    "    pickle.dump(w2idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries, pretrained embeddings\n",
    "with open('data/glv_w2idx.pkl', 'rb') as f:\n",
    "    w2idx = pickle.load(f)\n",
    "with open('data/glv_embed_matrix.pkl', 'rb') as f:\n",
    "    embedding = pickle.load(f)\n",
    "    \n",
    "# need to append BOS ('\\t') and EOS ('\\n') tokens to embeddings\n",
    "# give (consistently) random initialization since they don't actually mean anything\n",
    "# padding already exists as '' at the end of the embedding\n",
    "\n",
    "pad = len(w2idx) - 1\n",
    "\n",
    "w2idx['\\t'] = embedding.shape[0]\n",
    "np.random.seed(1)\n",
    "embedding = np.append(embedding, np.random.rand(1, 300), axis=0)\n",
    "\n",
    "w2idx['\\n'] = embedding.shape[0]\n",
    "np.random.seed(2)\n",
    "embedding = np.append(embedding, np.random.rand(1, 300), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Replacement</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do not</td>\n",
       "      <td>don't</td>\n",
       "      <td>I do not know what to say.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>will not</td>\n",
       "      <td>won't</td>\n",
       "      <td>The girl will not go to bed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>will not</td>\n",
       "      <td>won't</td>\n",
       "      <td>He will not come tomorrow night.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>would not</td>\n",
       "      <td>wouldn't</td>\n",
       "      <td>They would not want you to do that.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>can not</td>\n",
       "      <td>can't</td>\n",
       "      <td>We can not believe that this happened.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Original Replacement                                Sentence\n",
       "0     do not       don't              I do not know what to say.\n",
       "1   will not       won't            The girl will not go to bed.\n",
       "2   will not       won't        He will not come tomorrow night.\n",
       "3  would not    wouldn't     They would not want you to do that.\n",
       "4    can not       can't  We can not believe that this happened."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# placeholder dataset\n",
    "\n",
    "df = pd.DataFrame({'Sentence': [\"I do not know what to say.\", \n",
    "                                \"The girl will not go to bed.\",\n",
    "                               \"He will not come tomorrow night.\",\n",
    "                               \"They would not want you to do that.\",\n",
    "                               \"We can not believe that this happened.\",\n",
    "                               \"I could not handle the truth.\"], \n",
    "                  'Original': [\"do not\", \"will not\", \"will not\", \"would not\", \"can not\", \"could not\"],\n",
    "                  'Replacement': [\"don't\", \"won't\", \"won't\", \"wouldn't\", \"can't\", \"couldn't\"]})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Original</th>\n",
       "      <th>Replacement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Imagine that you have just written what you be...</td>\n",
       "      <td>you have</td>\n",
       "      <td>you 've</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are ready to get it off your plate and sen...</td>\n",
       "      <td>You are</td>\n",
       "      <td>You 're</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Before you hit the publish button , are you po...</td>\n",
       "      <td>you have</td>\n",
       "      <td>you 've</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After all , you have probably worked hard to c...</td>\n",
       "      <td>you have</td>\n",
       "      <td>you 've</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maybe you are already asking yourself some of ...</td>\n",
       "      <td>you are</td>\n",
       "      <td>you 're</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Original Replacement\n",
       "0  Imagine that you have just written what you be...  you have     you 've\n",
       "1  You are ready to get it off your plate and sen...   You are     You 're\n",
       "2  Before you hit the publish button , are you po...  you have     you 've\n",
       "3  After all , you have probably worked hard to c...  you have     you 've\n",
       "4  Maybe you are already asking yourself some of ...   you are     you 're"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('data/acrolinx_blog_annotated_df.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "# change from text to indices\n",
    "\n",
    "def sent_to_word_idx(df):\n",
    "    new = []\n",
    "    for idx, row in tqdm(df.iterrows(), total = df.shape[0]):\n",
    "        sent = word_tokenize(row['Sentence'])\n",
    "        # add start-of-sequence ('\\t') and end-of-sequence ('\\n') markers to all texts\n",
    "        sent = ['\\t'] + sent + ['\\n']\n",
    "        sent_indices = []\n",
    "        for word in sent:\n",
    "            word = word.lower()\n",
    "            if word in w2idx:\n",
    "                sent_indices.append(w2idx[word])\n",
    "            else:\n",
    "                sent_indices.append(pad)            \n",
    "        new.append(sent_indices)\n",
    "    df['x_word'] = new\n",
    "    return df\n",
    "\n",
    "def orig_to_place_idx(df):\n",
    "    # takes the part of the sentence to be replaced and turns it into a pair of start/end indices\n",
    "    y_start = []\n",
    "    y_end = []\n",
    "    for idx, row in tqdm(df.iterrows(), total = df.shape[0]):\n",
    "        sent = word_tokenize(row['Original'])\n",
    "        sent_indices = []\n",
    "        for word in sent:\n",
    "            word = word.lower()\n",
    "            if word in w2idx:\n",
    "                sent_indices.append(w2idx[word])\n",
    "            else:\n",
    "                sent_indices.append(pad)\n",
    "        # take indices and find the slice in the whole sentence\n",
    "        slice_length = len(sent_indices)\n",
    "        starts = [i for i, x in enumerate(row['x_word']) if x == sent_indices[0]]\n",
    "        slice_idx = np.nan\n",
    "        for potential_start in starts:\n",
    "            potential_slice = row['x_word'][potential_start : potential_start + slice_length]\n",
    "            if (potential_slice == np.array(sent_indices)).all():\n",
    "                y_start.append(potential_start)\n",
    "                y_end.append(potential_start + slice_length - 1)\n",
    "                break\n",
    "    df['y_start'] = y_start\n",
    "    df['y_end'] = y_end\n",
    "    return df\n",
    "\n",
    "def repl_to_word_idx(df):\n",
    "    # takes original & replacement texts and turns them into both decoder input and decoder output\n",
    "    # both so that teacher forcing can be done\n",
    "    y_rep = []\n",
    "    y_orig = []\n",
    "    for idx, row in tqdm(df.iterrows(), total = df.shape[0]):\n",
    "        sent = word_tokenize(row['Replacement'])\n",
    "        # add start-of-sequence ('\\t') and end-of-sequence ('\\n') markers to all texts\n",
    "        sent = ['\\t'] + sent + ['\\n']\n",
    "        sent_indices = []\n",
    "        for word in sent:\n",
    "            word = word.lower()\n",
    "            if word in w2idx:\n",
    "                sent_indices.append(w2idx[word])\n",
    "            else:\n",
    "                sent_indices.append(pad)\n",
    "        y_rep.append(sent_indices)\n",
    "    for idx, row in tqdm(df.iterrows(), total = df.shape[0]):\n",
    "        sent = word_tokenize(row['Original'])\n",
    "        # add start-of-sequence ('\\t') and end-of-sequence ('\\n') markers to all texts\n",
    "        sent = ['\\t'] + sent + ['\\n']\n",
    "        sent_indices = []\n",
    "        for word in sent:\n",
    "            word = word.lower()\n",
    "            if word in w2idx:\n",
    "                sent_indices.append(w2idx[word])\n",
    "            else:\n",
    "                sent_indices.append(pad)\n",
    "        y_orig.append(sent_indices)\n",
    "    df['y_orig'] = y_orig\n",
    "    df['y_rep'] = y_rep\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0183838bca4a6aaabb18ce251f7b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4848), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a45dde7a75f44c38247643ef5635017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4848), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-9b11a08d9c1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_to_word_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig_to_place_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepl_to_word_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-af55ce9b4f3f>\u001b[0m in \u001b[0;36morig_to_place_idx\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0my_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpotential_start\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mslice_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_start'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_end'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3368\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3369\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3370\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3372\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3444\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3445\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3446\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3629\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3630\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3631\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3632\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Length of values does not match length of index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "df = sent_to_word_idx(df)\n",
    "df = orig_to_place_idx(df)\n",
    "df = repl_to_word_idx(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data to arrays from df, add pre-padding\n",
    "\n",
    "X = pad_sequences(df['x_word'], value = pad).astype('int64')\n",
    "y_rep = pad_sequences(df['y_rep'], value = pad).astype('int64')\n",
    "y_orig = pad_sequences(df['y_orig'], value = pad).astype('int64')\n",
    "\n",
    "# set up target data from output sequence, 1 timestep off from y_rep\n",
    "#y_rep_output\n",
    "\n",
    "y_start = to_categorical(np.array(df['y_start']), num_classes = X.shape[1], dtype = 'int64')\n",
    "y_end = to_categorical(np.array(df['y_end']), num_classes = X.shape[1], dtype = 'int64')\n",
    "y_rep_cat = np.array([to_categorical(x, num_classes = embedding.shape[0]) for x in y_rep]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_units = 256 # I think 512 or 1028 is standard - lessening for memory purposes for now\n",
    "epochs = 200\n",
    "batch_size = len(X)\n",
    "learning_rate = 0.1\n",
    "\n",
    "input_len = X.shape[1]\n",
    "orig_len = y_orig.shape[1]\n",
    "repl_len = y_rep.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# input sentences in form of word indices\n",
    "main_input = Input(shape = (input_len,), dtype = 'int64', name = 'main_input')\n",
    "repl_input = Input(shape = (repl_len,), dtype = 'int64', name = 'repl_input')\n",
    "orig_input = Input(shape = (orig_len,), dtype = 'int64', name = 'orig_input')\n",
    "\n",
    "# embedding layer\n",
    "# note for later: can use mask_zero parameter in embedding layer, but would need to go back and change some indices\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    embedding_layer = Embedding(input_dim = embedding.shape[0],\n",
    "                          output_dim = embedding.shape[1],\n",
    "                          weights = [embedding],\n",
    "                          trainable = False, \n",
    "                          name = 'embedding_layer')\n",
    "\n",
    "    input_embed = embedding_layer(main_input)\n",
    "    repl_embed = embedding_layer(repl_input)\n",
    "    orig_embed = embedding_layer(orig_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function taken from https://github.com/datalogue/keras-attention/issues/15\n",
    "def _time_distributed_dense(x, w, b=None, dropout=None,\n",
    "                        input_dim=None, output_dim=None,\n",
    "                        timesteps=None, training=None):\n",
    "    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        w: weight matrix.\n",
    "        b: optional bias vector.\n",
    "        dropout: wether to apply dropout (same dropout mask\n",
    "            for every temporal slice of the input).\n",
    "        input_dim: integer; optional dimensionality of the input.\n",
    "        output_dim: integer; optional dimensionality of the output.\n",
    "        timesteps: integer; optional number of timesteps.\n",
    "        training: training phase tensor or boolean.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    if not input_dim:\n",
    "        input_dim = K.shape(x)[2]\n",
    "    if not timesteps:\n",
    "        timesteps = K.shape(x)[1]\n",
    "    if not output_dim:\n",
    "        output_dim = K.shape(w)[1]\n",
    "\n",
    "    if dropout is not None and 0. < dropout < 1.:\n",
    "        # apply the same dropout pattern at every timestep\n",
    "        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
    "        dropout_matrix = K.dropout(ones, dropout)\n",
    "        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
    "        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n",
    "\n",
    "    # collapse time dimension and batch dimension together\n",
    "    x = K.reshape(x, (-1, input_dim))\n",
    "    x = K.dot(x, w)\n",
    "    if b is not None:\n",
    "        x = K.bias_add(x, b)\n",
    "        \n",
    "    # reshape to 3D tensor\n",
    "    if K.backend() == 'tensorflow':\n",
    "        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n",
    "        x.set_shape([None, None, output_dim])\n",
    "    else:\n",
    "        x = K.reshape(x, (-1, timesteps, output_dim))\n",
    "        \n",
    "    return x\n",
    "\n",
    "# pointer network implementation\n",
    "class PointerNet(LSTM):\n",
    "    def __init__(self, units, *args, **kwargs):\n",
    "        super().__init__(units, *args, **kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # immediately set variables for later use\n",
    "        # keep same number as units as encoder LSTM by default\n",
    "        self.num_units = input_shape[2]\n",
    "        self.seq_len = input_shape[1]\n",
    "        \n",
    "        # add trainable attention weights\n",
    "        self.W1 = self.add_weight(name=\"W1\",\n",
    "                                  shape=(self.num_units, 1),\n",
    "                                  initializer=\"uniform\",\n",
    "                                  trainable=True)\n",
    "        self.W2 = self.add_weight(name=\"W2\",\n",
    "                                  shape=(self.num_units, 1),\n",
    "                                  initializer=\"uniform\",\n",
    "                                  trainable=True)\n",
    "        self.vt = self.add_weight(name=\"vt\",\n",
    "                                  shape=(self.seq_len, 1),\n",
    "                                  initializer='uniform',\n",
    "                                  trainable=True)\n",
    "        \n",
    "        super(PointerNet, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        initial_state = self.get_initial_state(x)\n",
    "                \n",
    "        pointer, _, _ = K.rnn(self.step, x, initial_state, \n",
    "                              constants = [x], input_length = self.seq_len)\n",
    "        \n",
    "        return pointer # only need 1 pointer for whole sequence, so h/c don't matter for this task\n",
    "    \n",
    "    def step(self, x_input, states):\n",
    "        # x_input = original input at current time stamp (batch_size, num_units)\n",
    "        # states = 3 tensors:\n",
    "        # states[0] = h hidden state (batch_size, num_units)\n",
    "        # states[1] = c cell state/memory (batch_size, num_units)\n",
    "        # states[2] = x next word input (batch_size, seq_len, num_units)        \n",
    "        encoded = states[2]\n",
    "        _, [h, c] = self.cell.call(x_input, states[0:2])\n",
    "        decoded = K.repeat(h, self.seq_len)\n",
    "\n",
    "        # vt*tanh(W1*e+W2*d)\n",
    "        W1_eij = _time_distributed_dense(encoded, self.W1, output_dim=1)\n",
    "        W2_dij = _time_distributed_dense(decoded, self.W2, output_dim=1)\n",
    "        U = self.vt * tanh(W1_eij + W2_dij)\n",
    "        U = K.squeeze(U, 2) # removes a 1-dimension at 2nd axis\n",
    "\n",
    "        # softmax over U to get probability distribution over input length\n",
    "        pointer = softmax(U)\n",
    "        return pointer, [h, c]\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # input shape should be (batch_size, seq_len, units)\n",
    "        # output shape should be (batch_size, seq_len)\n",
    "        return (input_shape[0], input_shape[1])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get indices\n",
    "\n",
    "lstm = LSTM(return_sequences = True, units = num_units, name = 'lstm')(input_embed)\n",
    "\n",
    "y_start_output = PointerNet(units = num_units, activation=\"softmax\", input_shape = (batch_size, input_len, num_units), name = 'y_start_output')(lstm)\n",
    "y_end_output = PointerNet(units = num_units, activation=\"softmax\", input_shape = (batch_size, input_len, num_units), name = 'y_end_output')(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### connect indices to main_input\n",
    "\n",
    "## later use this to slice and concatenate with context\n",
    "## in the meantime just assume y_orig is somehow the output\n",
    "\n",
    "#y_indices = concatenate([K.argmax(y_start_output, axis = 1), K.argmax(y_end_output, axis = 1)])\n",
    "y_start_sparse = Lambda(lambda x : K.argmax(x, axis = 1))(y_start_output)\n",
    "y_end_sparse = Lambda(lambda x : K.argmax(x, axis = 1))(y_end_output)\n",
    "y_start_reshape = Reshape((1,))(y_start_sparse)\n",
    "y_end_reshape = Reshape((1,))(y_end_sparse)\n",
    "y_indices = K.cast(concatenate([y_start_reshape, y_end_reshape]), 'int32')\n",
    "#return_input = concatenate([y_start_reshape, y_end_reshape, main_input], axis = 1)\n",
    "return_input = tf.gather_nd(input_embed, y_indices)\n",
    "\n",
    "# https://stackoverflow.com/questions/50820639/keras-how-to-slice-tensor-using-information-from-another-tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### feed encoder input (main_input), decoder input (repl_input) and sliced replacement text to enc-dec system\n",
    "\n",
    "# these should change later to some sort of context-based or conditional model\n",
    "# also with attention\n",
    "\n",
    "# decoder given 2*units to accept bidirectional outputs\n",
    "encoder = Bidirectional(LSTM(return_state = True, units = num_units), name = \"encoder\")\n",
    "decoder = LSTM(return_sequences = True, return_state = True, name = \"decoder\", units = 2 * num_units)\n",
    "\n",
    "# sequence is unnecessary for the encoder - just states, to start the decoder correctly\n",
    "# state and sequence for decoder will be necessary in inference, but not right now\n",
    "enc_output, enc_h_forward, enc_c_forward, enc_h_backward, enc_c_backward = encoder(orig_embed)\n",
    "enc_h = Concatenate()([enc_h_forward, enc_h_backward])\n",
    "enc_c = Concatenate()([enc_c_forward, enc_c_backward])\n",
    "dec_output, _, _ = decoder(repl_embed, initial_state = [enc_h, enc_c])\n",
    "\n",
    "# Dropout?\n",
    "\n",
    "dec_tdd = TimeDistributed(Dense(embedding.shape[0], activation='softmax'), \n",
    "                               name = 'y_rep_output')\n",
    "y_rep_output = dec_tdd(dec_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/300\n",
      "6/6 [==============================] - 13s 2s/step - loss: 17.6916 - y_start_output_loss: 2.3990 - y_end_output_loss: 2.3989 - y_rep_output_loss: 12.8938 - y_start_output_acc: 0.0000e+00 - y_end_output_acc: 0.0000e+00 - y_rep_output_acc: 0.0000e+00\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 17.5927 - y_start_output_loss: 2.3952 - y_end_output_loss: 2.3906 - y_rep_output_loss: 12.8069 - y_start_output_acc: 0.3333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 17.4395 - y_start_output_loss: 2.3915 - y_end_output_loss: 2.3828 - y_rep_output_loss: 12.6652 - y_start_output_acc: 0.6667 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 17.1770 - y_start_output_loss: 2.3880 - y_end_output_loss: 2.3761 - y_rep_output_loss: 12.4130 - y_start_output_acc: 0.6667 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 16.7779 - y_start_output_loss: 2.3848 - y_end_output_loss: 2.3706 - y_rep_output_loss: 12.0225 - y_start_output_acc: 0.6667 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 16.2412 - y_start_output_loss: 2.3819 - y_end_output_loss: 2.3666 - y_rep_output_loss: 11.4927 - y_start_output_acc: 0.6667 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 15.5666 - y_start_output_loss: 2.3793 - y_end_output_loss: 2.3639 - y_rep_output_loss: 10.8233 - y_start_output_acc: 0.6667 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 14.7369 - y_start_output_loss: 2.3771 - y_end_output_loss: 2.3622 - y_rep_output_loss: 9.9977 - y_start_output_acc: 0.6667 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 13.7766 - y_start_output_loss: 2.3750 - y_end_output_loss: 2.3610 - y_rep_output_loss: 9.0405 - y_start_output_acc: 0.6667 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 12.8177 - y_start_output_loss: 2.3732 - y_end_output_loss: 2.3603 - y_rep_output_loss: 8.0842 - y_start_output_acc: 0.6667 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 11.9281 - y_start_output_loss: 2.3715 - y_end_output_loss: 2.3597 - y_rep_output_loss: 7.1969 - y_start_output_acc: 0.6667 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 11.0770 - y_start_output_loss: 2.3700 - y_end_output_loss: 2.3594 - y_rep_output_loss: 6.3476 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 10.2441 - y_start_output_loss: 2.3688 - y_end_output_loss: 2.3591 - y_rep_output_loss: 5.5162 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 9.4193 - y_start_output_loss: 2.3677 - y_end_output_loss: 2.3589 - y_rep_output_loss: 4.6927 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 8.6202 - y_start_output_loss: 2.3668 - y_end_output_loss: 2.3587 - y_rep_output_loss: 3.8947 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 7.8880 - y_start_output_loss: 2.3660 - y_end_output_loss: 2.3586 - y_rep_output_loss: 3.1634 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 7.2794 - y_start_output_loss: 2.3653 - y_end_output_loss: 2.3585 - y_rep_output_loss: 2.5556 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 6.8287 - y_start_output_loss: 2.3646 - y_end_output_loss: 2.3584 - y_rep_output_loss: 2.1056 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 6.5161 - y_start_output_loss: 2.3641 - y_end_output_loss: 2.3583 - y_rep_output_loss: 1.7937 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 6.3009 - y_start_output_loss: 2.3638 - y_end_output_loss: 2.3583 - y_rep_output_loss: 1.5788 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 6.1551 - y_start_output_loss: 2.3635 - y_end_output_loss: 2.3582 - y_rep_output_loss: 1.4334 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 6.0608 - y_start_output_loss: 2.3632 - y_end_output_loss: 2.3582 - y_rep_output_loss: 1.3394 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 6.0044 - y_start_output_loss: 2.3629 - y_end_output_loss: 2.3581 - y_rep_output_loss: 1.2833 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.9746 - y_start_output_loss: 2.3627 - y_end_output_loss: 2.3581 - y_rep_output_loss: 1.2538 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.9603 - y_start_output_loss: 2.3624 - y_end_output_loss: 2.3581 - y_rep_output_loss: 1.2398 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.9521 - y_start_output_loss: 2.3620 - y_end_output_loss: 2.3581 - y_rep_output_loss: 1.2320 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.9434 - y_start_output_loss: 2.3615 - y_end_output_loss: 2.3580 - y_rep_output_loss: 1.2238 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.9302 - y_start_output_loss: 2.3608 - y_end_output_loss: 2.3580 - y_rep_output_loss: 1.2113 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.9113 - y_start_output_loss: 2.3601 - y_end_output_loss: 2.3580 - y_rep_output_loss: 1.1932 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.8871 - y_start_output_loss: 2.3593 - y_end_output_loss: 2.3580 - y_rep_output_loss: 1.1698 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.8589 - y_start_output_loss: 2.3587 - y_end_output_loss: 2.3580 - y_rep_output_loss: 1.1423 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 5.8292 - y_start_output_loss: 2.3583 - y_end_output_loss: 2.3580 - y_rep_output_loss: 1.1130 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.8001 - y_start_output_loss: 2.3580 - y_end_output_loss: 2.3580 - y_rep_output_loss: 1.0842 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.7735 - y_start_output_loss: 2.3578 - y_end_output_loss: 2.3579 - y_rep_output_loss: 1.0578 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.7508 - y_start_output_loss: 2.3576 - y_end_output_loss: 2.3579 - y_rep_output_loss: 1.0353 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.7328 - y_start_output_loss: 2.3574 - y_end_output_loss: 2.3579 - y_rep_output_loss: 1.0175 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.7195 - y_start_output_loss: 2.3573 - y_end_output_loss: 2.3579 - y_rep_output_loss: 1.0043 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.7094 - y_start_output_loss: 2.3571 - y_end_output_loss: 2.3579 - y_rep_output_loss: 0.9943 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.7500\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.7002 - y_start_output_loss: 2.3570 - y_end_output_loss: 2.3579 - y_rep_output_loss: 0.9853 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 5.6902 - y_start_output_loss: 2.3569 - y_end_output_loss: 2.3579 - y_rep_output_loss: 0.9754 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.6785 - y_start_output_loss: 2.3568 - y_end_output_loss: 2.3579 - y_rep_output_loss: 0.9638 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.6646 - y_start_output_loss: 2.3567 - y_end_output_loss: 2.3579 - y_rep_output_loss: 0.9500 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 5.6493 - y_start_output_loss: 2.3566 - y_end_output_loss: 2.3579 - y_rep_output_loss: 0.9348 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.6333 - y_start_output_loss: 2.3565 - y_end_output_loss: 2.3579 - y_rep_output_loss: 0.9189 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.6185 - y_start_output_loss: 2.3564 - y_end_output_loss: 2.3579 - y_rep_output_loss: 0.9042 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.6052 - y_start_output_loss: 2.3563 - y_end_output_loss: 2.3579 - y_rep_output_loss: 0.8909 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.5933 - y_start_output_loss: 2.3562 - y_end_output_loss: 2.3579 - y_rep_output_loss: 0.8792 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.5828 - y_start_output_loss: 2.3561 - y_end_output_loss: 2.3579 - y_rep_output_loss: 0.8688 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.5735 - y_start_output_loss: 2.3560 - y_end_output_loss: 2.3579 - y_rep_output_loss: 0.8596 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 5.5648 - y_start_output_loss: 2.3559 - y_end_output_loss: 2.3579 - y_rep_output_loss: 0.8510 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.5561 - y_start_output_loss: 2.3558 - y_end_output_loss: 2.3579 - y_rep_output_loss: 0.8425 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.5473 - y_start_output_loss: 2.3556 - y_end_output_loss: 2.3579 - y_rep_output_loss: 0.8337 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 5.5382 - y_start_output_loss: 2.3556 - y_end_output_loss: 2.3579 - y_rep_output_loss: 0.8248 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.5291 - y_start_output_loss: 2.3555 - y_end_output_loss: 2.3579 - y_rep_output_loss: 0.8157 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.5199 - y_start_output_loss: 2.3554 - y_end_output_loss: 2.3579 - y_rep_output_loss: 0.8067 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 5.5108 - y_start_output_loss: 2.3552 - y_end_output_loss: 2.3579 - y_rep_output_loss: 0.7977 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.5022 - y_start_output_loss: 2.3551 - y_end_output_loss: 2.3579 - y_rep_output_loss: 0.7892 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.4938 - y_start_output_loss: 2.3551 - y_end_output_loss: 2.3579 - y_rep_output_loss: 0.7808 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 5.4857 - y_start_output_loss: 2.3550 - y_end_output_loss: 2.3579 - y_rep_output_loss: 0.7728 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.4780 - y_start_output_loss: 2.3549 - y_end_output_loss: 2.3579 - y_rep_output_loss: 0.7653 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.4707 - y_start_output_loss: 2.3548 - y_end_output_loss: 2.3579 - y_rep_output_loss: 0.7581 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.4639 - y_start_output_loss: 2.3547 - y_end_output_loss: 2.3578 - y_rep_output_loss: 0.7513 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.4573 - y_start_output_loss: 2.3545 - y_end_output_loss: 2.3578 - y_rep_output_loss: 0.7449 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.4509 - y_start_output_loss: 2.3544 - y_end_output_loss: 2.3578 - y_rep_output_loss: 0.7387 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.4446 - y_start_output_loss: 2.3543 - y_end_output_loss: 2.3578 - y_rep_output_loss: 0.7325 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 5.4385 - y_start_output_loss: 2.3541 - y_end_output_loss: 2.3578 - y_rep_output_loss: 0.7265 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.4324 - y_start_output_loss: 2.3541 - y_end_output_loss: 2.3578 - y_rep_output_loss: 0.7206 - y_start_output_acc: 0.8333 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.4262 - y_start_output_loss: 2.3539 - y_end_output_loss: 2.3578 - y_rep_output_loss: 0.7146 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.4202 - y_start_output_loss: 2.3538 - y_end_output_loss: 2.3578 - y_rep_output_loss: 0.7087 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.4144 - y_start_output_loss: 2.3536 - y_end_output_loss: 2.3577 - y_rep_output_loss: 0.7030 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 5.4087 - y_start_output_loss: 2.3534 - y_end_output_loss: 2.3577 - y_rep_output_loss: 0.6976 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.4031 - y_start_output_loss: 2.3532 - y_end_output_loss: 2.3577 - y_rep_output_loss: 0.6922 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.3976 - y_start_output_loss: 2.3529 - y_end_output_loss: 2.3577 - y_rep_output_loss: 0.6870 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.3923 - y_start_output_loss: 2.3527 - y_end_output_loss: 2.3576 - y_rep_output_loss: 0.6821 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.3871 - y_start_output_loss: 2.3524 - y_end_output_loss: 2.3575 - y_rep_output_loss: 0.6771 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.3822 - y_start_output_loss: 2.3522 - y_end_output_loss: 2.3575 - y_rep_output_loss: 0.6725 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.3771 - y_start_output_loss: 2.3520 - y_end_output_loss: 2.3574 - y_rep_output_loss: 0.6678 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 5.3720 - y_start_output_loss: 2.3517 - y_end_output_loss: 2.3572 - y_rep_output_loss: 0.6631 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 5.3671 - y_start_output_loss: 2.3515 - y_end_output_loss: 2.3569 - y_rep_output_loss: 0.6587 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.3621 - y_start_output_loss: 2.3513 - y_end_output_loss: 2.3566 - y_rep_output_loss: 0.6543 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.3570 - y_start_output_loss: 2.3512 - y_end_output_loss: 2.3560 - y_rep_output_loss: 0.6499 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 5.3522 - y_start_output_loss: 2.3514 - y_end_output_loss: 2.3552 - y_rep_output_loss: 0.6456 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.3477 - y_start_output_loss: 2.3520 - y_end_output_loss: 2.3543 - y_rep_output_loss: 0.6414 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.3417 - y_start_output_loss: 2.3509 - y_end_output_loss: 2.3535 - y_rep_output_loss: 0.6373 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.3377 - y_start_output_loss: 2.3517 - y_end_output_loss: 2.3527 - y_rep_output_loss: 0.6332 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.3327 - y_start_output_loss: 2.3514 - y_end_output_loss: 2.3521 - y_rep_output_loss: 0.6293 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.3282 - y_start_output_loss: 2.3513 - y_end_output_loss: 2.3516 - y_rep_output_loss: 0.6252 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.3236 - y_start_output_loss: 2.3510 - y_end_output_loss: 2.3513 - y_rep_output_loss: 0.6213 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.3196 - y_start_output_loss: 2.3511 - y_end_output_loss: 2.3510 - y_rep_output_loss: 0.6175 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.3154 - y_start_output_loss: 2.3508 - y_end_output_loss: 2.3508 - y_rep_output_loss: 0.6138 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.3115 - y_start_output_loss: 2.3510 - y_end_output_loss: 2.3506 - y_rep_output_loss: 0.6098 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 5.3071 - y_start_output_loss: 2.3505 - y_end_output_loss: 2.3505 - y_rep_output_loss: 0.6061 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 5.3036 - y_start_output_loss: 2.3507 - y_end_output_loss: 2.3505 - y_rep_output_loss: 0.6024 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.2992 - y_start_output_loss: 2.3503 - y_end_output_loss: 2.3504 - y_rep_output_loss: 0.5984 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.2951 - y_start_output_loss: 2.3504 - y_end_output_loss: 2.3503 - y_rep_output_loss: 0.5944 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.2910 - y_start_output_loss: 2.3503 - y_end_output_loss: 2.3503 - y_rep_output_loss: 0.5905 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.2872 - y_start_output_loss: 2.3501 - y_end_output_loss: 2.3502 - y_rep_output_loss: 0.5869 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 5.2836 - y_start_output_loss: 2.3502 - y_end_output_loss: 2.3502 - y_rep_output_loss: 0.5833 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.2798 - y_start_output_loss: 2.3500 - y_end_output_loss: 2.3502 - y_rep_output_loss: 0.5796 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.2761 - y_start_output_loss: 2.3499 - y_end_output_loss: 2.3501 - y_rep_output_loss: 0.5761 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.2720 - y_start_output_loss: 2.3499 - y_end_output_loss: 2.3501 - y_rep_output_loss: 0.5720 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.2683 - y_start_output_loss: 2.3498 - y_end_output_loss: 2.3501 - y_rep_output_loss: 0.5684 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.2646 - y_start_output_loss: 2.3497 - y_end_output_loss: 2.3501 - y_rep_output_loss: 0.5648 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 5.2611 - y_start_output_loss: 2.3497 - y_end_output_loss: 2.3501 - y_rep_output_loss: 0.5613 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.2573 - y_start_output_loss: 2.3496 - y_end_output_loss: 2.3500 - y_rep_output_loss: 0.5577 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.2538 - y_start_output_loss: 2.3495 - y_end_output_loss: 2.3500 - y_rep_output_loss: 0.5543 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.2503 - y_start_output_loss: 2.3495 - y_end_output_loss: 2.3500 - y_rep_output_loss: 0.5507 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.2467 - y_start_output_loss: 2.3495 - y_end_output_loss: 2.3500 - y_rep_output_loss: 0.5472 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.2432 - y_start_output_loss: 2.3494 - y_end_output_loss: 2.3500 - y_rep_output_loss: 0.5438 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 5.2396 - y_start_output_loss: 2.3494 - y_end_output_loss: 2.3500 - y_rep_output_loss: 0.5403 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 5.2362 - y_start_output_loss: 2.3493 - y_end_output_loss: 2.3499 - y_rep_output_loss: 0.5369 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 5.2328 - y_start_output_loss: 2.3493 - y_end_output_loss: 2.3499 - y_rep_output_loss: 0.5335 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.2293 - y_start_output_loss: 2.3493 - y_end_output_loss: 2.3499 - y_rep_output_loss: 0.5301 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.2258 - y_start_output_loss: 2.3493 - y_end_output_loss: 2.3499 - y_rep_output_loss: 0.5266 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.2224 - y_start_output_loss: 2.3492 - y_end_output_loss: 2.3499 - y_rep_output_loss: 0.5233 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 5.2188 - y_start_output_loss: 2.3492 - y_end_output_loss: 2.3499 - y_rep_output_loss: 0.5198 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.2154 - y_start_output_loss: 2.3492 - y_end_output_loss: 2.3499 - y_rep_output_loss: 0.5164 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.2120 - y_start_output_loss: 2.3491 - y_end_output_loss: 2.3499 - y_rep_output_loss: 0.5130 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.2086 - y_start_output_loss: 2.3491 - y_end_output_loss: 2.3499 - y_rep_output_loss: 0.5096 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.2051 - y_start_output_loss: 2.3491 - y_end_output_loss: 2.3498 - y_rep_output_loss: 0.5062 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.2020 - y_start_output_loss: 2.3491 - y_end_output_loss: 2.3498 - y_rep_output_loss: 0.5031 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 5.1984 - y_start_output_loss: 2.3490 - y_end_output_loss: 2.3498 - y_rep_output_loss: 0.4995 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.1950 - y_start_output_loss: 2.3490 - y_end_output_loss: 2.3498 - y_rep_output_loss: 0.4962 - y_start_output_acc: 1.0000 - y_end_output_acc: 0.8333 - y_rep_output_acc: 0.8333\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 5.1918 - y_start_output_loss: 2.3490 - y_end_output_loss: 2.3498 - y_rep_output_loss: 0.4930 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.8333\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.1884 - y_start_output_loss: 2.3490 - y_end_output_loss: 2.3498 - y_rep_output_loss: 0.4897 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.8750\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.1849 - y_start_output_loss: 2.3490 - y_end_output_loss: 2.3498 - y_rep_output_loss: 0.4862 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.8750\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 5.1816 - y_start_output_loss: 2.3489 - y_end_output_loss: 2.3498 - y_rep_output_loss: 0.4829 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.8750\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.1782 - y_start_output_loss: 2.3489 - y_end_output_loss: 2.3497 - y_rep_output_loss: 0.4795 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.8750\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 5.1748 - y_start_output_loss: 2.3489 - y_end_output_loss: 2.3497 - y_rep_output_loss: 0.4762 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.8750\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.1715 - y_start_output_loss: 2.3489 - y_end_output_loss: 2.3497 - y_rep_output_loss: 0.4728 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.8750\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.1681 - y_start_output_loss: 2.3489 - y_end_output_loss: 2.3497 - y_rep_output_loss: 0.4695 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.8750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.1647 - y_start_output_loss: 2.3489 - y_end_output_loss: 2.3497 - y_rep_output_loss: 0.4662 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.8750\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 5.1615 - y_start_output_loss: 2.3489 - y_end_output_loss: 2.3496 - y_rep_output_loss: 0.4630 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.8750\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.1582 - y_start_output_loss: 2.3489 - y_end_output_loss: 2.3496 - y_rep_output_loss: 0.4597 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.8750\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.1549 - y_start_output_loss: 2.3489 - y_end_output_loss: 2.3496 - y_rep_output_loss: 0.4564 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.8750\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.1516 - y_start_output_loss: 2.3488 - y_end_output_loss: 2.3496 - y_rep_output_loss: 0.4532 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.8750\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.1483 - y_start_output_loss: 2.3488 - y_end_output_loss: 2.3495 - y_rep_output_loss: 0.4499 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.8750\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.1450 - y_start_output_loss: 2.3488 - y_end_output_loss: 2.3495 - y_rep_output_loss: 0.4467 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.8750\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.1418 - y_start_output_loss: 2.3488 - y_end_output_loss: 2.3495 - y_rep_output_loss: 0.4436 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.8750\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.1385 - y_start_output_loss: 2.3488 - y_end_output_loss: 2.3494 - y_rep_output_loss: 0.4403 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.8750\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.1353 - y_start_output_loss: 2.3488 - y_end_output_loss: 2.3494 - y_rep_output_loss: 0.4371 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.8750\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 5.1321 - y_start_output_loss: 2.3488 - y_end_output_loss: 2.3494 - y_rep_output_loss: 0.4340 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.8750\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 5.1290 - y_start_output_loss: 2.3488 - y_end_output_loss: 2.3493 - y_rep_output_loss: 0.4308 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.8750\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.1259 - y_start_output_loss: 2.3488 - y_end_output_loss: 2.3493 - y_rep_output_loss: 0.4278 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.8750\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.1228 - y_start_output_loss: 2.3488 - y_end_output_loss: 2.3493 - y_rep_output_loss: 0.4248 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.8750\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.1197 - y_start_output_loss: 2.3488 - y_end_output_loss: 2.3493 - y_rep_output_loss: 0.4217 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.8750\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 5.1166 - y_start_output_loss: 2.3488 - y_end_output_loss: 2.3492 - y_rep_output_loss: 0.4186 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.8750\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.1135 - y_start_output_loss: 2.3488 - y_end_output_loss: 2.3492 - y_rep_output_loss: 0.4155 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9167\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 5.1105 - y_start_output_loss: 2.3488 - y_end_output_loss: 2.3492 - y_rep_output_loss: 0.4125 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9167\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.1074 - y_start_output_loss: 2.3488 - y_end_output_loss: 2.3492 - y_rep_output_loss: 0.4095 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9167\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.1043 - y_start_output_loss: 2.3487 - y_end_output_loss: 2.3491 - y_rep_output_loss: 0.4064 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9167\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.1013 - y_start_output_loss: 2.3487 - y_end_output_loss: 2.3491 - y_rep_output_loss: 0.4034 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9167\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.0983 - y_start_output_loss: 2.3487 - y_end_output_loss: 2.3491 - y_rep_output_loss: 0.4005 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9167\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.0953 - y_start_output_loss: 2.3487 - y_end_output_loss: 2.3491 - y_rep_output_loss: 0.3975 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9167\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 5.0923 - y_start_output_loss: 2.3487 - y_end_output_loss: 2.3491 - y_rep_output_loss: 0.3945 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9167\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 5.0893 - y_start_output_loss: 2.3487 - y_end_output_loss: 2.3491 - y_rep_output_loss: 0.3916 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9167\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.0865 - y_start_output_loss: 2.3487 - y_end_output_loss: 2.3490 - y_rep_output_loss: 0.3888 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9167\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.0836 - y_start_output_loss: 2.3487 - y_end_output_loss: 2.3490 - y_rep_output_loss: 0.3859 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9167\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 5.0807 - y_start_output_loss: 2.3487 - y_end_output_loss: 2.3490 - y_rep_output_loss: 0.3830 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9167\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 5.0779 - y_start_output_loss: 2.3487 - y_end_output_loss: 2.3490 - y_rep_output_loss: 0.3802 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9167\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.0751 - y_start_output_loss: 2.3487 - y_end_output_loss: 2.3490 - y_rep_output_loss: 0.3774 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9167\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.0723 - y_start_output_loss: 2.3487 - y_end_output_loss: 2.3490 - y_rep_output_loss: 0.3747 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9583\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.0695 - y_start_output_loss: 2.3487 - y_end_output_loss: 2.3490 - y_rep_output_loss: 0.3718 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9583\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 5.0667 - y_start_output_loss: 2.3487 - y_end_output_loss: 2.3490 - y_rep_output_loss: 0.3690 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 5.0638 - y_start_output_loss: 2.3487 - y_end_output_loss: 2.3490 - y_rep_output_loss: 0.3662 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9583\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.0610 - y_start_output_loss: 2.3486 - y_end_output_loss: 2.3489 - y_rep_output_loss: 0.3634 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9583\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 5.0580 - y_start_output_loss: 2.3486 - y_end_output_loss: 2.3489 - y_rep_output_loss: 0.3605 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9583\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.0551 - y_start_output_loss: 2.3486 - y_end_output_loss: 2.3489 - y_rep_output_loss: 0.3576 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9583\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.0523 - y_start_output_loss: 2.3486 - y_end_output_loss: 2.3489 - y_rep_output_loss: 0.3548 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9583\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.0495 - y_start_output_loss: 2.3486 - y_end_output_loss: 2.3489 - y_rep_output_loss: 0.3519 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9583\n",
      "Epoch 171/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.0467 - y_start_output_loss: 2.3486 - y_end_output_loss: 2.3489 - y_rep_output_loss: 0.3492 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9583\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.0440 - y_start_output_loss: 2.3486 - y_end_output_loss: 2.3489 - y_rep_output_loss: 0.3464 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9583\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.0412 - y_start_output_loss: 2.3486 - y_end_output_loss: 2.3489 - y_rep_output_loss: 0.3437 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9583\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.0385 - y_start_output_loss: 2.3486 - y_end_output_loss: 2.3489 - y_rep_output_loss: 0.3410 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9583\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.0358 - y_start_output_loss: 2.3486 - y_end_output_loss: 2.3489 - y_rep_output_loss: 0.3384 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9583\n",
      "Epoch 176/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.0332 - y_start_output_loss: 2.3486 - y_end_output_loss: 2.3489 - y_rep_output_loss: 0.3357 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9583\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 5.0305 - y_start_output_loss: 2.3486 - y_end_output_loss: 2.3489 - y_rep_output_loss: 0.3331 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9583\n",
      "Epoch 178/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 5.0279 - y_start_output_loss: 2.3486 - y_end_output_loss: 2.3489 - y_rep_output_loss: 0.3304 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9583\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.0253 - y_start_output_loss: 2.3486 - y_end_output_loss: 2.3489 - y_rep_output_loss: 0.3278 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9583\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.0227 - y_start_output_loss: 2.3486 - y_end_output_loss: 2.3489 - y_rep_output_loss: 0.3253 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9583\n",
      "Epoch 181/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.0201 - y_start_output_loss: 2.3486 - y_end_output_loss: 2.3489 - y_rep_output_loss: 0.3227 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9583\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.0176 - y_start_output_loss: 2.3486 - y_end_output_loss: 2.3489 - y_rep_output_loss: 0.3202 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9583\n",
      "Epoch 183/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.0151 - y_start_output_loss: 2.3486 - y_end_output_loss: 2.3489 - y_rep_output_loss: 0.3177 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9583\n",
      "Epoch 184/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.0126 - y_start_output_loss: 2.3486 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.3152 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9583\n",
      "Epoch 185/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.0101 - y_start_output_loss: 2.3486 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.3127 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9583\n",
      "Epoch 186/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 5.0077 - y_start_output_loss: 2.3485 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.3103 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9583\n",
      "Epoch 187/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 5.0053 - y_start_output_loss: 2.3485 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.3079 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 0.9583\n",
      "Epoch 188/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.0029 - y_start_output_loss: 2.3485 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.3056 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 189/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.0006 - y_start_output_loss: 2.3485 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.3032 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 190/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 4.9983 - y_start_output_loss: 2.3485 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.3009 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 191/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 4.9960 - y_start_output_loss: 2.3485 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2986 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 192/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 4.9937 - y_start_output_loss: 2.3485 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2963 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 193/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.9914 - y_start_output_loss: 2.3485 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2940 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 194/300\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 4.9891 - y_start_output_loss: 2.3485 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2918 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 195/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 4.9869 - y_start_output_loss: 2.3485 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2896 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 196/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 4.9847 - y_start_output_loss: 2.3485 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2873 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 197/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 4.9824 - y_start_output_loss: 2.3485 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2851 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 4.9803 - y_start_output_loss: 2.3485 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2829 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 199/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 4.9781 - y_start_output_loss: 2.3485 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2808 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 200/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.9759 - y_start_output_loss: 2.3485 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2787 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 201/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 4.9738 - y_start_output_loss: 2.3485 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2766 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 202/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 4.9717 - y_start_output_loss: 2.3485 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2745 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 203/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 4.9696 - y_start_output_loss: 2.3485 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2724 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 204/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.9676 - y_start_output_loss: 2.3485 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2703 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 205/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.9656 - y_start_output_loss: 2.3485 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2683 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 206/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 4.9635 - y_start_output_loss: 2.3485 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2663 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 207/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 4.9615 - y_start_output_loss: 2.3485 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2643 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 208/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 4.9596 - y_start_output_loss: 2.3485 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2623 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 209/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 4.9576 - y_start_output_loss: 2.3485 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2604 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 210/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.9556 - y_start_output_loss: 2.3485 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2584 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 211/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.9537 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2565 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 212/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 4.9518 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2546 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 213/300\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 4.9500 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2527 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 214/300\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 4.9481 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2509 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 215/300\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 4.9463 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2491 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 216/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 4.9445 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2473 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 217/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 4.9427 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2455 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 218/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.9410 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2438 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 219/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 4.9392 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2420 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 220/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 4.9375 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2403 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 221/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.9358 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2387 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 222/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 4.9342 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2370 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 223/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.9325 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2353 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 224/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 4.9309 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2337 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 225/300\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 4.9293 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2321 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 226/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.9278 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2306 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 227/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 4.9261 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2290 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 228/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 4.9246 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2274 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 229/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 4.9231 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2259 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 230/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 4.9216 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3488 - y_rep_output_loss: 0.2244 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 231/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 4.9201 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.2230 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 232/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 4.9186 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.2215 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 233/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 4.9172 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.2200 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 234/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.9157 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.2186 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 235/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 4.9143 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.2172 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 236/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 4.9129 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.2158 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 237/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 4.9115 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.2144 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 238/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.9101 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.2130 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 239/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.9088 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.2117 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 240/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.9074 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.2103 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 241/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 4.9061 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.2090 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 242/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 4.9048 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.2077 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 243/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 4.9035 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.2064 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 244/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.9022 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.2051 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 245/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 4.9009 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.2038 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 246/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.8996 - y_start_output_loss: 2.3484 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.2025 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 247/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 4.8984 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.2013 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 248/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 4.8972 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.2001 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 249/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.8960 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1989 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 250/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 4.8948 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1977 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 251/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.8936 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1965 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 252/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.8924 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1954 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 253/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 4.8913 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1942 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 254/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 4.8901 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1931 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 255/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 4.8890 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1920 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 256/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 4.8879 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1909 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 257/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.8868 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1898 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 258/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 4.8857 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1887 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 259/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 4.8846 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1876 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 260/300\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 4.8836 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1865 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 261/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 4.8825 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1855 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 262/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 4.8815 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1844 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 263/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 4.8804 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1834 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 264/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 4.8794 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1824 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 265/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 4.8784 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1814 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 266/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 4.8774 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1804 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 267/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.8764 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1794 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 268/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 4.8754 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1784 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 269/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.8744 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1774 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 270/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 4.8735 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1765 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 271/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 4.8725 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1755 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 272/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.8716 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1746 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 273/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.8707 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1737 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 274/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.8697 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1727 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 275/300\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 4.8688 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1718 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 276/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 4.8679 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1709 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 277/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.8670 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1700 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 278/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.8661 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1691 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 279/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 4.8652 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1683 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 280/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 4.8644 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1674 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 281/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 4.8635 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1665 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 282/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 4.8627 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1657 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 283/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 4.8618 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1648 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 284/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 4.8610 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1640 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 285/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.8601 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1632 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 286/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 4.8593 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1624 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 287/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.8585 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1616 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 288/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 4.8577 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1607 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 289/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.8569 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1599 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 290/300\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 4.8561 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1592 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 291/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 4.8553 - y_start_output_loss: 2.3483 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1584 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 292/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 4.8545 - y_start_output_loss: 2.3482 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1576 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 293/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 4.8538 - y_start_output_loss: 2.3482 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1568 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 294/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.8530 - y_start_output_loss: 2.3482 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1561 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 295/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.8522 - y_start_output_loss: 2.3482 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1553 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 296/300\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 4.8515 - y_start_output_loss: 2.3482 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1545 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 4.8507 - y_start_output_loss: 2.3482 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1538 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 298/300\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 4.8500 - y_start_output_loss: 2.3482 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1531 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 299/300\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 4.8493 - y_start_output_loss: 2.3482 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1523 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n",
      "Epoch 300/300\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 4.8485 - y_start_output_loss: 2.3482 - y_end_output_loss: 2.3487 - y_rep_output_loss: 0.1516 - y_start_output_acc: 1.0000 - y_end_output_acc: 1.0000 - y_rep_output_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#### define & train model\n",
    "\n",
    "# other parameters\n",
    "# https://keras.io/examples/lstm_seq2seq/\n",
    "\n",
    "model = Model(inputs = [main_input, orig_input, repl_input], outputs = [y_start_output, y_end_output, y_rep_output])\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit([X, y_orig, y_rep], [y_start, y_end, y_rep_cat], epochs = epochs, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repl_input (InputLayer)         (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "orig_input (InputLayer)         (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer (Embedding)     multiple             120000900   main_input[0][0]                 \n",
      "                                                                 repl_input[0][0]                 \n",
      "                                                                 orig_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Bidirectional)         [(None, 512), (None, 1140736     embedding_layer[2][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512)          0           encoder[0][1]                    \n",
      "                                                                 encoder[0][3]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 512)          0           encoder[0][2]                    \n",
      "                                                                 encoder[0][4]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 11, 256)      570368      embedding_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder (LSTM)                  [(None, 4, 512), (No 1665024     embedding_layer[1][0]            \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "y_start_output (PointerNet)     (None, 11)           525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "y_end_output (PointerNet)       (None, 11)           525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "y_rep_output (TimeDistributed)  (None, 4, 400003)    205201539   decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 329,629,191\n",
      "Trainable params: 209,628,291\n",
      "Non-trainable params: 120,000,900\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fbd875e1710>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['y_start_output_acc'], label='y_start accuracy')\n",
    "plt.plot(history.history['y_end_output_acc'], label='y_end accuracy')\n",
    "plt.plot(history.history['y_rep_output_acc'], label='y_rep accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fbd87556710>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='overall train loss')\n",
    "plt.plot(history.history['y_start_output_loss'], label='y_start loss')\n",
    "plt.plot(history.history['y_end_output_loss'], label='y_end loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_dec_h_input = Input(shape=(num_units * 2,))\n",
    "inf_dec_c_input = Input(shape=(num_units * 2,))\n",
    "inf_dec_states_input = [inf_dec_h_input, inf_dec_c_input]\n",
    "\n",
    "inf_dec_main, inf_dec_h, inf_dec_c = decoder(repl_embed, initial_state = inf_dec_states_input)\n",
    "inf_dec_states = [inf_dec_h, inf_dec_c]\n",
    "inf_dec_output = dec_tdd(inf_dec_main)\n",
    "\n",
    "inf_enc_model = Model(inputs = [orig_input], outputs = [enc_h, enc_c])\n",
    "inf_dec_model = Model([repl_input] + inf_dec_states_input, [inf_dec_output] + inf_dec_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "        if verbose:\n",
    "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
    "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
    "            if verbose:\n",
    "                print('wa.s>',W_a_dot_s.shape)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "            if verbose:\n",
    "                print('Ua.h>',U_a_dot_h.shape)\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
    "            if verbose:\n",
    "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            if verbose:\n",
    "                print('ei>', e_i.shape)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print('ci>', c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        def create_inital_state(inputs, hidden_size):\n",
    "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
    "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
    "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
    "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
    "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
    "            return fake_state\n",
    "\n",
    "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
    "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        \"\"\" Computing energy outputs \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step, decoder_out_seq, [fake_state_e],\n",
    "        )\n",
    "\n",
    "        \"\"\" Computing context vectors \"\"\"\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step, e_outputs, [fake_state_c],\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_nmt(hidden_size, batch_size, en_timesteps, en_vsize, fr_timesteps, fr_vsize):\n",
    "    \"\"\" Defining a NMT model \"\"\"\n",
    "\n",
    "    # Define an input sequence and process it.\n",
    "    if batch_size:\n",
    "        encoder_inputs = Input(batch_shape=(batch_size, en_timesteps, en_vsize), name='encoder_inputs')\n",
    "        decoder_inputs = Input(batch_shape=(batch_size, fr_timesteps - 1, fr_vsize), name='decoder_inputs')\n",
    "    else:\n",
    "        encoder_inputs = Input(shape=(en_timesteps, en_vsize), name='encoder_inputs')\n",
    "        decoder_inputs = Input(shape=(fr_timesteps - 1, fr_vsize), name='decoder_inputs')\n",
    "\n",
    "    # Encoder GRU\n",
    "    encoder_gru = Bidirectional(GRU(hidden_size, return_sequences=True, return_state=True, name='encoder_gru'), name='bidirectional_encoder')\n",
    "    encoder_out, encoder_fwd_state, encoder_back_state = encoder_gru(encoder_inputs)\n",
    "\n",
    "    # Set up the decoder GRU, using `encoder_states` as initial state.\n",
    "    decoder_gru = Bidirectional(GRU(hidden_size, return_sequences=True, return_state=True, name='decoder_gru'), name='bidirectional_decoder')\n",
    "    decoder_out, decoder_fwd_state, decoder_back_state = decoder_gru(decoder_inputs, initial_state=[encoder_fwd_state, encoder_back_state])\n",
    "\n",
    "    # Attention layer\n",
    "    attn_layer = AttentionLayer(name='attention_layer')\n",
    "    attn_out, attn_states = attn_layer([encoder_out, decoder_out])\n",
    "\n",
    "    # Concat attention input and decoder GRU output\n",
    "    decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_out, attn_out])\n",
    "\n",
    "    # Dense layer\n",
    "    dense = Dense(fr_vsize, activation='softmax', name='softmax_layer')\n",
    "    dense_time = TimeDistributed(dense, name='time_distributed_layer')\n",
    "    decoder_pred = dense_time(decoder_concat_input)\n",
    "\n",
    "    # Full model\n",
    "    full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
    "    full_model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "    full_model.summary()\n",
    "\n",
    "    \"\"\" Inference model \"\"\"\n",
    "    batch_size = 1\n",
    "\n",
    "    \"\"\" Encoder (Inference) model \"\"\"\n",
    "    encoder_inf_inputs = Input(batch_shape=(batch_size, en_timesteps, en_vsize), name='encoder_inf_inputs')\n",
    "    encoder_inf_out, encoder_inf_fwd_state, encoder_inf_back_state = encoder_gru(encoder_inf_inputs)\n",
    "    encoder_model = Model(inputs=encoder_inf_inputs, outputs=[encoder_inf_out, encoder_inf_fwd_state, encoder_inf_back_state])\n",
    "\n",
    "    \"\"\" Decoder (Inference) model \"\"\"\n",
    "    decoder_inf_inputs = Input(batch_shape=(batch_size, 1, fr_vsize), name='decoder_word_inputs')\n",
    "    encoder_inf_states = Input(batch_shape=(batch_size, en_timesteps, 2*hidden_size), name='encoder_inf_states')\n",
    "    decoder_init_fwd_state = Input(batch_shape=(batch_size, hidden_size), name='decoder_fwd_init')\n",
    "    decoder_init_back_state = Input(batch_shape=(batch_size, hidden_size), name='decoder_back_init')\n",
    "\n",
    "    decoder_inf_out, decoder_inf_fwd_state, decoder_inf_back_state = decoder_gru(decoder_inf_inputs, initial_state=[decoder_init_fwd_state, decoder_init_back_state])\n",
    "    attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n",
    "    decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_inf_out, attn_inf_out])\n",
    "    decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n",
    "    decoder_model = Model(inputs=[encoder_inf_states, decoder_init_fwd_state, decoder_init_back_state, decoder_inf_inputs],\n",
    "                          outputs=[decoder_inf_pred, attn_inf_states, decoder_inf_fwd_state, decoder_inf_back_state])\n",
    "\n",
    "    return full_model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/oarriaga/7ac353a70fd68f953514f4d404c203ae\n",
    "# https://machinelearningmastery.com/encoder-decoder-attention-sequence-to-sequence-prediction-keras/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
