{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import random as rand\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brown dataset: vectorization using doc2vec"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "brown_disc_df = pd.read_pickle('data/discourse_markers/brown_disc_df.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_tokens = []\n",
    "for idx, row in brown_disc_df.iterrows():\n",
    "    if type(row['clean']) == float:\n",
    "        X_tokens.append(row['sent'])\n",
    "    else:\n",
    "        X_tokens.append(row['clean'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tagged_data = [TaggedDocument(words = sent, tags=[str(i)]) for i, sent in enumerate(X_tokens)]\n",
    "# should I have lowered? decided not to for now"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "d2v_brown = Doc2Vec(vector_size = 50, min_count = 1, dm = 1)\n",
    "d2v_brown.build_vocab(tagged_data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "d2v_brown.train(tagged_data, total_examples = d2v_brown.corpus_count, epochs = 20)\n",
    "print('training finished')\n",
    "d2v_brown.save(\"data/discourse_markers/d2v_brown.model\")\n",
    "print(\"trained & saved\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "features = brown_disc_df.columns[1:-1]\n",
    "num_features = len(features)\n",
    "num_samples = len(brown_disc_df)\n",
    "y = np.zeros([num_samples, num_features])\n",
    "for idx in range(len(features)):\n",
    "    y[:, idx] = brown_disc_df[features[idx]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X = np.zeros([num_samples, 50]) # 50 is vector_size\n",
    "for i in range(num_samples):\n",
    "    X[i] = d2v_brown.docvecs[str(i)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('data/discourse_markers/X_50dim_d2v.pkl', 'wb') as f:\n",
    "    pickle.dump(X, f)\n",
    "with open('data/discourse_markers/y.pkl', 'wb') as f:\n",
    "    pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/discourse_markers/X_50dim_d2v.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "with open('data/discourse_markers/y.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of non-false items\n",
    "num_true = 4000 #sum([sum(x) != 0 for x in y])\n",
    "\n",
    "balanced_X = np.zeros([num_true * 2, X.shape[1]])\n",
    "balanced_y = np.zeros([num_true * 2, y.shape[1]])\n",
    "\n",
    "tr_idx = 0\n",
    "fa_idx = 0\n",
    "overall_idx = 0\n",
    "\n",
    "for idx in range(len(X)):\n",
    "    if sum(y[idx]) > 1:\n",
    "        continue\n",
    "    if tr_idx < num_true and sum(y[idx]) != 0:\n",
    "        balanced_X[overall_idx, :] = X[idx]\n",
    "        balanced_y[overall_idx, :] = y[idx]\n",
    "        tr_idx += 1\n",
    "        overall_idx += 1\n",
    "    if fa_idx < num_true and sum(y[idx]) == 0:\n",
    "        balanced_X[overall_idx, :] = X[idx]\n",
    "        balanced_y[overall_idx, :] = y[idx]\n",
    "        fa_idx += 1\n",
    "        overall_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(balanced_X, balanced_y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_units = 128\n",
    "vector_size = X.shape[1] # 50\n",
    "num_outputs = y.shape[1] # 59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_layer = Input(shape = (vector_size,), dtype = 'float32', name = 'input_layer')\n",
    "dense_1 = Dense(num_units, activation = 'relu', name = 'dense_1')(input_layer)\n",
    "dense_2 = Dense(num_units, activation = 'relu', name = 'dense_2')(dense_1)\n",
    "output = Dense(num_outputs, activation = 'softmax', name = 'output_layer')(dense_2)\n",
    "\n",
    "#output = Dense(num_outputs, activation = 'sigmoid', name = 'output_layer')(dense_2)\n",
    "\n",
    "#out_and = Lambda(lambda x: x[..., 0:1], name = 'and')(output)\n",
    "#out_so = Lambda(lambda x: x[..., 1:2], name = 'so')(output)\n",
    "\n",
    "#model = Model(inputs = input_layer, outputs = [out_and, out_so])\n",
    "#model.compile(optimizer = 'adam',\n",
    "#              loss = ['binary_crossentropy', 'binary_crossentropy'],\n",
    "#              metrics = ['accuracy'])\n",
    "\n",
    "model = Model(inputs = input_layer, outputs = output)\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-9abe38712095>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m c_weights.append(class_weight.compute_class_weight('balanced_and'\n\u001b[1;32m      3\u001b[0m                                                \u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                                ,y_train[..., 0:1]))\n\u001b[0m\u001b[1;32m      5\u001b[0m c_weights.append(class_weight.compute_class_weight('balanced_and'\n\u001b[1;32m      6\u001b[0m                                                \u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/utils/class_weight.py\u001b[0m in \u001b[0;36mcompute_class_weight\u001b[0;34m(class_weight, classes, y)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         raise ValueError(\"classes should include all valid labels that can \"\n\u001b[1;32m     42\u001b[0m                          \"be in y\")\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "c_weights = []\n",
    "c_weights.append(class_weight.compute_class_weight('balanced_and'\n",
    "                                               ,np.unique(y_train[..., 0:1])\n",
    "                                               ,y_train[..., 0:1]))\n",
    "c_weights.append(class_weight.compute_class_weight('balanced_and'\n",
    "                                               ,np.unique(y_train[..., 1:2])\n",
    "                                               ,y_train[..., 1:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5040 samples, validate on 2160 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 1.4258 - acc: 0.2286 - val_loss: 1.2097 - val_acc: 0.5593\n",
      "Epoch 2/20\n",
      " - 1s - loss: 1.2472 - acc: 0.2595 - val_loss: 1.1816 - val_acc: 0.3009\n",
      "Epoch 3/20\n",
      " - 1s - loss: 1.2262 - acc: 0.3036 - val_loss: 1.1789 - val_acc: 0.1699\n",
      "Epoch 4/20\n",
      " - 1s - loss: 1.2056 - acc: 0.2750 - val_loss: 1.1677 - val_acc: 0.3671\n",
      "Epoch 5/20\n",
      " - 1s - loss: 1.1831 - acc: 0.3014 - val_loss: 1.1562 - val_acc: 0.3639\n",
      "Epoch 6/20\n",
      " - 1s - loss: 1.1614 - acc: 0.3131 - val_loss: 1.1540 - val_acc: 0.3620\n",
      "Epoch 7/20\n",
      " - 1s - loss: 1.1372 - acc: 0.3101 - val_loss: 1.1588 - val_acc: 0.3046\n",
      "Epoch 8/20\n",
      " - 1s - loss: 1.1129 - acc: 0.3185 - val_loss: 1.1755 - val_acc: 0.5319\n",
      "Epoch 9/20\n",
      " - 1s - loss: 1.0965 - acc: 0.3288 - val_loss: 1.1624 - val_acc: 0.2412\n",
      "Epoch 10/20\n",
      " - 1s - loss: 1.0711 - acc: 0.3145 - val_loss: 1.1672 - val_acc: 0.3986\n",
      "Epoch 11/20\n",
      " - 1s - loss: 1.0499 - acc: 0.3387 - val_loss: 1.1757 - val_acc: 0.2412\n",
      "Epoch 12/20\n",
      " - 1s - loss: 1.0319 - acc: 0.3329 - val_loss: 1.1802 - val_acc: 0.2500\n",
      "Epoch 13/20\n",
      " - 1s - loss: 1.0098 - acc: 0.3317 - val_loss: 1.1851 - val_acc: 0.2704\n",
      "Epoch 14/20\n",
      " - 1s - loss: 0.9891 - acc: 0.3353 - val_loss: 1.1939 - val_acc: 0.3690\n",
      "Epoch 15/20\n",
      " - 1s - loss: 0.9665 - acc: 0.3575 - val_loss: 1.2123 - val_acc: 0.3134\n",
      "Epoch 16/20\n",
      " - 1s - loss: 0.9514 - acc: 0.3567 - val_loss: 1.2247 - val_acc: 0.3912\n",
      "Epoch 17/20\n",
      " - 1s - loss: 0.9305 - acc: 0.3663 - val_loss: 1.2157 - val_acc: 0.3125\n",
      "Epoch 18/20\n",
      " - 1s - loss: 0.9077 - acc: 0.3736 - val_loss: 1.2392 - val_acc: 0.3750\n",
      "Epoch 19/20\n",
      " - 1s - loss: 0.8923 - acc: 0.3800 - val_loss: 1.2402 - val_acc: 0.3106\n",
      "Epoch 20/20\n",
      " - 1s - loss: 0.8710 - acc: 0.3808 - val_loss: 1.2513 - val_acc: 0.2926\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, \n",
    "                    validation_split = 0.3, epochs = 20, batch_size = 32, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.346786060333252, 0.29]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent\n",
      "And\n",
      "So\n",
      "But\n",
      "Plus\n",
      "Now\n",
      "Or\n",
      "After all\n",
      "Sure\n",
      "Of course\n",
      "By the way\n",
      "Yet\n",
      "Okay\n",
      "Ok\n",
      "Well\n",
      "Again\n",
      "Since then\n",
      "Basically\n",
      "As such\n",
      "Oh\n",
      "Not only that\n",
      "As a result\n",
      "Anywho\n",
      "Anyhow\n",
      "Anyway\n",
      "Ah\n",
      "So there you have it\n",
      "Finally\n",
      "For starters\n",
      "At least\n",
      "Seriously\n",
      "In fact\n",
      "I'd argue\n",
      "You know\n",
      "You see\n",
      "I mean\n",
      "If so\n",
      "Beyond that\n",
      "Alas\n",
      "That way\n",
      "All told\n",
      "Yes\n",
      "Hey\n",
      "That's ok\n",
      "That's okay\n",
      "That said\n",
      "With that said\n",
      "Uh\n",
      "Admittedly\n",
      "Unfortunately\n",
      "Fortunately\n",
      "First\n",
      "Recently\n",
      "Here goes...\n",
      "Heck\n",
      "In short\n",
      "First of all\n",
      "That is\n",
      "For example\n",
      "Also\n",
      "clean\n"
     ]
    }
   ],
   "source": [
    "for x in brown_disc_df:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
