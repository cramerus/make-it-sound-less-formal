{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils import shuffle\n",
    "import random as rand\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda, Dropout, Bidirectional, LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train doc2vec encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/discourse_markers/oanc_df.zip')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# remove unwanted nans\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    for item in row['clean_and_tokenized']:\n",
    "        if type(item) == float:\n",
    "            df = df.drop([idx])\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f83defa86eb41d496839ccf1f257894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=65101), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_tokens = []\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    for item in row['clean_and_tokenized']:\n",
    "        X_tokens.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50e5d97ecf549549e56b98baccba5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=338890), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tagged = []\n",
    "for i, sent in enumerate(tqdm(X_tokens)):\n",
    "    tagged.append(TaggedDocument(words = sent, tags = [str(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary built\n"
     ]
    }
   ],
   "source": [
    "d2v_oanc = Doc2Vec(vector_size = 50, min_count = 1, dm = 1)\n",
    "d2v_oanc.build_vocab(tagged)\n",
    "print('vocabulary built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training finished\n",
      "trained & saved\n"
     ]
    }
   ],
   "source": [
    "d2v_oanc.train(tagged, total_examples = d2v_oanc.corpus_count, epochs = 20)\n",
    "print('training finished')\n",
    "d2v_oanc.save(\"data/discourse_markers/d2v_oanc.model\")\n",
    "print(\"trained & saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorize texts and add to new df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e25a8276ef4837822ab1c5f1d4ee47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=65101), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vecs = []\n",
    "\n",
    "index = 0\n",
    "for idx, row in tqdm(df.iterrows(), total = len(df)):\n",
    "    current_vecs = []\n",
    "    for item in row['clean_and_tokenized']:\n",
    "        assert tagged[index].words == item\n",
    "        current_vecs.append(d2v_oanc.docvecs[str(index)])\n",
    "        index += 1\n",
    "    vecs.append(current_vecs)\n",
    "        \n",
    "df['X'] = vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['sents', 'text'])\n",
    "df = df.rename(columns={\"vectors\": \"y\"})\n",
    "df.to_pickle('data/discourse_markers/vectorized_oanc_df.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract X and y, prepare balancing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.read_pickle('data/discourse_markers/vectorized_oanc_df.zip')\n",
    "with open('data/discourse_markers/oanc_terms.pkl', 'rb') as f:\n",
    "    terms_dict = pickle.load(f)\n",
    "ind_dict = {v: k for k, v in terms_dict.items()}\n",
    "ind_dict[9] = 'NULL'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for idx, row in tqdm(df.iterrows(), total = len(df)):\n",
    "    # realized that i'd put the null vector as [0] * 10 instead of [0]*9,[1]\n",
    "    # need to fix that here\n",
    "    new_y = []\n",
    "    for item in row['y']:\n",
    "        if item == [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]:\n",
    "            new_y.append([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
    "        else:\n",
    "            new_y.append(item)\n",
    "    df.at[idx, 'y'] = new_y\n",
    "\n",
    "df.to_pickle('data/discourse_markers/vectorized_oanc_df.zip')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# check distribution of classes\n",
    "distribution = [0] * 10\n",
    "num_samples = 0\n",
    "class_weights = {}\n",
    "count = Counter()\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total = len(df)):    \n",
    "    # to make things a bit more balanced, skip if all sentences are in NULL class\n",
    "    if all(item == [0, 0, 0, 0, 0, 0, 0, 0, 0, 1] for item in row['y']):\n",
    "        continue\n",
    "        \n",
    "    # cut out super long paragraphs first\n",
    "    if len(row['y']) > 8:\n",
    "        continue\n",
    "    count[len(row['y'])] += 1\n",
    "        \n",
    "    row_sum = [sum(i) for i in zip(*row['y'])]\n",
    "    distribution = [x + y for x, y in zip(distribution, row_sum)]\n",
    "    \n",
    "    num_samples += 1\n",
    "\n",
    "print(distribution)\n",
    "total = sum(distribution)\n",
    "print('number of samples\\t' + str(num_samples))\n",
    "print('number of subsamples\\t' + str(total))\n",
    "print()\n",
    "\n",
    "for idx in range(len(distribution)):\n",
    "    print(ind_dict[idx] + '\\t' + \"{0:.0%}\".format(distribution[idx]/float(total)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def create_class_weight(dist, mu=0.15):\n",
    "    total = sum(dist)\n",
    "    labels = range(len(dist))\n",
    "    class_weight = {}\n",
    "\n",
    "    for label in labels:\n",
    "        #score = math.log(mu*total/float(labels_dict[key]))\n",
    "        class_weight[label] = total / dist[label]\n",
    "\n",
    "    return class_weight\n",
    "\n",
    "class_weights = create_class_weight(distribution)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total = len(df)):    \n",
    "    # to make things a bit more balanced, skip if all sentences are in NULL class\n",
    "    if all(item == [0, 0, 0, 0, 0, 0, 0, 0, 0, 1] for item in row['y']):\n",
    "        continue\n",
    "        \n",
    "    # cut out super long paragraphs\n",
    "    if len(row['y']) > 8:\n",
    "        continue\n",
    "        \n",
    "    assert len(row['y']) == len(row['X'])\n",
    "    \n",
    "    X.append(row['X'])\n",
    "    y.append(row['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## padding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.random.seed = 47\n",
    "X_pad = np.random.rand(50)\n",
    "\n",
    "X = pad_sequences(X, maxlen=8, dtype='float32', padding='post', truncating='post', value=X_pad)\n",
    "print(X.shape)\n",
    "y = pad_sequences(y, maxlen=8, dtype='int32', padding='post', value = [0]*10)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('data/discourse_markers/oanc_X.pkl', 'wb') as f:\n",
    "    pickle.dump(X, f)\n",
    "with open('data/discourse_markers/oanc_y.pkl', 'wb') as f:\n",
    "    pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset take 2: only sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/discourse_markers/vectorized_oanc_df.zip')\n",
    "with open('data/discourse_markers/oanc_terms.pkl', 'rb') as f:\n",
    "    terms_dict = pickle.load(f)\n",
    "ind_dict = {v: k for k, v in terms_dict.items()}\n",
    "ind_dict[9] = 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>clean_and_tokenized</th>\n",
       "      <th>y</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[[In, my, three, decades, of, teaching, univer...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...</td>\n",
       "      <td>[[0.054467976, 0.1869069, 0.06425432, 0.130815...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[[As, a, byproduct, of, those, experiences, ,,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...</td>\n",
       "      <td>[[0.23088518, 5.699069e-05, -0.19189279, 0.195...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[[When, we, looked, for, a, preschool, ,, many...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...</td>\n",
       "      <td>[[0.16534813, 0.3831386, -0.071578294, 0.27849...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[[I, ’, ve, read, that, it, ’, s, the, quality...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...</td>\n",
       "      <td>[[-0.16124506, 0.21021883, -0.029272433, -0.12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[[His, father, ﬁrmly, insists, that, he, do, i...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...</td>\n",
       "      <td>[[0.17460386, 0.14078914, -0.039566375, -0.223...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      label  \\\n",
       "0  non-fiction/OUP/Berk/ch1   \n",
       "1  non-fiction/OUP/Berk/ch1   \n",
       "2  non-fiction/OUP/Berk/ch1   \n",
       "3  non-fiction/OUP/Berk/ch1   \n",
       "4  non-fiction/OUP/Berk/ch1   \n",
       "\n",
       "                                 clean_and_tokenized  \\\n",
       "0  [[In, my, three, decades, of, teaching, univer...   \n",
       "1  [[As, a, byproduct, of, those, experiences, ,,...   \n",
       "2  [[When, we, looked, for, a, preschool, ,, many...   \n",
       "3  [[I, ’, ve, read, that, it, ’, s, the, quality...   \n",
       "4  [[His, father, ﬁrmly, insists, that, he, do, i...   \n",
       "\n",
       "                                                   y  \\\n",
       "0  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...   \n",
       "1  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...   \n",
       "2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...   \n",
       "3  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...   \n",
       "4  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...   \n",
       "\n",
       "                                                   X  \n",
       "0  [[0.054467976, 0.1869069, 0.06425432, 0.130815...  \n",
       "1  [[0.23088518, 5.699069e-05, -0.19189279, 0.195...  \n",
       "2  [[0.16534813, 0.3831386, -0.071578294, 0.27849...  \n",
       "3  [[-0.16124506, 0.21021883, -0.029272433, -0.12...  \n",
       "4  [[0.17460386, 0.14078914, -0.039566375, -0.223...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda6553d2d0e48c985b5f730120e9945"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent1</th>\n",
       "      <th>sent2</th>\n",
       "      <th>label</th>\n",
       "      <th>X</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[In, my, three, decades, of, teaching, univers...</td>\n",
       "      <td>[I, also, served, on, boards, of, directors, a...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[[0.054467976, 0.1869069, 0.06425432, 0.130815...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[I, also, served, on, boards, of, directors, a...</td>\n",
       "      <td>[My, research, continually, drew, me, into, cl...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[[-0.04379302, 0.39241648, 0.17816554, 0.17242...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[As, a, byproduct, of, those, experiences, ,, ...</td>\n",
       "      <td>[Their, fervent, questions, ,, at, times, ridd...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[[0.23088518, 5.699069e-05, -0.19189279, 0.195...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[When, we, looked, for, a, preschool, ,, many,...</td>\n",
       "      <td>[To, me, ,, Lydia, ’, s, preschool, seems, lik...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[[0.16534813, 0.3831386, -0.071578294, 0.27849...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[To, me, ,, Lydia, ’, s, preschool, seems, lik...</td>\n",
       "      <td>[Why, is, Lydia, ,, who, ’, s, always, been, a...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[[0.22163266, 0.12578495, 0.051759634, 0.14625...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sent1  \\\n",
       "0  [In, my, three, decades, of, teaching, univers...   \n",
       "1  [I, also, served, on, boards, of, directors, a...   \n",
       "2  [As, a, byproduct, of, those, experiences, ,, ...   \n",
       "3  [When, we, looked, for, a, preschool, ,, many,...   \n",
       "4  [To, me, ,, Lydia, ’, s, preschool, seems, lik...   \n",
       "\n",
       "                                               sent2  \\\n",
       "0  [I, also, served, on, boards, of, directors, a...   \n",
       "1  [My, research, continually, drew, me, into, cl...   \n",
       "2  [Their, fervent, questions, ,, at, times, ridd...   \n",
       "3  [To, me, ,, Lydia, ’, s, preschool, seems, lik...   \n",
       "4  [Why, is, Lydia, ,, who, ’, s, always, been, a...   \n",
       "\n",
       "                      label  \\\n",
       "0  non-fiction/OUP/Berk/ch1   \n",
       "1  non-fiction/OUP/Berk/ch1   \n",
       "2  non-fiction/OUP/Berk/ch1   \n",
       "3  non-fiction/OUP/Berk/ch1   \n",
       "4  non-fiction/OUP/Berk/ch1   \n",
       "\n",
       "                                                   X  \\\n",
       "0  [[0.054467976, 0.1869069, 0.06425432, 0.130815...   \n",
       "1  [[-0.04379302, 0.39241648, 0.17816554, 0.17242...   \n",
       "2  [[0.23088518, 5.699069e-05, -0.19189279, 0.195...   \n",
       "3  [[0.16534813, 0.3831386, -0.071578294, 0.27849...   \n",
       "4  [[0.22163266, 0.12578495, 0.051759634, 0.14625...   \n",
       "\n",
       "                                y  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new DF of all sentence pairs\n",
    "sent_1 = []\n",
    "sent_2 = []\n",
    "label = []\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total = len(df)):    \n",
    "    seq_len = len(row['clean_and_tokenized'])\n",
    "    assert seq_len == len(row['y']) \n",
    "    assert seq_len == len(row['X'])\n",
    "    \n",
    "    for i in range(seq_len - 1):\n",
    "        label.append(row['label'])\n",
    "        sent_1.append(row['clean_and_tokenized'][i])\n",
    "        sent_2.append(row['clean_and_tokenized'][i+1])\n",
    "        X.append(row['X'][i:i+2])\n",
    "        y.append(row['y'][i+1])\n",
    "\n",
    "pair_df = pd.DataFrame()\n",
    "pair_df['sent1'] = sent_1\n",
    "pair_df['sent2'] = sent_2\n",
    "pair_df['label'] = label\n",
    "pair_df['X'] = X\n",
    "pair_df['y'] = y\n",
    "pair_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_df['y_dense'] = pair_df['y'].apply(np.argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_df.to_pickle('data/discourse_markers/oanc_pair_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_df = pd.read_pickle('data/discourse_markers/oanc_pair_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({9: 252574, 8: 9685, 7: 5545, 6: 1629, 5: 924, 4: 850, 2: 795, 3: 763, 1: 625, 0: 399})\n"
     ]
    }
   ],
   "source": [
    "# check distribution of classes\n",
    "counts = Counter()\n",
    "for count in pair_df[\"y_dense\"]:\n",
    "    counts[count] += 1\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for idx in range(9):\n",
    "    X.extend(pair_df[pair_df.y_dense == idx].X)\n",
    "    y.extend(pair_df[pair_df.y_dense == idx].y_dense)\n",
    "\n",
    "sampled_df = pair_df[pair_df.y_dense == 9].sample(n=10000, random_state=1)\n",
    "\n",
    "X.extend(sampled_df.X)\n",
    "y.extend(sampled_df.y_dense)\n",
    "\n",
    "X, y = shuffle(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equally sampled\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for idx in range(10):\n",
    "    X.extend(pair_df[pair_df.y_dense == idx].sample(n=399, random_state=1).X)\n",
    "    y.extend(pair_df[pair_df.y_dense == idx].sample(n=399, random_state=1).y_dense)\n",
    "\n",
    "X, y = shuffle(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/discourse_markers/oanc_X_pair.pkl', 'wb') as f:\n",
    "    pickle.dump(X, f)\n",
    "with open('data/discourse_markers/oanc_y_pair.pkl', 'wb') as f:\n",
    "    pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_len = 2\n",
    "num_units = 256\n",
    "embed_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/users/rcramerus/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "main_input = Input(shape = (input_len, embed_dim), dtype = 'float32', name = 'main_input')\n",
    "\n",
    "lstm = Bidirectional(LSTM(return_sequences = False, units = num_units), name = 'lstm')(main_input)\n",
    "dropout = Dropout(rate = 0.25, name = 'dropout')(lstm)\n",
    "output = Dense(10, activation='softmax', name = 'output')(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 2, 50)             0         \n",
      "_________________________________________________________________\n",
      "lstm (Bidirectional)         (None, 512)               628736    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 633,866\n",
      "Trainable params: 633,866\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs = main_input, outputs = output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/discourse_markers/oanc_X_pair.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "with open('data/discourse_markers/oanc_y_pair.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.keras.utils.to_categorical(y, num_classes=10, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 3591 samples, validate on 399 samples\n",
      "Epoch 1/5\n",
      "3591/3591 [==============================] - 6s 2ms/step - loss: 2.2864 - acc: 0.1295 - val_loss: 2.2746 - val_acc: 0.1028\n",
      "Epoch 2/5\n",
      "3591/3591 [==============================] - 2s 469us/step - loss: 2.2262 - acc: 0.1924 - val_loss: 2.2380 - val_acc: 0.1704\n",
      "Epoch 3/5\n",
      "3591/3591 [==============================] - 2s 466us/step - loss: 2.1784 - acc: 0.2236 - val_loss: 2.2253 - val_acc: 0.1880\n",
      "Epoch 4/5\n",
      "3591/3591 [==============================] - 2s 455us/step - loss: 2.1469 - acc: 0.2345 - val_loss: 2.2411 - val_acc: 0.1805\n",
      "Epoch 5/5\n",
      "3591/3591 [==============================] - 2s 474us/step - loss: 2.1190 - acc: 0.2473 - val_loss: 2.2379 - val_acc: 0.1754\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X, y, \n",
    "                    epochs = 5, \n",
    "                    batch_size = 32, \n",
    "                    validation_split = 0.1)#,\n",
    "                    #class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d2v = Doc2Vec.load(\"data/discourse_markers/d2v_oanc.model\")\n",
    "np.random.seed = 47\n",
    "X_pad = np.random.rand(50)\n",
    "with open('data/discourse_markers/oanc_terms.pkl', 'rb') as f:\n",
    "    terms_dict = pickle.load(f)\n",
    "ind_dict = {v: k for k, v in terms_dict.items()}\n",
    "ind_dict[9] = 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(passage, vectorized = False):\n",
    "    if not vectorized:\n",
    "        sentences = sent_tokenize(passage)\n",
    "        tok_sent = [word_tokenize(sentence) for sentence in sentences]\n",
    "        vectors = [d2v.infer_vector(sentence) for sentence in tok_sent]\n",
    "    else:\n",
    "        vectors = passage\n",
    "    \n",
    "    for idx in range(len(vectors) - 1):\n",
    "        if idx == 0 and not vectorized:\n",
    "            print(sentences[idx])\n",
    "        input_vec = np.array([vectors[idx], vectors[idx+1]])\n",
    "        ans = model.predict(np.array([input_vec,]))\n",
    "        if not vectorized:\n",
    "            print('[' + ind_dict[np.argmax(ans[0])] + '] ' + sentences[idx+1])\n",
    "        else:\n",
    "            return(ind_dict[np.argmax(ans[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "If you’re going to try, go all the way. Otherwise, don’t even start. This could mean losing girlfriends, wives, relatives and maybe even your mind. It could mean not eating for three or four days. It could mean freezing on a park bench. It could mean jail. It could mean derision. It could mean mockery–isolation. Isolation is the gift. All the others are a test of your endurance, of how much you really want to do it. And, you’ll do it, despite rejection and the worst odds. And it will be better than anything else you can imagine. If you’re going to try, go all the way. There is no other feeling like that. You will be alone with the gods, and the nights will flame with fire. You will ride life straight to perfect laughter. It’s the only good fight there is.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "If you’re going to try, go all the way.\n",
      "[Yet] Otherwise, don’t even start.\n",
      "[Now] This could mean losing girlfriends, wives, relatives and maybe even your mind.\n",
      "[So] It could mean not eating for three or four days.\n",
      "[So] It could mean freezing on a park bench.\n",
      "[Now] It could mean jail.\n",
      "[Now] It could mean derision.\n",
      "[Now] It could mean mockery–isolation.\n",
      "[So] Isolation is the gift.\n",
      "[NULL] All the others are a test of your endurance, of how much you really want to do it.\n",
      "[So] And, you’ll do it, despite rejection and the worst odds.\n",
      "[Now] And it will be better than anything else you can imagine.\n",
      "[Yet] If you’re going to try, go all the way.\n",
      "[So] There is no other feeling like that.\n",
      "[So] You will be alone with the gods, and the nights will flame with fire.\n",
      "[So] You will ride life straight to perfect laughter.\n",
      "[So] It’s the only good fight there is.\n"
     ]
    }
   ],
   "source": [
    "pred(text) # Also, she had"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2c7260941c44139f667f39fc403b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3990), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# see where the errors are landing\n",
    "\n",
    "errors = Counter()\n",
    "total_pred = Counter()\n",
    "correct = 0\n",
    "total = len(X)\n",
    "\n",
    "for idx in tqdm(range(len(X))):\n",
    "    predicted = pred(X[idx], True)\n",
    "    true = ind_dict[np.argmax(y[idx])]\n",
    "    \n",
    "    total_pred[predicted] += 1\n",
    "    \n",
    "    if predicted == true:\n",
    "        correct += 1\n",
    "    else:\n",
    "        errors[true + ' => ' + predicted] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.36591478696742\n"
     ]
    }
   ],
   "source": [
    "print(str(correct/float(total)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Now', 568),\n",
       " ('But', 511),\n",
       " ('Or', 471),\n",
       " ('First', 434),\n",
       " ('Also', 399),\n",
       " ('Well', 396),\n",
       " ('NULL', 348),\n",
       " ('So', 326),\n",
       " ('Yet', 319),\n",
       " ('And', 218)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_pred.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Yet => But', 68),\n",
       " ('And => But', 60),\n",
       " ('Well => Or', 60),\n",
       " ('And => Now', 59),\n",
       " ('So => But', 57),\n",
       " ('NULL => Now', 57),\n",
       " ('Or => Well', 55),\n",
       " ('Also => But', 55),\n",
       " ('Well => Now', 52),\n",
       " ('So => Well', 51),\n",
       " ('So => Now', 50),\n",
       " ('NULL => But', 49),\n",
       " ('Also => Now', 48),\n",
       " ('Yet => First', 46),\n",
       " ('But => Now', 46),\n",
       " ('NULL => Also', 45),\n",
       " ('Yet => Now', 44),\n",
       " ('Now => Or', 43),\n",
       " ('First => Now', 42),\n",
       " ('And => Or', 41),\n",
       " ('NULL => First', 40),\n",
       " ('And => Well', 39),\n",
       " ('So => Or', 39),\n",
       " ('Or => Now', 39),\n",
       " ('But => Also', 39),\n",
       " ('Also => First', 38),\n",
       " ('And => Also', 38),\n",
       " ('First => But', 38),\n",
       " ('First => Also', 37),\n",
       " ('But => So', 37),\n",
       " ('Well => So', 37),\n",
       " ('Now => First', 36),\n",
       " ('And => Yet', 36),\n",
       " ('Now => NULL', 35),\n",
       " ('Now => Also', 35),\n",
       " ('So => First', 34),\n",
       " ('Also => NULL', 34),\n",
       " ('First => NULL', 33),\n",
       " ('So => Yet', 33),\n",
       " ('But => Or', 33),\n",
       " ('And => NULL', 33),\n",
       " ('Yet => Also', 33),\n",
       " ('Now => Well', 32),\n",
       " ('Yet => NULL', 31),\n",
       " ('But => First', 31),\n",
       " ('Or => So', 31),\n",
       " ('Or => And', 30),\n",
       " ('Yet => Or', 30),\n",
       " ('But => Yet', 30),\n",
       " ('NULL => Or', 30),\n",
       " ('Also => So', 30),\n",
       " ('Now => But', 29),\n",
       " ('NULL => Yet', 29),\n",
       " ('But => And', 29),\n",
       " ('Or => But', 28),\n",
       " ('First => So', 28),\n",
       " ('NULL => So', 28),\n",
       " ('First => Or', 28),\n",
       " ('First => Yet', 27),\n",
       " ('And => So', 27),\n",
       " ('And => First', 27),\n",
       " ('Well => And', 27),\n",
       " ('Well => NULL', 25),\n",
       " ('Well => First', 25),\n",
       " ('So => Also', 24),\n",
       " ('But => Well', 24),\n",
       " ('First => Well', 23),\n",
       " ('Now => Yet', 22),\n",
       " ('So => NULL', 22),\n",
       " ('Or => First', 22),\n",
       " ('Also => Yet', 22),\n",
       " ('Or => NULL', 21),\n",
       " ('Yet => So', 21),\n",
       " ('Also => Well', 20),\n",
       " ('Also => Or', 20),\n",
       " ('But => NULL', 20),\n",
       " ('Yet => Well', 20),\n",
       " ('Now => And', 19),\n",
       " ('Also => And', 19),\n",
       " ('So => And', 19),\n",
       " ('Well => Yet', 18),\n",
       " ('Well => Also', 18),\n",
       " ('Now => So', 17),\n",
       " ('Well => But', 17),\n",
       " ('Or => Also', 17),\n",
       " ('NULL => And', 15),\n",
       " ('Yet => And', 13),\n",
       " ('NULL => Well', 12),\n",
       " ('Or => Yet', 9),\n",
       " ('First => And', 8)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
