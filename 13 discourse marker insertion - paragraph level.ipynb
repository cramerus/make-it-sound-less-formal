{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils import shuffle\n",
    "import random as rand\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda, Dropout, Bidirectional, LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train doc2vec encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/discourse_markers/oanc_df.zip')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# remove unwanted nans\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    for item in row['clean_and_tokenized']:\n",
    "        if type(item) == float:\n",
    "            df = df.drop([idx])\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f83defa86eb41d496839ccf1f257894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=65101), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_tokens = []\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    for item in row['clean_and_tokenized']:\n",
    "        X_tokens.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50e5d97ecf549549e56b98baccba5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=338890), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tagged = []\n",
    "for i, sent in enumerate(tqdm(X_tokens)):\n",
    "    tagged.append(TaggedDocument(words = sent, tags = [str(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary built\n"
     ]
    }
   ],
   "source": [
    "d2v_oanc = Doc2Vec(vector_size = 50, min_count = 1, dm = 1)\n",
    "d2v_oanc.build_vocab(tagged)\n",
    "print('vocabulary built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training finished\n",
      "trained & saved\n"
     ]
    }
   ],
   "source": [
    "d2v_oanc.train(tagged, total_examples = d2v_oanc.corpus_count, epochs = 20)\n",
    "print('training finished')\n",
    "d2v_oanc.save(\"data/discourse_markers/d2v_oanc.model\")\n",
    "print(\"trained & saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorize texts and add to new df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e25a8276ef4837822ab1c5f1d4ee47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=65101), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vecs = []\n",
    "\n",
    "index = 0\n",
    "for idx, row in tqdm(df.iterrows(), total = len(df)):\n",
    "    current_vecs = []\n",
    "    for item in row['clean_and_tokenized']:\n",
    "        assert tagged[index].words == item\n",
    "        current_vecs.append(d2v_oanc.docvecs[str(index)])\n",
    "        index += 1\n",
    "    vecs.append(current_vecs)\n",
    "        \n",
    "df['X'] = vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['sents', 'text'])\n",
    "df = df.rename(columns={\"vectors\": \"y\"})\n",
    "df.to_pickle('data/discourse_markers/vectorized_oanc_df.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract X and y, prepare balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/discourse_markers/vectorized_oanc_df.zip')\n",
    "with open('data/discourse_markers/oanc_terms.pkl', 'rb') as f:\n",
    "    terms_dict = pickle.load(f)\n",
    "ind_dict = {v: k for k, v in terms_dict.items()}\n",
    "ind_dict[9] = 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>clean_and_tokenized</th>\n",
       "      <th>y</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[[In, my, three, decades, of, teaching, univer...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...</td>\n",
       "      <td>[[0.054467976, 0.1869069, 0.06425432, 0.130815...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[[As, a, byproduct, of, those, experiences, ,,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...</td>\n",
       "      <td>[[0.23088518, 5.699069e-05, -0.19189279, 0.195...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[[When, we, looked, for, a, preschool, ,, many...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...</td>\n",
       "      <td>[[0.16534813, 0.3831386, -0.071578294, 0.27849...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[[I, ’, ve, read, that, it, ’, s, the, quality...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...</td>\n",
       "      <td>[[-0.16124506, 0.21021883, -0.029272433, -0.12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[[His, father, ﬁrmly, insists, that, he, do, i...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...</td>\n",
       "      <td>[[0.17460386, 0.14078914, -0.039566375, -0.223...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      label  \\\n",
       "0  non-fiction/OUP/Berk/ch1   \n",
       "1  non-fiction/OUP/Berk/ch1   \n",
       "2  non-fiction/OUP/Berk/ch1   \n",
       "3  non-fiction/OUP/Berk/ch1   \n",
       "4  non-fiction/OUP/Berk/ch1   \n",
       "\n",
       "                                 clean_and_tokenized  \\\n",
       "0  [[In, my, three, decades, of, teaching, univer...   \n",
       "1  [[As, a, byproduct, of, those, experiences, ,,...   \n",
       "2  [[When, we, looked, for, a, preschool, ,, many...   \n",
       "3  [[I, ’, ve, read, that, it, ’, s, the, quality...   \n",
       "4  [[His, father, ﬁrmly, insists, that, he, do, i...   \n",
       "\n",
       "                                                   y  \\\n",
       "0  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...   \n",
       "1  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...   \n",
       "2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...   \n",
       "3  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...   \n",
       "4  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...   \n",
       "\n",
       "                                                   X  \n",
       "0  [[0.054467976, 0.1869069, 0.06425432, 0.130815...  \n",
       "1  [[0.23088518, 5.699069e-05, -0.19189279, 0.195...  \n",
       "2  [[0.16534813, 0.3831386, -0.071578294, 0.27849...  \n",
       "3  [[-0.16124506, 0.21021883, -0.029272433, -0.12...  \n",
       "4  [[0.17460386, 0.14078914, -0.039566375, -0.223...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for idx, row in tqdm(df.iterrows(), total = len(df)):\n",
    "    # realized that i'd put the null vector as [0] * 10 instead of [0]*9,[1]\n",
    "    # need to fix that here\n",
    "    new_y = []\n",
    "    for item in row['y']:\n",
    "        if item == [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]:\n",
    "            new_y.append([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
    "        else:\n",
    "            new_y.append(item)\n",
    "    df.at[idx, 'y'] = new_y\n",
    "\n",
    "df.to_pickle('data/discourse_markers/vectorized_oanc_df.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "081f5b9033e544f09c8f6a1ba8b70aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=65101), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[436, 586, 691, 678, 692, 978, 1658, 4858, 8762, 55002]\n",
      "number of samples\t15699\n",
      "number of subsamples\t74341\n",
      "\n",
      "Well\t1%\n",
      "Yet\t1%\n",
      "Or\t1%\n",
      "First\t1%\n",
      "Also\t1%\n",
      "Now\t1%\n",
      "So\t2%\n",
      "And\t7%\n",
      "But\t12%\n",
      "NULL\t74%\n"
     ]
    }
   ],
   "source": [
    "# check distribution of classes\n",
    "distribution = [0] * 10\n",
    "num_samples = 0\n",
    "class_weights = {}\n",
    "count = Counter()\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total = len(df)):    \n",
    "    # to make things a bit more balanced, skip if all sentences are in NULL class\n",
    "    if all(item == [0, 0, 0, 0, 0, 0, 0, 0, 0, 1] for item in row['y']):\n",
    "        continue\n",
    "        \n",
    "    # cut out super long paragraphs first\n",
    "    if len(row['y']) > 8:\n",
    "        continue\n",
    "    count[len(row['y'])] += 1\n",
    "        \n",
    "    row_sum = [sum(i) for i in zip(*row['y'])]\n",
    "    distribution = [x + y for x, y in zip(distribution, row_sum)]\n",
    "    \n",
    "    num_samples += 1\n",
    "\n",
    "print(distribution)\n",
    "total = sum(distribution)\n",
    "print('number of samples\\t' + str(num_samples))\n",
    "print('number of subsamples\\t' + str(total))\n",
    "print()\n",
    "\n",
    "for idx in range(len(distribution)):\n",
    "    print(ind_dict[idx] + '\\t' + \"{0:.0%}\".format(distribution[idx]/float(total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 170.50688073394497,\n",
       " 1: 126.8617747440273,\n",
       " 2: 107.58465991316932,\n",
       " 3: 109.64749262536873,\n",
       " 4: 107.42919075144509,\n",
       " 5: 76.01329243353783,\n",
       " 6: 44.83775633293124,\n",
       " 7: 15.302799505969535,\n",
       " 8: 8.484478429582287,\n",
       " 9: 1.3516053961674122}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_class_weight(dist, mu=0.15):\n",
    "    total = sum(dist)\n",
    "    labels = range(len(dist))\n",
    "    class_weight = {}\n",
    "\n",
    "    for label in labels:\n",
    "        #score = math.log(mu*total/float(labels_dict[key]))\n",
    "        class_weight[label] = total / dist[label]\n",
    "\n",
    "    return class_weight\n",
    "\n",
    "class_weights = create_class_weight(distribution)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86789d8a98fb4ca0b39284353bb5257a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=65101), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total = len(df)):    \n",
    "    # to make things a bit more balanced, skip if all sentences are in NULL class\n",
    "    if all(item == [0, 0, 0, 0, 0, 0, 0, 0, 0, 1] for item in row['y']):\n",
    "        continue\n",
    "        \n",
    "    # cut out super long paragraphs\n",
    "    if len(row['y']) > 8:\n",
    "        continue\n",
    "        \n",
    "    assert len(row['y']) == len(row['X'])\n",
    "    \n",
    "    X.append(row['X'])\n",
    "    y.append(row['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15699, 8, 50)\n",
      "(15699, 8, 10)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed = 47\n",
    "X_pad = np.random.rand(50)\n",
    "\n",
    "X = pad_sequences(X, maxlen=8, dtype='float32', padding='post', truncating='post', value=X_pad)\n",
    "print(X.shape)\n",
    "y = pad_sequences(y, maxlen=8, dtype='int32', padding='post', value = [0]*10)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/discourse_markers/oanc_X.pkl', 'wb') as f:\n",
    "    pickle.dump(X, f)\n",
    "with open('data/discourse_markers/oanc_y.pkl', 'wb') as f:\n",
    "    pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_len = X.shape[1]\n",
    "num_units = 128\n",
    "embed_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_input = Input(shape = (input_len, embed_dim), dtype = 'float32', name = 'main_input')\n",
    "\n",
    "bi_lstm = Bidirectional(LSTM(return_sequences = True, units = num_units), name = 'bi-lstm')(main_input)\n",
    "dropout = Dropout(rate = 0.25, name = 'dropout')(bi_lstm)\n",
    "output = Dense(10, activation='softmax', name = 'output')(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 8, 50)             0         \n",
      "_________________________________________________________________\n",
      "bi-lstm (Bidirectional)      (None, 8, 256)            183296    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 8, 10)             2570      \n",
      "=================================================================\n",
      "Total params: 185,866\n",
      "Trainable params: 185,866\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs = main_input, outputs = output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/discourse_markers/oanc_X.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "with open('data/discourse_markers/oanc_y.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "14129/14129 [==============================] - 22s 2ms/step - loss: 0.5893 - acc: 0.4384\n",
      "Epoch 2/5\n",
      "14129/14129 [==============================] - 19s 1ms/step - loss: 0.5632 - acc: 0.4387\n",
      "Epoch 3/5\n",
      "14129/14129 [==============================] - 19s 1ms/step - loss: 0.5584 - acc: 0.4391\n",
      "Epoch 4/5\n",
      "14129/14129 [==============================] - 19s 1ms/step - loss: 0.5555 - acc: 0.4396\n",
      "Epoch 5/5\n",
      "14129/14129 [==============================] - 18s 1ms/step - loss: 0.5534 - acc: 0.4397\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = 5, batch_size = 32)#, class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tbennun/keras-bucketed-sequence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
