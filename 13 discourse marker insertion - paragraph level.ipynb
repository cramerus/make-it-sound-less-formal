{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils import shuffle\n",
    "import random as rand\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda, Dropout\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load OANC corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/discourse_markers/oanc_df.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>sents</th>\n",
       "      <th>clean_and_tokenized</th>\n",
       "      <th>vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In my three decades of teaching university cou...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[In my three decades of teaching university co...</td>\n",
       "      <td>[[In, my, three, decades, of, teaching, univer...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As a byproduct of those experiences, parents r...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[As a byproduct of those experiences, parents ...</td>\n",
       "      <td>[[As, a, byproduct, of, those, experiences, ,,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>•Bob and Sharon, parents of a 4-year-old: Our ...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[When we looked for a preschool, many programs...</td>\n",
       "      <td>[[When, we, looked, for, a, preschool, ,, many...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>•Angela, mother of a 4-year-old and 6-year-old...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[I’ve read that it’s the quality of time we sp...</td>\n",
       "      <td>[[I, ’, ve, read, that, it, ’, s, the, quality...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>•Talia, mother of a 7-year-old: My son Anselmo...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[His father ﬁrmly insists that he do it by him...</td>\n",
       "      <td>[[His, father, ﬁrmly, insists, that, he, do, i...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  In my three decades of teaching university cou...   \n",
       "1  As a byproduct of those experiences, parents r...   \n",
       "2  •Bob and Sharon, parents of a 4-year-old: Our ...   \n",
       "3  •Angela, mother of a 4-year-old and 6-year-old...   \n",
       "4  •Talia, mother of a 7-year-old: My son Anselmo...   \n",
       "\n",
       "                      label  \\\n",
       "0  non-fiction/OUP/Berk/ch1   \n",
       "1  non-fiction/OUP/Berk/ch1   \n",
       "2  non-fiction/OUP/Berk/ch1   \n",
       "3  non-fiction/OUP/Berk/ch1   \n",
       "4  non-fiction/OUP/Berk/ch1   \n",
       "\n",
       "                                               sents  \\\n",
       "0  [In my three decades of teaching university co...   \n",
       "1  [As a byproduct of those experiences, parents ...   \n",
       "2  [When we looked for a preschool, many programs...   \n",
       "3  [I’ve read that it’s the quality of time we sp...   \n",
       "4  [His father ﬁrmly insists that he do it by him...   \n",
       "\n",
       "                                 clean_and_tokenized  \\\n",
       "0  [[In, my, three, decades, of, teaching, univer...   \n",
       "1  [[As, a, byproduct, of, those, experiences, ,,...   \n",
       "2  [[When, we, looked, for, a, preschool, ,, many...   \n",
       "3  [[I, ’, ve, read, that, it, ’, s, the, quality...   \n",
       "4  [[His, father, ﬁrmly, insists, that, he, do, i...   \n",
       "\n",
       "                                             vectors  \n",
       "0  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "1  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "3  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "4  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train doc2vec encodings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# remove unwanted nans\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    for item in row['clean_and_tokenized']:\n",
    "        if type(item) == float:\n",
    "            df = df.drop([idx])\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87b751c2f614b2498c7e2cb4bed0900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=65101), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_tokens = []\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    for item in row['clean_and_tokenized']:\n",
    "        X_tokens.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "774bb5a865544804a824403e1eafacdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=338890), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tagged = []\n",
    "for i, sent in enumerate(tqdm(X_tokens)):\n",
    "    tagged.append(TaggedDocument(words = sent, tags = [str(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary built\n"
     ]
    }
   ],
   "source": [
    "d2v_oanc = Doc2Vec(vector_size = 50, min_count = 1, dm = 1)\n",
    "d2v_oanc.build_vocab(tagged)\n",
    "print('vocabulary built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training finished\n",
      "trained & saved\n"
     ]
    }
   ],
   "source": [
    "d2v_oanc.train(tagged, total_examples = d2v_oanc.corpus_count, epochs = 20)\n",
    "print('training finished')\n",
    "d2v_oanc.save(\"data/discourse_markers/d2v_oanc.model\")\n",
    "print(\"trained & saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorize texts and add to new df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = []\n",
    "\n",
    "for idx, row in tqdm(brown_disc_df.iterrows(), total = len(brown_disc_df)):\n",
    "    vecs.append(d2v.docvecs[str(idx)])\n",
    "        \n",
    "brown_disc_df['vec'] = vecs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
