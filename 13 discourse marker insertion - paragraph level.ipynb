{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils import shuffle\n",
    "import random as rand\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda, Dropout, Bidirectional, LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train doc2vec encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/discourse_markers/oanc_df.zip')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# remove unwanted nans\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    for item in row['clean_and_tokenized']:\n",
    "        if type(item) == float:\n",
    "            df = df.drop([idx])\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f83defa86eb41d496839ccf1f257894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=65101), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_tokens = []\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    for item in row['clean_and_tokenized']:\n",
    "        X_tokens.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50e5d97ecf549549e56b98baccba5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=338890), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tagged = []\n",
    "for i, sent in enumerate(tqdm(X_tokens)):\n",
    "    tagged.append(TaggedDocument(words = sent, tags = [str(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary built\n"
     ]
    }
   ],
   "source": [
    "d2v_oanc = Doc2Vec(vector_size = 50, min_count = 1, dm = 1)\n",
    "d2v_oanc.build_vocab(tagged)\n",
    "print('vocabulary built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training finished\n",
      "trained & saved\n"
     ]
    }
   ],
   "source": [
    "d2v_oanc.train(tagged, total_examples = d2v_oanc.corpus_count, epochs = 20)\n",
    "print('training finished')\n",
    "d2v_oanc.save(\"data/discourse_markers/d2v_oanc.model\")\n",
    "print(\"trained & saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorize texts and add to new df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e25a8276ef4837822ab1c5f1d4ee47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=65101), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vecs = []\n",
    "\n",
    "index = 0\n",
    "for idx, row in tqdm(df.iterrows(), total = len(df)):\n",
    "    current_vecs = []\n",
    "    for item in row['clean_and_tokenized']:\n",
    "        assert tagged[index].words == item\n",
    "        current_vecs.append(d2v_oanc.docvecs[str(index)])\n",
    "        index += 1\n",
    "    vecs.append(current_vecs)\n",
    "        \n",
    "df['X'] = vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['sents', 'text'])\n",
    "df = df.rename(columns={\"vectors\": \"y\"})\n",
    "df.to_pickle('data/discourse_markers/vectorized_oanc_df.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract X and y, prepare balancing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.read_pickle('data/discourse_markers/vectorized_oanc_df.zip')\n",
    "with open('data/discourse_markers/oanc_terms.pkl', 'rb') as f:\n",
    "    terms_dict = pickle.load(f)\n",
    "ind_dict = {v: k for k, v in terms_dict.items()}\n",
    "ind_dict[9] = 'NULL'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for idx, row in tqdm(df.iterrows(), total = len(df)):\n",
    "    # realized that i'd put the null vector as [0] * 10 instead of [0]*9,[1]\n",
    "    # need to fix that here\n",
    "    new_y = []\n",
    "    for item in row['y']:\n",
    "        if item == [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]:\n",
    "            new_y.append([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
    "        else:\n",
    "            new_y.append(item)\n",
    "    df.at[idx, 'y'] = new_y\n",
    "\n",
    "df.to_pickle('data/discourse_markers/vectorized_oanc_df.zip')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# check distribution of classes\n",
    "distribution = [0] * 10\n",
    "num_samples = 0\n",
    "class_weights = {}\n",
    "count = Counter()\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total = len(df)):    \n",
    "    # to make things a bit more balanced, skip if all sentences are in NULL class\n",
    "    if all(item == [0, 0, 0, 0, 0, 0, 0, 0, 0, 1] for item in row['y']):\n",
    "        continue\n",
    "        \n",
    "    # cut out super long paragraphs first\n",
    "    if len(row['y']) > 8:\n",
    "        continue\n",
    "    count[len(row['y'])] += 1\n",
    "        \n",
    "    row_sum = [sum(i) for i in zip(*row['y'])]\n",
    "    distribution = [x + y for x, y in zip(distribution, row_sum)]\n",
    "    \n",
    "    num_samples += 1\n",
    "\n",
    "print(distribution)\n",
    "total = sum(distribution)\n",
    "print('number of samples\\t' + str(num_samples))\n",
    "print('number of subsamples\\t' + str(total))\n",
    "print()\n",
    "\n",
    "for idx in range(len(distribution)):\n",
    "    print(ind_dict[idx] + '\\t' + \"{0:.0%}\".format(distribution[idx]/float(total)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def create_class_weight(dist, mu=0.15):\n",
    "    total = sum(dist)\n",
    "    labels = range(len(dist))\n",
    "    class_weight = {}\n",
    "\n",
    "    for label in labels:\n",
    "        #score = math.log(mu*total/float(labels_dict[key]))\n",
    "        class_weight[label] = total / dist[label]\n",
    "\n",
    "    return class_weight\n",
    "\n",
    "class_weights = create_class_weight(distribution)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total = len(df)):    \n",
    "    # to make things a bit more balanced, skip if all sentences are in NULL class\n",
    "    if all(item == [0, 0, 0, 0, 0, 0, 0, 0, 0, 1] for item in row['y']):\n",
    "        continue\n",
    "        \n",
    "    # cut out super long paragraphs\n",
    "    if len(row['y']) > 8:\n",
    "        continue\n",
    "        \n",
    "    assert len(row['y']) == len(row['X'])\n",
    "    \n",
    "    X.append(row['X'])\n",
    "    y.append(row['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## padding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.random.seed = 47\n",
    "X_pad = np.random.rand(50)\n",
    "\n",
    "X = pad_sequences(X, maxlen=8, dtype='float32', padding='post', truncating='post', value=X_pad)\n",
    "print(X.shape)\n",
    "y = pad_sequences(y, maxlen=8, dtype='int32', padding='post', value = [0]*10)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('data/discourse_markers/oanc_X.pkl', 'wb') as f:\n",
    "    pickle.dump(X, f)\n",
    "with open('data/discourse_markers/oanc_y.pkl', 'wb') as f:\n",
    "    pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset take 2: only sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/discourse_markers/vectorized_oanc_df.zip')\n",
    "with open('data/discourse_markers/oanc_terms.pkl', 'rb') as f:\n",
    "    terms_dict = pickle.load(f)\n",
    "ind_dict = {v: k for k, v in terms_dict.items()}\n",
    "ind_dict[9] = 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>clean_and_tokenized</th>\n",
       "      <th>y</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[[In, my, three, decades, of, teaching, univer...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...</td>\n",
       "      <td>[[0.054467976, 0.1869069, 0.06425432, 0.130815...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[[As, a, byproduct, of, those, experiences, ,,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...</td>\n",
       "      <td>[[0.23088518, 5.699069e-05, -0.19189279, 0.195...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[[When, we, looked, for, a, preschool, ,, many...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...</td>\n",
       "      <td>[[0.16534813, 0.3831386, -0.071578294, 0.27849...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[[I, ’, ve, read, that, it, ’, s, the, quality...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...</td>\n",
       "      <td>[[-0.16124506, 0.21021883, -0.029272433, -0.12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[[His, father, ﬁrmly, insists, that, he, do, i...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...</td>\n",
       "      <td>[[0.17460386, 0.14078914, -0.039566375, -0.223...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      label  \\\n",
       "0  non-fiction/OUP/Berk/ch1   \n",
       "1  non-fiction/OUP/Berk/ch1   \n",
       "2  non-fiction/OUP/Berk/ch1   \n",
       "3  non-fiction/OUP/Berk/ch1   \n",
       "4  non-fiction/OUP/Berk/ch1   \n",
       "\n",
       "                                 clean_and_tokenized  \\\n",
       "0  [[In, my, three, decades, of, teaching, univer...   \n",
       "1  [[As, a, byproduct, of, those, experiences, ,,...   \n",
       "2  [[When, we, looked, for, a, preschool, ,, many...   \n",
       "3  [[I, ’, ve, read, that, it, ’, s, the, quality...   \n",
       "4  [[His, father, ﬁrmly, insists, that, he, do, i...   \n",
       "\n",
       "                                                   y  \\\n",
       "0  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...   \n",
       "1  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...   \n",
       "2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...   \n",
       "3  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...   \n",
       "4  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, ...   \n",
       "\n",
       "                                                   X  \n",
       "0  [[0.054467976, 0.1869069, 0.06425432, 0.130815...  \n",
       "1  [[0.23088518, 5.699069e-05, -0.19189279, 0.195...  \n",
       "2  [[0.16534813, 0.3831386, -0.071578294, 0.27849...  \n",
       "3  [[-0.16124506, 0.21021883, -0.029272433, -0.12...  \n",
       "4  [[0.17460386, 0.14078914, -0.039566375, -0.223...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda6553d2d0e48c985b5f730120e9945"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent1</th>\n",
       "      <th>sent2</th>\n",
       "      <th>label</th>\n",
       "      <th>X</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[In, my, three, decades, of, teaching, univers...</td>\n",
       "      <td>[I, also, served, on, boards, of, directors, a...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[[0.054467976, 0.1869069, 0.06425432, 0.130815...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[I, also, served, on, boards, of, directors, a...</td>\n",
       "      <td>[My, research, continually, drew, me, into, cl...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[[-0.04379302, 0.39241648, 0.17816554, 0.17242...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[As, a, byproduct, of, those, experiences, ,, ...</td>\n",
       "      <td>[Their, fervent, questions, ,, at, times, ridd...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[[0.23088518, 5.699069e-05, -0.19189279, 0.195...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[When, we, looked, for, a, preschool, ,, many,...</td>\n",
       "      <td>[To, me, ,, Lydia, ’, s, preschool, seems, lik...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[[0.16534813, 0.3831386, -0.071578294, 0.27849...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[To, me, ,, Lydia, ’, s, preschool, seems, lik...</td>\n",
       "      <td>[Why, is, Lydia, ,, who, ’, s, always, been, a...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[[0.22163266, 0.12578495, 0.051759634, 0.14625...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sent1  \\\n",
       "0  [In, my, three, decades, of, teaching, univers...   \n",
       "1  [I, also, served, on, boards, of, directors, a...   \n",
       "2  [As, a, byproduct, of, those, experiences, ,, ...   \n",
       "3  [When, we, looked, for, a, preschool, ,, many,...   \n",
       "4  [To, me, ,, Lydia, ’, s, preschool, seems, lik...   \n",
       "\n",
       "                                               sent2  \\\n",
       "0  [I, also, served, on, boards, of, directors, a...   \n",
       "1  [My, research, continually, drew, me, into, cl...   \n",
       "2  [Their, fervent, questions, ,, at, times, ridd...   \n",
       "3  [To, me, ,, Lydia, ’, s, preschool, seems, lik...   \n",
       "4  [Why, is, Lydia, ,, who, ’, s, always, been, a...   \n",
       "\n",
       "                      label  \\\n",
       "0  non-fiction/OUP/Berk/ch1   \n",
       "1  non-fiction/OUP/Berk/ch1   \n",
       "2  non-fiction/OUP/Berk/ch1   \n",
       "3  non-fiction/OUP/Berk/ch1   \n",
       "4  non-fiction/OUP/Berk/ch1   \n",
       "\n",
       "                                                   X  \\\n",
       "0  [[0.054467976, 0.1869069, 0.06425432, 0.130815...   \n",
       "1  [[-0.04379302, 0.39241648, 0.17816554, 0.17242...   \n",
       "2  [[0.23088518, 5.699069e-05, -0.19189279, 0.195...   \n",
       "3  [[0.16534813, 0.3831386, -0.071578294, 0.27849...   \n",
       "4  [[0.22163266, 0.12578495, 0.051759634, 0.14625...   \n",
       "\n",
       "                                y  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new DF of all sentence pairs\n",
    "sent_1 = []\n",
    "sent_2 = []\n",
    "label = []\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total = len(df)):    \n",
    "    seq_len = len(row['clean_and_tokenized'])\n",
    "    assert seq_len == len(row['y']) \n",
    "    assert seq_len == len(row['X'])\n",
    "    \n",
    "    for i in range(seq_len - 1):\n",
    "        label.append(row['label'])\n",
    "        sent_1.append(row['clean_and_tokenized'][i])\n",
    "        sent_2.append(row['clean_and_tokenized'][i+1])\n",
    "        X.append(row['X'][i:i+2])\n",
    "        y.append(row['y'][i+1])\n",
    "\n",
    "pair_df = pd.DataFrame()\n",
    "pair_df['sent1'] = sent_1\n",
    "pair_df['sent2'] = sent_2\n",
    "pair_df['label'] = label\n",
    "pair_df['X'] = X\n",
    "pair_df['y'] = y\n",
    "pair_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_df['y_dense'] = pair_df['y'].apply(np.argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_df.to_pickle('data/discourse_markers/oanc_pair_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({9: 252574, 8: 9685, 7: 5545, 6: 1629, 5: 924, 4: 850, 2: 795, 3: 763, 1: 625, 0: 399})\n"
     ]
    }
   ],
   "source": [
    "# check distribution of classes\n",
    "counts = Counter()\n",
    "for count in pair_df[\"y_dense\"]:\n",
    "    counts[count] += 1\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for idx in range(9):\n",
    "    X.extend(pair_df[pair_df.y_dense == idx].X)\n",
    "    y.extend(pair_df[pair_df.y_dense == idx].y_dense)\n",
    "\n",
    "sampled_df = pair_df[pair_df.y_dense == 9].sample(n=10000, random_state=1)\n",
    "\n",
    "X.extend(sampled_df.X)\n",
    "y.extend(sampled_df.y_dense)\n",
    "\n",
    "X, y = shuffle(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equally sampled\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for idx in range(10):\n",
    "    X.extend(pair_df[pair_df.y_dense == idx].sample(n=399, random_state=1).X)\n",
    "    y.extend(pair_df[pair_df.y_dense == idx].sample(n=399, random_state=1).y_dense)\n",
    "\n",
    "X, y = shuffle(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/discourse_markers/oanc_X_pair.pkl', 'wb') as f:\n",
    "    pickle.dump(X, f)\n",
    "with open('data/discourse_markers/oanc_y_pair.pkl', 'wb') as f:\n",
    "    pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# append BNC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_len = 2\n",
    "num_units = 256\n",
    "embed_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_input = Input(shape = (input_len, embed_dim), dtype = 'float32', name = 'main_input')\n",
    "\n",
    "lstm = Bidirectional(LSTM(return_sequences = False, units = num_units), name = 'lstm')(main_input)\n",
    "dropout = Dropout(rate = 0.25, name = 'dropout')(lstm)\n",
    "output = Dense(10, activation='softmax', name = 'output')(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 2, 50)             0         \n",
      "_________________________________________________________________\n",
      "lstm (Bidirectional)         (None, 512)               628736    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 633,866\n",
      "Trainable params: 633,866\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs = main_input, outputs = output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/discourse_markers/oanc_X_pair.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "with open('data/discourse_markers/oanc_y_pair.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.keras.utils.to_categorical(y, num_classes=10, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3591 samples, validate on 399 samples\n",
      "Epoch 1/5\n",
      "3591/3591 [==============================] - 3s 825us/step - loss: 2.2884 - acc: 0.1387 - val_loss: 2.2744 - val_acc: 0.1454\n",
      "Epoch 2/5\n",
      "3591/3591 [==============================] - 2s 476us/step - loss: 2.2263 - acc: 0.1983 - val_loss: 2.2412 - val_acc: 0.1704\n",
      "Epoch 3/5\n",
      "3591/3591 [==============================] - 2s 476us/step - loss: 2.1776 - acc: 0.2178 - val_loss: 2.2477 - val_acc: 0.1805\n",
      "Epoch 4/5\n",
      "3591/3591 [==============================] - 2s 473us/step - loss: 2.1489 - acc: 0.2356 - val_loss: 2.2410 - val_acc: 0.1654\n",
      "Epoch 5/5\n",
      "3591/3591 [==============================] - 2s 484us/step - loss: 2.1210 - acc: 0.2426 - val_loss: 2.2461 - val_acc: 0.1855\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X, y, \n",
    "                    epochs = 5, \n",
    "                    batch_size = 32, \n",
    "                    validation_split = 0.1)#,\n",
    "                    #class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/discourse_markers/d2v_oanc.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-9af131bc2cb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/discourse_markers/d2v_oanc.model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m47\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_pad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/discourse_markers/oanc_terms.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mterms_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rebekah/.local/lib/python3.6/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \"\"\"\n\u001b[1;32m   1105\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model saved using code from earlier Gensim Version. Re-loading old model in a compatible way.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rebekah/.local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \"\"\"\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseWordEmbeddingsModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ns_exponent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns_exponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rebekah/.local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \"\"\"\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rebekah/.local/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rebekah/.local/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m     \"\"\"\n\u001b[0;32m-> 1381\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1382\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rebekah/.local/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0mtransport_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_ext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_extension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransport_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mscrubbed_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rebekah/.local/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m     )\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rebekah/.local/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/discourse_markers/d2v_oanc.model'"
     ]
    }
   ],
   "source": [
    "d2v = Doc2Vec.load(\"data/discourse_markers/d2v_oanc.model\")\n",
    "np.random.seed = 47\n",
    "X_pad = np.random.rand(50)\n",
    "with open('data/discourse_markers/oanc_terms.pkl', 'rb') as f:\n",
    "    terms_dict = pickle.load(f)\n",
    "ind_dict = {v: k for k, v in terms_dict.items()}\n",
    "ind_dict[9] = 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(passage, vectorized = False):\n",
    "    if not vectorized:\n",
    "        sentences = sent_tokenize(passage)\n",
    "        tok_sent = [word_tokenize(sentence) for sentence in sentences]\n",
    "        vectors = [d2v.infer_vector(sentence) for sentence in tok_sent]\n",
    "    else:\n",
    "        vectors = passage\n",
    "    \n",
    "    for idx in range(len(vectors) - 1):\n",
    "        if idx == 0 and not vectorized:\n",
    "            print(sentences[idx])\n",
    "        input_vec = np.array([vectors[idx], vectors[idx+1]])\n",
    "        ans = model.predict(np.array([input_vec,]))\n",
    "        if not vectorized:\n",
    "            print('[' + ind_dict[np.argmax(ans[0])] + '] ' + sentences[idx+1])\n",
    "        else:\n",
    "            return(ind_dict[np.argmax(ans[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "If you’re going to try, go all the way. Otherwise, don’t even start. This could mean losing girlfriends, wives, relatives and maybe even your mind. It could mean not eating for three or four days. It could mean freezing on a park bench. It could mean jail. It could mean derision. It could mean mockery–isolation. Isolation is the gift. All the others are a test of your endurance, of how much you really want to do it. And, you’ll do it, despite rejection and the worst odds. And it will be better than anything else you can imagine. If you’re going to try, go all the way. There is no other feeling like that. You will be alone with the gods, and the nights will flame with fire. You will ride life straight to perfect laughter. It’s the only good fight there is.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "If you’re going to try, go all the way.\n",
      "[But] Otherwise, don’t even start.\n",
      "[But] This could mean losing girlfriends, wives, relatives and maybe even your mind.\n",
      "[But] It could mean not eating for three or four days.\n",
      "[But] It could mean freezing on a park bench.\n",
      "[But] It could mean jail.\n",
      "[But] It could mean derision.\n",
      "[But] It could mean mockery–isolation.\n",
      "[But] Isolation is the gift.\n",
      "[NULL] All the others are a test of your endurance, of how much you really want to do it.\n",
      "[But] And, you’ll do it, despite rejection and the worst odds.\n",
      "[But] And it will be better than anything else you can imagine.\n",
      "[But] If you’re going to try, go all the way.\n",
      "[But] There is no other feeling like that.\n",
      "[NULL] You will be alone with the gods, and the nights will flame with fire.\n",
      "[But] You will ride life straight to perfect laughter.\n",
      "[But] It’s the only good fight there is.\n"
     ]
    }
   ],
   "source": [
    "pred(text) # Also, she had"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa0afb972e74bdc91e24e801e25e081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31215), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# see where the errors are landing\n",
    "\n",
    "errors = Counter()\n",
    "total_pred = Counter()\n",
    "correct = 0\n",
    "total = len(X)\n",
    "\n",
    "for idx in tqdm(range(len(X))):\n",
    "    predicted = pred(X[idx], True)\n",
    "    true = ind_dict[np.argmax(y[idx])]\n",
    "    \n",
    "    total_pred[predicted] += 1\n",
    "    \n",
    "    if predicted == true:\n",
    "        correct += 1\n",
    "    else:\n",
    "        errors[true + ' => ' + predicted] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.02610924235143\n"
     ]
    }
   ],
   "source": [
    "print(str(correct/float(total)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NULL', 16788),\n",
       " ('But', 13235),\n",
       " ('And', 1144),\n",
       " ('First', 18),\n",
       " ('Or', 17),\n",
       " ('So', 8),\n",
       " ('Now', 4),\n",
       " ('Well', 1)]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_pred.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('But => NULL', 4267),\n",
       " ('NULL => But', 3125),\n",
       " ('And => NULL', 2708),\n",
       " ('And => But', 2474),\n",
       " ('So => But', 825),\n",
       " ('So => NULL', 704),\n",
       " ('Also => NULL', 578),\n",
       " ('Now => NULL', 535),\n",
       " ('First => NULL', 454),\n",
       " ('Or => NULL', 391),\n",
       " ('Now => But', 349),\n",
       " ('Yet => But', 317),\n",
       " ('Or => But', 310),\n",
       " ('Yet => NULL', 293),\n",
       " ('First => But', 280),\n",
       " ('Also => But', 250),\n",
       " ('But => And', 244),\n",
       " ('NULL => And', 226),\n",
       " ('Well => NULL', 215),\n",
       " ('Well => But', 141),\n",
       " ('So => And', 96),\n",
       " ('Or => And', 85),\n",
       " ('Well => And', 42),\n",
       " ('Now => And', 40),\n",
       " ('Also => And', 20),\n",
       " ('First => And', 20),\n",
       " ('Yet => And', 14),\n",
       " ('And => Or', 4),\n",
       " ('But => First', 3),\n",
       " ('But => So', 3),\n",
       " ('NULL => Or', 3),\n",
       " ('But => Or', 2),\n",
       " ('And => First', 2),\n",
       " ('But => Now', 2),\n",
       " ('NULL => So', 2),\n",
       " ('Also => First', 1),\n",
       " ('Yet => First', 1),\n",
       " ('So => First', 1),\n",
       " ('So => Now', 1),\n",
       " ('First => Or', 1),\n",
       " ('NULL => First', 1),\n",
       " ('Or => First', 1),\n",
       " ('Or => So', 1),\n",
       " ('Also => Now', 1)]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
