{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Dropout, Concatenate, TimeDistributed\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/lexical_repl/embedding.pkl', 'rb') as f:\n",
    "    embedding = pickle.load(f)\n",
    "    \n",
    "with open('data/lexical_repl/idx2w.pkl', 'rb') as f:\n",
    "    idx2w = pickle.load(f)\n",
    "    \n",
    "with open('data/lexical_repl/w2idx.pkl', 'rb') as f:\n",
    "    w2idx = pickle.load(f)\n",
    "    \n",
    "assert 0 not in idx2w\n",
    "assert '\\t' in w2idx # SOS\n",
    "assert '\\n' in w2idx # EOS\n",
    "assert '[UNK]' in w2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/lexical_repl/sents_marked_df.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_idx(string):\n",
    "    # turns sequence of tokens to sequence of indices\n",
    "    seq = word_tokenize(string)\n",
    "    idx = []\n",
    "    for word in seq:\n",
    "        word = word.lower()\n",
    "        if word in w2idx:\n",
    "            idx.append(w2idx[word])\n",
    "        #else: #unknown tokens?\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b3cb75e04e48a884ccae29a965da6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=509285), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# extract data to arrays from df, add POST-padding\n",
    "X = pad_sequences(df.idx, maxlen = 50, value = 0, padding = 'post').astype('int64')\n",
    "y_span = pad_sequences(df.binary, maxlen = 50, value = 0, padding = 'post').astype('int64')\n",
    "\n",
    "y_span_cat = np.zeros((y_span.shape[0], y_span.shape[1], 2))\n",
    "for idx_1 in tqdm(range(y_span.shape[0])):\n",
    "    for idx_2 in range(y_span.shape[1]):\n",
    "        y_span_cat[idx_1][idx_2] = to_categorical(y_span[idx_1][idx_2], num_classes = 2)\n",
    "        \n",
    "X, y_span_cat = shuffle(X, y_span_cat)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_span_cat, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# extract data to arrays from df, add POST-padding\n",
    "X_orig = pad_sequences(df['x_orig'], value = pad, padding = 'post').astype('int64')\n",
    "dec_input = pad_sequences(df['dec_input'], value = pad, padding = 'post').astype('int64')\n",
    "dec_target = pad_sequences(df['dec_target'], value = pad, padding = 'post').astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model: find spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_find_model(input_len, embedding, num_units, dropout_rate):\n",
    "    K.clear_session()\n",
    "    \n",
    "    main_input = Input(shape = (input_len,), dtype = 'int64', name = 'main_input')\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        embedding_layer = Embedding(input_dim = embedding.shape[0],\n",
    "                              output_dim = embedding.shape[1],\n",
    "                              weights = [embedding],\n",
    "                              trainable = False, \n",
    "                              mask_zero = True,\n",
    "                              name = 'embedding_layer')\n",
    "        input_embed = embedding_layer(main_input)\n",
    "        \n",
    "    bi_lstm = Bidirectional(LSTM(return_sequences = True, units = num_units), name='bi-lstm')(input_embed)\n",
    "    dropout_lstm = Dropout(rate = dropout_rate, name = 'dropout_lstm')(bi_lstm)\n",
    "    dense = TimeDistributed(Dense(num_units, activation = 'relu'), name = 'dense')(dropout_lstm)\n",
    "    dropout_dense = Dropout(rate = dropout_rate, name = 'dropout_dense')(dense)\n",
    "    # is timedistributed even needed anymore? dense can handle 3D input now?\n",
    "    output = TimeDistributed(Dense(2, activation = 'softmax'), name = 'output')(dropout_dense)\n",
    "    \n",
    "    model = Model(inputs = main_input, outputs = output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_layer (Embedding)  (None, 50, 100)           14830400  \n",
      "_________________________________________________________________\n",
      "bi-lstm (Bidirectional)      (None, 50, 256)           234496    \n",
      "_________________________________________________________________\n",
      "dropout_lstm (Dropout)       (None, 50, 256)           0         \n",
      "_________________________________________________________________\n",
      "dense (TimeDistributed)      (None, 50, 128)           32896     \n",
      "_________________________________________________________________\n",
      "dropout_dense (Dropout)      (None, 50, 128)           0         \n",
      "_________________________________________________________________\n",
      "output (TimeDistributed)     (None, 50, 2)             258       \n",
      "=================================================================\n",
      "Total params: 15,098,050\n",
      "Trainable params: 267,650\n",
      "Non-trainable params: 14,830,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "find_model = build_find_model(input_len = X.shape[1], \n",
    "                         embedding = embedding, \n",
    "                         num_units = 128, \n",
    "                         dropout_rate = 0.25)\n",
    "find_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "458356/458356 [==============================] - 2502s 5ms/step - loss: 0.0339 - binary_accuracy: 0.9907\n"
     ]
    }
   ],
   "source": [
    "find_model.compile(optimizer = 'adam',\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics = ['binary_accuracy'])\n",
    "\n",
    "find_history = find_model.fit(X_train, y_train, epochs = 1, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f597d87b794db7bb72a5a48abdc030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50929), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Absolute accuracy (all correct):\t\t0.8779084607983664\n",
      "Average number of incorrect labels per answer:\t0.5282844744644505\n"
     ]
    }
   ],
   "source": [
    "def evaluate_find(X, y):\n",
    "    true = np.argmax(y, axis = 2)\n",
    "    pred = np.argmax(find_model.predict(X), axis = 2)\n",
    "    total = float(y.shape[0])\n",
    "    \n",
    "    total_correct = 0\n",
    "    indiv_wrong = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for i in tqdm(range(y.shape[0])):\n",
    "        if (true[i] == pred[i]).all():\n",
    "            total_correct += 1\n",
    "        for j in range(y.shape[1]):\n",
    "            if true[i][j] != pred[i][j]:\n",
    "                indiv_wrong += 1\n",
    "            \n",
    "    print('Absolute accuracy (all correct):\\t\\t' + str(total_correct / total))\n",
    "    \n",
    "    print('Average number of incorrect labels per answer:\\t' + str(indiv_wrong / total))\n",
    "    \n",
    "    return pred\n",
    "\n",
    "results = evaluate_find(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\the\n",
      "0\tinstilled\n",
      "0\tupon\n",
      "0\ther\n",
      "0\ta\n",
      "0\tlove\n",
      "0\tfor\n",
      "0\tanimals\n",
      "0\t.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_find(X):\n",
    "    pad_X = pad_sequences([X], value = 0, padding = 'post', maxlen = 50).astype('int64')\n",
    "    pred = np.argmax(find_model.predict([pad_X], batch_size = 1), axis = 2)\n",
    "    result = ''\n",
    "    length = 50 if len(X) > 50 else len(X)\n",
    "    for i in range(length):\n",
    "        result += str(pred[0][i]) + '\\t' + idx2w[X[i]] + '\\n'\n",
    "    print(result)\n",
    "    \n",
    "text = \"\"\"\n",
    "He instilled upon her a love for animals.\n",
    "\"\"\"\n",
    "predict_find(seq_to_idx(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model: suggest replacements for spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_suggest_model(orig_len, repl_len, embedding, num_units, dropout_rate):\n",
    "    K.clear_session()\n",
    "    \n",
    "    orig_input = Input(shape = (orig_len,), dtype = 'int64', name = 'orig_input')\n",
    "    repl_input = Input(shape = (repl_len,), dtype = 'int64', name = 'repl_input')\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        embedding_layer = Embedding(input_dim = embedding.shape[0],\n",
    "                              output_dim = embedding.shape[1],\n",
    "                              weights = [embedding],\n",
    "                              trainable = False, \n",
    "                              mask_zero = True,\n",
    "                              name = 'embedding_layer')\n",
    "        orig_embed = embedding_layer(orig_input)\n",
    "        repl_embed = embedding_layer(repl_input)\n",
    "        \n",
    "    ### feed encoder input (main_input), decoder input (repl_input) and sliced replacement text to enc-dec system\n",
    "\n",
    "    # these should change later to some sort of context-based or conditional model\n",
    "    # also with attention\n",
    "\n",
    "    encoder = Bidirectional(LSTM(return_state = True, units = num_units), name = \"encoder\")\n",
    "    decoder = LSTM(return_sequences = True, return_state = True, name = \"decoder\", units = 2 * num_units)\n",
    "\n",
    "    enc_output, enc_h_forward, enc_c_forward, enc_h_backward, enc_c_backward = encoder(orig_embed)\n",
    "    enc_h = Concatenate()([enc_h_forward, enc_h_backward])\n",
    "    enc_c = Concatenate()([enc_c_forward, enc_c_backward])\n",
    "    dec_output, _, _ = decoder(repl_embed, initial_state = [enc_h, enc_c])\n",
    "\n",
    "    # Dropout?\n",
    "    # between enc-dec\n",
    "\n",
    "    dense = TimeDistributed(Dense(num_units, activation = 'relu'), name = 'dense_layer')\n",
    "    dec_tdd = TimeDistributed(Dense(embedding.shape[0], activation='softmax'), name = 'dense_output')\n",
    "\n",
    "    dec_dense = dense(dec_output)\n",
    "    repl_output = dec_tdd(dec_dense)\n",
    "    \n",
    "    model = Model(inputs = [orig_input, repl_input], outputs = repl_output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_generator_train(data, batch_size):\n",
    "    X_orig_whole = data[0]\n",
    "    dec_input_whole = data[1]\n",
    "    dec_target_whole = data[2]\n",
    "    \n",
    "    X_orig_whole, dec_input_whole, dec_target_whole = shuffle(X_orig_whole, \n",
    "                                                              dec_input_whole, \n",
    "                                                              dec_target_whole)\n",
    "    \n",
    "    i = 0\n",
    "            \n",
    "    while True:\n",
    "        if i + batch_size > len(X_orig_whole):\n",
    "            X_orig_batch = X_orig[i:]\n",
    "            dec_input_batch = dec_input_whole[i:]\n",
    "            dec_target_batch = dec_target_whole[i:]\n",
    "            i = 0\n",
    "        else:\n",
    "            X_orig_batch = X_orig[i:i+batch_size]\n",
    "            dec_input_batch = dec_input_whole[i:i+batch_size]\n",
    "            dec_target_batch = dec_target_whole[i:i+batch_size]\n",
    "            i += batch_size\n",
    "        \n",
    "        inputs = [X_orig_batch, dec_input_batch]\n",
    "        targets = np.array([to_categorical(x, num_classes = embedding.shape[0]) for x in dec_target_batch])\n",
    "        \n",
    "        yield inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggest_model = build_suggest_model(orig_len = X_orig.shape[1],\n",
    "                                   repl_len = dec_input.shape[1],\n",
    "                                   embedding = embedding,\n",
    "                                   num_units = 128,\n",
    "                                   dropout_rate = 0.25)\n",
    "\n",
    "suggest_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggest_model.compile(optimizer = 'adam',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "suggest_history = suggest_model.fit_generator(\n",
    "                            suggest_generator_train([X_orig, dec_input, dec_target], \n",
    "                                                    batch_size),\n",
    "                              steps_per_epoch=len(X_orig) // batch_size,\n",
    "                              epochs = 5,b\n",
    "                              verbose = 1)#,\n",
    "                              #validation_data = (x_val, y_val),\n",
    "                              #use_multiprocessing = True,\n",
    "                              #workers = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "#model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine encoder model: takes original input, outputs the states\n",
    "enc_model = Model(inputs = orig_input, outputs = [enc_h, enc_c])\n",
    "\n",
    "# define the states to input into the decoder (this is what you get from the encoder)\n",
    "inf_dec_h_input = Input(shape=(num_units * 2,)) #enc_h\n",
    "inf_dec_c_input = Input(shape=(num_units * 2,)) #enc_c\n",
    "inf_dec_states_input = [inf_dec_h_input, inf_dec_c_input]\n",
    "\n",
    "# these are the outputs you get when you run the decoder, set them up matching the original model\n",
    "# repl_embed is more of a placeholder - of course you won't actually have the answer when you infer\n",
    "inf_dec_main, inf_dec_h, inf_dec_c = decoder(repl_embed, initial_state = inf_dec_states_input)\n",
    "inf_dec_states = [inf_dec_h, inf_dec_c]\n",
    "inf_dec_dense = dense(inf_dec_main)\n",
    "inf_dec_output = dec_tdd(inf_dec_dense)\n",
    "\n",
    "# define decoder model\n",
    "dec_model = Model([repl_input] + inf_dec_states_input, [inf_dec_output] + inf_dec_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(input_seq):\n",
    "    # takes input sequence in form of seq of token indices\n",
    "    states = enc_model.predict(input_seq)\n",
    "    \n",
    "    # begin output sequence, use start character\n",
    "    target_seq = np.zeros((1, repl_len))\n",
    "    target_seq[0, 0] = w2idx['\\t']\n",
    "    \n",
    "    # using batch_size = 1, sample in a loop\n",
    "    stop = False\n",
    "    decoded = []\n",
    "    while not stop:\n",
    "        output_tok, h, c = dec_model.predict([target_seq] + states)\n",
    "        states = [h, c] # update states\n",
    "        \n",
    "        # sample a token\n",
    "        sample_idx = np.argmax(output_tok[0, -1, :]) # takes the last one in output\n",
    "        sample_tok = idx2w[sample_idx]\n",
    "        \n",
    "        # exit if maxlen is reached or stop character is found\n",
    "        if (sample_tok == '\\n' or len(decoded) > repl_len):\n",
    "            stop = True\n",
    "        else:\n",
    "            # update target_seq\n",
    "            decoded.append(sample_tok)\n",
    "            target_seq = np.zeros((1, repl_len))\n",
    "            target_seq[0, 0] = sample_idx\n",
    "    \n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'is not'\n",
    "\n",
    "decode(pad_sequences([seq_to_idx(test)], value = pad, padding = 'post', maxlen = orig_len).astype('int64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in np.random.choice(len(X_orig), 10):\n",
    "    sent = ''\n",
    "    for x in X_orig[idx]:\n",
    "        if x != 0:\n",
    "            sent += idx2w[x] + ' '\n",
    "    print(sent)\n",
    "    print(decode(np.array([X_orig[idx]])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
