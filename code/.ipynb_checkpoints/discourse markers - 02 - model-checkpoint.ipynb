{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils import shuffle\n",
    "import random as rand\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda, Dropout, Bidirectional, SimpleRNN\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was used to build model(s) to predict the different chosen discourse markers which might come between sentence pairs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import OANC and BNC datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "oanc_df = pd.read_pickle('data/discourse_markers/oanc_pair_df.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnc_df = pd.read_pickle('data/discourse_markers/bnc_pair_df.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorize using doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I trained a doc2vec model using gensim on all (original) sentences included in the datasets, and then vectorized all texts to include in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9fb890db6c546f88497714abacdb0ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=561648), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218e6763115f45d480cd5461c949a1c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=273702), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_tokens = []\n",
    "for idx, row in tqdm(bnc_df.iterrows(), total=len(bnc_df)):\n",
    "    X_tokens.append(row['sent1'])\n",
    "for idx, row in tqdm(oanc_df.iterrows(), total=len(oanc_df)):\n",
    "    X_tokens.append(row['sent1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92988e2f03994d1a86ce5dae347a86f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=835350), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tagged = []\n",
    "for i, sent in enumerate(tqdm(X_tokens)):\n",
    "    tagged.append(TaggedDocument(words = sent, tags = [str(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary built\n",
      "training finished\n",
      "trained & saved\n"
     ]
    }
   ],
   "source": [
    "d2v = Doc2Vec(vector_size = 100, min_count = 1, dm = 0)\n",
    "d2v.build_vocab(tagged)\n",
    "print('vocabulary built')\n",
    "d2v.train(tagged, total_examples = d2v.corpus_count, epochs = 20)\n",
    "print('training finished')\n",
    "d2v.save(\"data/discourse_markers/d2v.model\")\n",
    "print(\"trained & saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ffcddc6b504fde8273e43e2e10bf3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=561648), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59cdc9b2d3314909b7a47cee9784e25f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=273702), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 0\n",
    "\n",
    "X = []\n",
    "for idx, row in tqdm(bnc_df.iterrows(), total=len(bnc_df)):\n",
    "    assert tagged[index].words == row['sent1']\n",
    "    sent1_vec = d2v.docvecs[str(index)]\n",
    "    sent2_vec = d2v.infer_vector(row['sent2'])\n",
    "    if index + 1 < len(tagged):\n",
    "        if tagged[index+1].words == row['sent2']:\n",
    "            sent2_vec = d2v.docvecs[str(index)]\n",
    "    index += 1\n",
    "    X.append([sent1_vec, sent2_vec])  \n",
    "bnc_df['X'] = X\n",
    "\n",
    "X = []\n",
    "for idx, row in tqdm(oanc_df.iterrows(), total=len(oanc_df)):\n",
    "    assert tagged[index].words == row['sent1']\n",
    "    sent1_vec = d2v.docvecs[str(index)]\n",
    "    sent2_vec = d2v.infer_vector(row['sent2'])\n",
    "    if index + 1 < len(tagged):\n",
    "        if tagged[index+1].words == row['sent2']:\n",
    "            sent2_vec = d2v.docvecs[str(index)]\n",
    "    index += 1\n",
    "    X.append([sent1_vec, sent2_vec])  \n",
    "oanc_df['X'] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "oanc_df.to_pickle('data/discourse_markers/oanc_pair_df.zip')\n",
    "bnc_df.to_pickle('data/discourse_markers/bnc_pair_df.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the data is selected for training. First, a marker is chosen via its index in the provided dictionary. The resulting training set is balanced between positive and negative samples. A balanced test set is created by splitting the first results, and then an unbalanced dataset that matches the distribution in the original datasets is also sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_X_y(term_idx):\n",
    "    # takes an index for a term\n",
    "    # returns X_train, X_test, y_train, y_test\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df = pd.concat([df, \n",
    "                    oanc_df[oanc_df.y_dense == term_idx], \n",
    "                    bnc_df[bnc_df.y_dense == term_idx]],\n",
    "                  sort = False)\n",
    "    \n",
    "    sampled_oanc = oanc_df[oanc_df.y_dense != term_idx].sample(n=int(len(df)/2), random_state = 47)\n",
    "    sampled_bnc = bnc_df[bnc_df.y_dense != term_idx].sample(n=int(len(df)/2), random_state = 47)\n",
    "    df = pd.concat([df, sampled_oanc, sampled_bnc], sort = False)\n",
    "    df = df.reset_index(drop = True)\n",
    "    \n",
    "    indices = list(df.sample(frac=0.1, random_state = 47).index.values)\n",
    "    \n",
    "    set_labels = []\n",
    "    for idx, row in df.iterrows():\n",
    "        if idx in indices:\n",
    "            set_labels.append('test')\n",
    "        else:\n",
    "            set_labels.append('train')\n",
    "    df['set'] = set_labels\n",
    "    \n",
    "    X_train = np.stack(df[df.set == 'train'].X, axis = 0)\n",
    "    X_test = np.stack(df[df.set == 'test'].X, axis = 0)\n",
    "    y_train = [1 if x == term_idx else 0 for x in df[df.set == 'train'].y_dense]\n",
    "    y_train = to_categorical(y_train, 2)\n",
    "    y_test = [1 if x == term_idx else 0 for x in df[df.set == 'test'].y_dense]\n",
    "    y_test = to_categorical(y_test, 2)\n",
    "    \n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=0)\n",
    "    \n",
    "    return df, (X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(X_train, y_train, rnn_units = 128, dense_units = 100, dropout_rate = 0.5):\n",
    "    \n",
    "    input_len = 2\n",
    "    embed_dim = 100\n",
    "    batch_size = 32\n",
    "    \n",
    "    # make sure weights are empty\n",
    "    #model.load_weights('data/discourse_markers/model.clean')\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "    main_input = Input(shape = (input_len, embed_dim), dtype = 'float32', name = 'main_input')\n",
    "\n",
    "    rnn = Bidirectional(SimpleRNN(return_sequences = False, units = rnn_units), name = 'rnn')(main_input)\n",
    "    rnn_dropout = Dropout(rate = dropout_rate, name = 'dropout')(rnn)\n",
    "\n",
    "    dense = Dense(dense_units, activation = 'relu', name = 'dense')(rnn_dropout)\n",
    "    dense_dropout = Dropout(rate = dropout_rate, name = 'dense_dropout')(dense)\n",
    "\n",
    "    dense2 = Dense(dense_units, activation = 'relu', name = 'dense2')(dense_dropout)\n",
    "    dense2_dropout = Dropout(rate = dropout_rate, name = 'dense2_dropout')(dense2)\n",
    "\n",
    "    output = Dense(2, activation = 'softmax', name = 'output')(dense2_dropout)\n",
    "    \n",
    "    model = Model(inputs = main_input, outputs = output)\n",
    "    #model.summary()\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics = ['accuracy'])\n",
    "    \n",
    "    # save \"clear\" weights to retrain on different datasets\n",
    "    #model.save_weights('data/discourse_markers/model.clean')\n",
    "    \n",
    "    history = model.fit(X_train, y_train, \n",
    "                        epochs = 5, \n",
    "                        batch_size = batch_size, \n",
    "                        validation_split = 0.1,\n",
    "                       verbose = 0)\n",
    "    \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/discourse_markers/oanc_terms.pkl', 'rb') as f:\n",
    "    terms_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model for discourse marker:\tYet\n",
      "Length of training set:\t\t\t3091\n",
      "Length of validation set:\t\t343\n",
      "Length of test set:\t\t\t382\n",
      "Train accuracy:\t\t\t\t0.7094791329088276\n",
      "Validation accuracy:\t\t\t0.7005813939626827\n",
      "Test accuracy:\t\t\t\t0.7198952888943138\n",
      "\n",
      "Building model for discourse marker:\tSo\n",
      "Length of training set:\t\t\t7678\n",
      "Length of validation set:\t\t853\n",
      "Length of test set:\t\t\t948\n",
      "Train accuracy:\t\t\t\t0.7219327949521194\n",
      "Validation accuracy:\t\t\t0.7564402817283916\n",
      "Test accuracy:\t\t\t\t0.7710970469164949\n",
      "\n",
      "Building model for discourse marker:\tOr\n",
      "Length of training set:\t\t\t2601\n",
      "Length of validation set:\t\t289\n",
      "Length of test set:\t\t\t321\n",
      "Train accuracy:\t\t\t\t0.7412533641365664\n",
      "Validation accuracy:\t\t\t0.7517241379310344\n",
      "Test accuracy:\t\t\t\t0.7071651090342679\n",
      "\n",
      "Building model for discourse marker:\tAnd\n",
      "Length of training set:\t\t\t20926\n",
      "Length of validation set:\t\t2325\n",
      "Length of test set:\t\t\t2584\n",
      "Train accuracy:\t\t\t\t0.7520787536864366\n",
      "Validation accuracy:\t\t\t0.7687016336546821\n",
      "Test accuracy:\t\t\t\t0.7859907120743034\n",
      "\n",
      "Building model for discourse marker:\tNow\n",
      "Length of training set:\t\t\t3995\n",
      "Length of validation set:\t\t443\n",
      "Length of test set:\t\t\t493\n",
      "Train accuracy:\t\t\t\t0.6680851062636203\n",
      "Validation accuracy:\t\t\t0.67117117170815\n",
      "Test accuracy:\t\t\t\t0.7545638947651304\n",
      "\n",
      "Building model for discourse marker:\tBut\n",
      "Length of training set:\t\t\t34655\n",
      "Length of validation set:\t\t3850\n",
      "Length of test set:\t\t\t4278\n",
      "Train accuracy:\t\t\t\t0.7501082095004593\n",
      "Validation accuracy:\t\t\t0.7618800311607374\n",
      "Test accuracy:\t\t\t\t0.7648433846756363\n",
      "\n",
      "Building model for discourse marker:\tFirst\n",
      "Length of training set:\t\t\t2650\n",
      "Length of validation set:\t\t294\n",
      "Length of test set:\t\t\t327\n",
      "Train accuracy:\t\t\t\t0.7045283016618693\n",
      "Validation accuracy:\t\t\t0.7084745766752857\n",
      "Test accuracy:\t\t\t\t0.6941896024464832\n",
      "\n",
      "Building model for discourse marker:\tWell\n",
      "Length of training set:\t\t\t3690\n",
      "Length of validation set:\t\t410\n",
      "Length of test set:\t\t\t456\n",
      "Train accuracy:\t\t\t\t0.8151761516969055\n",
      "Validation accuracy:\t\t\t0.846715329917388\n",
      "Test accuracy:\t\t\t\t0.8552631578947368\n",
      "\n",
      "Building model for discourse marker:\tAlso\n",
      "Length of training set:\t\t\t1963\n",
      "Length of validation set:\t\t218\n",
      "Length of test set:\t\t\t242\n",
      "Train accuracy:\t\t\t\t0.6311767702496179\n",
      "Validation accuracy:\t\t\t0.6621004598870125\n",
      "Test accuracy:\t\t\t\t0.5743801657818566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for term in terms_dict:\n",
    "    print('Building model for discourse marker:\\t' + term)\n",
    "    \n",
    "    term_df, (X_train, X_test, y_train, y_test) = build_X_y(terms_dict[term])\n",
    "    term_df.to_pickle('data/discourse_markers_models/' + term + '_df.pkl')\n",
    "    print('Length of training set:\\t\\t\\t' + str(int(len(X_train)*.9)))\n",
    "    print('Length of validation set:\\t\\t' + str(int(len(X_train)*.1)))\n",
    "    print('Length of test set:\\t\\t\\t' + str(len(y_test)))\n",
    "    \n",
    "    history, model = build(X_train, y_train)\n",
    "    print('Train accuracy:\\t\\t\\t\\t' + str(history.history['acc'][-1]))\n",
    "    print('Validation accuracy:\\t\\t\\t' + str(history.history['val_acc'][-1]))\n",
    "        \n",
    "    loss, accuracy = model.evaluate(X_test, y_test, batch_size = 32, verbose = 0)\n",
    "    print('Test accuracy:\\t\\t\\t\\t' + str(accuracy) + '\\n')\n",
    "    \n",
    "    model.save('data/discourse_markers_models/model.' + term + '.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
