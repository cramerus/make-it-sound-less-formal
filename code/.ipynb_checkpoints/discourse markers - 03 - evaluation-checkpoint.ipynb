{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import pickle\n",
    "import random as rand\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "import keras\n",
    "import pydot\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import plot_model, to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I run tests on the data and the models trained previously for the discourse marker insertion task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# statistics on original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oanc_df = pd.read_pickle('data/discourse_markers/oanc_pair_df.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnc_df = pd.read_pickle('data/discourse_markers/bnc_pair_df.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/discourse_markers/oanc_terms.pkl', 'rb') as f:\n",
    "    terms_dict = pickle.load(f)\n",
    "idx_dict = {terms_dict[k]: k for k in terms_dict}\n",
    "idx_dict[9] = 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many samples for each chosen term\n",
    "for t in terms_dict:\n",
    "    idx = terms_dict[t]\n",
    "    print(t)\n",
    "    num = len(oanc_df[oanc_df.y_dense == idx]) + len(bnc_df[bnc_df.y_dense == idx])\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show random examples\n",
    "for idx, row in oanc_df.sample(100).iterrows():\n",
    "    if row['y_dense'] != 9:\n",
    "        print(' '.join(row['sent1']))\n",
    "        print(' '.join(row['sent2']))\n",
    "        print(idx_dict[row['y_dense']])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rebekah/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/rebekah/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/rebekah/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "model = load_model('data/discourse_markers_models/model.But.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_model(model, to_file = 'data/discourse_markers/graph.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load necessities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/discourse_markers/oanc_terms.pkl', 'rb') as f:\n",
    "    terms_dict = pickle.load(f)\n",
    "idx_dict = {terms_dict[k]: k for k in terms_dict}\n",
    "idx_dict[9] = 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/users/rcramerus/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "dfs = {}\n",
    "\n",
    "for term in terms_dict.keys():\n",
    "    models[term] = load_model('data/discourse_markers_models/model.' + term + '.h5')\n",
    "    dfs[term] = pd.read_pickle('data/discourse_markers_models/' + term + '_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d2v = Doc2Vec.load(\"data/discourse_markers/d2v.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(vecs, term):\n",
    "    # takes an array in shape [None, 2, 100]\n",
    "    # returns results for a given term\n",
    "        \n",
    "    return np.argmax(models[term].predict(vecs), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_vec(series):\n",
    "    # takes a series\n",
    "    # returns vectors in np.array format for pred function\n",
    "    \n",
    "    return np.array(list(series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_text(text, threshold = None):\n",
    "    # takes a single string of a passage\n",
    "    # predicts for all terms, what could be present\n",
    "    # can take a threshold, to reduce suggestion of discourse markers\n",
    "    sents = sent_tokenize(text)\n",
    "    tok_sents = [word_tokenize(sent) for sent in sents]\n",
    "    vectors = [d2v.infer_vector(sent) for sent in tok_sents]\n",
    "    \n",
    "    for idx in range(len(vectors) - 1):\n",
    "        input_vec = np.array([vectors[idx], vectors[idx+1]])\n",
    "        \n",
    "        ans = []\n",
    "        \n",
    "        for term in terms_dict.keys():\n",
    "            if threshold:\n",
    "                if models[term].predict(np.array([input_vec,]))[0][1] > threshold:\n",
    "                    ans.append(term)\n",
    "            elif np.argmax(models[term].predict(np.array([input_vec,]))[0]):\n",
    "                ans.append(term)\n",
    "                \n",
    "        if len(ans) > 0:                \n",
    "            ans_str = '/'.join(ans)\n",
    "            print(sents[idx])\n",
    "            print('[' + ans_str + '] ' + sents[idx+1])\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate training accuracy for all models, to see results without dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for term But on complete model:\t0.7709707578039786\n",
      "Training accuracy for term Now on complete model:\t0.724487497143768\n",
      "Training accuracy for term First on complete model:\t0.7263157894736842\n",
      "Training accuracy for term Also on complete model:\t0.6947754353257519\n",
      "Training accuracy for term And on complete model:\t0.789050404256046\n",
      "Training accuracy for term Yet on complete model:\t0.746433770014556\n",
      "Training accuracy for term Well on complete model:\t0.8546695927822482\n",
      "Training accuracy for term Or on complete model:\t0.7796610170316218\n",
      "Training accuracy for term So on complete model:\t0.774496014946456\n"
     ]
    }
   ],
   "source": [
    "for term in terms_dict.keys():\n",
    "    X_train = series_to_vec(dfs[term][dfs[term].set == 'train'].X)\n",
    "    y_train = to_categorical([1 if x==terms_dict[term] else 0 for x in dfs[term][dfs[term].set == 'train'].y_dense])\n",
    "    loss, accuracy = models[term].evaluate(X_train, y_train, batch_size = 32, verbose = 0)\n",
    "    print('Training accuracy for term ' + term + ' on complete model:\\t' + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## update dataframes with predictions immediately, and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working with term:\tFirst\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3272), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "working with term:\tYet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3817), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "working with term:\tOr\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3212), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "working with term:\tNow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4932), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "working with term:\tBut\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=42784), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "working with term:\tSo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9480), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "working with term:\tAlso\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2424), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "working with term:\tWell\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4557), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "working with term:\tAnd\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25836), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "for term in terms_dict:\n",
    "    print('working with term:\\t' + term)\n",
    "    y = []\n",
    "    predictions = []\n",
    "    for idx, row in tqdm(dfs[term].iterrows(), total = len(dfs[term]), leave = False):\n",
    "        if row.y_dense == terms_dict[term]: # change to a simpler and easier to use binary system\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "        predictions.append(pred(series_to_vec([row.X]), term)[0])\n",
    "    dfs[term]['pred'] = predictions\n",
    "    dfs[term]['y'] = y\n",
    "    dfs[term] = dfs[term].drop('y_dense', axis = 1) # we no longer need this row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in terms_dict.keys():\n",
    "    dfs[term].to_pickle('data/discourse_markers_models/' + term + '_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate precision, recall and f-score over all sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(true, pred):\n",
    "    # takes true values and predicted values of a model \n",
    "    # (binary classification only)\n",
    "    # returns precision, recall and f1 score\n",
    "    true = list(true)\n",
    "    pred = list(pred)\n",
    "    assert len(true) == len(pred)\n",
    "    \n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for i in range(len(true)):\n",
    "        if true[i] == 1:\n",
    "            if pred[i] == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if pred[i] == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "                \n",
    "    print('true positives:\\t\\t' + str(tp))\n",
    "    print('true negatives:\\t\\t' + str(tn))\n",
    "    print('false positives:\\t' + str(fp))\n",
    "    print('false negatives:\\t' + str(fn))\n",
    "    \n",
    "    a = (tp + tn) / (tp + tn + fp + fn)\n",
    "    p = tp / (tp+fp)\n",
    "    r = tp / (tp+fn)\n",
    "    f = 2 * (p*r) / (p+r)\n",
    "    \n",
    "    print('accuracy:\\t\\t' + str(a))\n",
    "    print('precision:\\t\\t' + str(p))\n",
    "    print('recall:\\t\\t\\t' + str(r))\n",
    "    print('f-score:\\t\\t' + str(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working with term:\tFirst\n",
      "true positives:\t\t91\n",
      "true negatives:\t\t136\n",
      "false positives:\t6\n",
      "false negatives:\t94\n",
      "accuracy:\t\t0.6941896024464832\n",
      "precision:\t\t0.9381443298969072\n",
      "recall:\t\t\t0.4918918918918919\n",
      "f-score:\t\t0.6453900709219857\n",
      "\n",
      "working with term:\tYet\n",
      "true positives:\t\t126\n",
      "true negatives:\t\t149\n",
      "false positives:\t44\n",
      "false negatives:\t63\n",
      "accuracy:\t\t0.7198952879581152\n",
      "precision:\t\t0.7411764705882353\n",
      "recall:\t\t\t0.6666666666666666\n",
      "f-score:\t\t0.701949860724234\n",
      "\n",
      "working with term:\tOr\n",
      "true positives:\t\t117\n",
      "true negatives:\t\t110\n",
      "false positives:\t57\n",
      "false negatives:\t37\n",
      "accuracy:\t\t0.7071651090342679\n",
      "precision:\t\t0.6724137931034483\n",
      "recall:\t\t\t0.7597402597402597\n",
      "f-score:\t\t0.7134146341463414\n",
      "\n",
      "working with term:\tNow\n",
      "true positives:\t\t152\n",
      "true negatives:\t\t220\n",
      "false positives:\t39\n",
      "false negatives:\t82\n",
      "accuracy:\t\t0.7545638945233266\n",
      "precision:\t\t0.7958115183246073\n",
      "recall:\t\t\t0.6495726495726496\n",
      "f-score:\t\t0.7152941176470589\n",
      "\n",
      "working with term:\tBut\n",
      "true positives:\t\t1370\n",
      "true negatives:\t\t1902\n",
      "false positives:\t243\n",
      "false negatives:\t763\n",
      "accuracy:\t\t0.7648433847592333\n",
      "precision:\t\t0.8493490390576566\n",
      "recall:\t\t\t0.6422878574777309\n",
      "f-score:\t\t0.7314468766684464\n",
      "\n",
      "working with term:\tSo\n",
      "true positives:\t\t301\n",
      "true negatives:\t\t430\n",
      "false positives:\t43\n",
      "false negatives:\t174\n",
      "accuracy:\t\t0.7710970464135021\n",
      "precision:\t\t0.875\n",
      "recall:\t\t\t0.6336842105263157\n",
      "f-score:\t\t0.735042735042735\n",
      "\n",
      "working with term:\tAlso\n",
      "true positives:\t\t54\n",
      "true negatives:\t\t85\n",
      "false positives:\t32\n",
      "false negatives:\t71\n",
      "accuracy:\t\t0.5743801652892562\n",
      "precision:\t\t0.627906976744186\n",
      "recall:\t\t\t0.432\n",
      "f-score:\t\t0.5118483412322274\n",
      "\n",
      "working with term:\tWell\n",
      "true positives:\t\t194\n",
      "true negatives:\t\t196\n",
      "false positives:\t17\n",
      "false negatives:\t49\n",
      "accuracy:\t\t0.8552631578947368\n",
      "precision:\t\t0.919431279620853\n",
      "recall:\t\t\t0.7983539094650206\n",
      "f-score:\t\t0.8546255506607928\n",
      "\n",
      "working with term:\tAnd\n",
      "true positives:\t\t948\n",
      "true negatives:\t\t1083\n",
      "false positives:\t162\n",
      "false negatives:\t391\n",
      "accuracy:\t\t0.7859907120743034\n",
      "precision:\t\t0.8540540540540541\n",
      "recall:\t\t\t0.7079910380881255\n",
      "f-score:\t\t0.7741935483870969\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for term in terms_dict.keys():\n",
    "    print('working with term:\\t' + term)\n",
    "    metrics(dfs[term][dfs[term].set == 'test'].y, dfs[term][dfs[term].set == 'test'].pred)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manual inspection of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['First', 'Yet', 'Or', 'Now', 'But', 'So', 'Also', 'Well', 'And'])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopped at but, so, also, well, have 1 example from: and, but. none from: so, also, well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = 'But'\n",
    "df = dfs[term][dfs[term].set == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243\n",
      "\n",
      "We have great confidence in our nation 's legal system and look for a timely resolution of this important matter .\n",
      "[FP/But] If you have any questions , comments or concerns , please do not hesitate to contact me .\n",
      "\n",
      "It also noted that the French national rugby team , currently in England for that sport 's World Cup , had ordered a meal of roast beef from room service at their hotel in Windsor with `` absolutely no stipulations as to where it came from . ''\n",
      "[FP/But] The paper wished the French team `` all the best '' in its semifinal on Sunday .\n",
      "\n",
      "Lewinsky -- dubbed a `` zaftig little rascal '' by the hometown LAT -- is generally perceived as childlike , weak , and sort of , like , stupid .\n",
      "[FP/But] Tripp is said to be a meatier character -- genuinely caring at times , staggeringly manipulative at others .\n",
      "\n",
      "Between 1993 and 1994 , they aired more than 12,000 times , and at one point Inphomation was shelling out half a million dollars a week to buy air time on cable stations .\n",
      "[FP/But] It was money well spent : At its peak , Psychic Friends was bringing in as much as $ 125 million a year , most of it through infomercials .\n",
      "\n",
      "You also get a lot of rapid reaction to things like Alan Greenspan speeches or employment-cost-index numbers .\n",
      "[FP/But] The conventional image of bondholders is that they 're conservative , long-term , careful investors .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FALSE POSITIVES\n",
    "print(len(df[(df.y == 0) & (df.pred == 1)]))\n",
    "print()\n",
    "for idx, row in df[(df.y == 0) & (df.pred == 1)].sample(5).iterrows():\n",
    "    print(' '.join(row['sent1']))\n",
    "    print('[FP/' + term + '] ' + ' '.join(row['sent2']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763\n",
      "\n",
      "The mostly one-piece garments worn by the inmates , which come in bright hues of yellow , blue , and ( especially ) red -- to judge from the photos , red is the dominant color this season on death row -- are not , Chatterbox presumes , actually available at Benetton outlets .\n",
      "[FN/But] Apparently , Benetton hopes that some of the death-row inmates ' existential glamour ( `` They broke the rules .\n",
      "\n",
      "If he had lied under oath about parking illegally I would n't be so disgusted .\n",
      "[FN/But] For a married man to have oral sex with a woman employee less than half his age in the Oval Office -- I ca n't claim not to be offended by that .\n",
      "\n",
      "They agreed on most matters regarding the runic characters and on many features of language .\n",
      "[FN/But] If Chapman had hoped to convert Einar Haugen to his own views of the authenticity of the stones , he did not succeed , for the Professor never deviated from his conviction that they were modern .\n",
      "\n",
      "I am glad that you talked to Ken Arrow .\n",
      "[FN/But] Nobel laureates , who have wide responsibilities and much on their mind , are not necessarily on top of what has been going on in research outside their usual field .\n",
      "\n",
      "Martin Needleman , the executive director of Brooklyn Legal Services Corp. A , said there is wide backing from the other programs for the lawsuit , Bronx Legal Services v. Legal Services for New York City , 02-6199 .\n",
      "[FN/But] He added that many other groups `` had gone along [ with the reorganization ] reluctantly '' when faced with the possible loss of most of their budgets .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FALSE NEGATIVES\n",
    "print(len(df[(df.y == 1) & (df.pred == 0)]))\n",
    "print()\n",
    "for idx, row in df[(df.y == 1) & (df.pred == 0)].sample(5).iterrows():\n",
    "    print(' '.join(row['sent1']))\n",
    "    print('[FN/' + term + '] ' + ' '.join(row['sent2']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1370\n",
      "\n",
      "Whether this reflects the inviolable rule of hospitality , the undeniable misogyny in the Old Testament , or that angels have higher standing than humans is open to question .\n",
      "[TP/But] The question can not even be asked when the details are mislaid .\n",
      "\n",
      "As a result of the no-confidence vote , Poland faces four months with a lame-duck government at a time when hard decisions about privatisation , tax and banking reforms have been left untaken .\n",
      "[TP/But] That is all .\n",
      "\n",
      "But she knew it would be hard and that the time would surely come when Creggan would grow angry with her as Kraal had done .\n",
      "[TP/But] She also knew that anger would be part of Creggan 's survival , part of the strength he would need if , as she hoped , his chance for freedom ever came .\n",
      "\n",
      "That sort of gossip certainly should be condemned ; that is the sort Hesiod warned against .\n",
      "[TP/But] Even he went on to say that gossip has ‘ a kind of divinity ’ .\n",
      "\n",
      "Normally Alexandra was not susceptible to that kind of come-on rubbish .\n",
      "[TP/But] There was no mistaking the admiration in his eyes , the way they lingered on her .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRUE POSITIVES\n",
    "print(len(df[(df.y == 1) & (df.pred == 1)]))\n",
    "print()\n",
    "for idx, row in df[(df.y == 1) & (df.pred == 1)].sample(5).iterrows():\n",
    "    print(' '.join(row['sent1']))\n",
    "    print('[TP/' + term + '] ' + ' '.join(row['sent2']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1902\n",
      "\n",
      "It is noticeable that the Russell-Copleston debate became embroiled in a discussion of necessary propositions , a discussion made necessary by Copleston 's desire to show Russell that the world is such that it must be the case that it has a Creator . But does theism have to make such a case ? Is n't it making the mistake of claiming too much ? Is n't it unnecessarily raising the stakes here ? Do we really have to be sure that God exists in order to believe in God ? Can not we argue , indeed , on the basis of the usual meaning of ‘ faith ’ that involves trust in the face of intellectual un certainty , that Russell 's uncertainty as to whether or not God exists , the agnostic position , is the one the theist in fact should hold ?\n",
      "[TN/But] Suppose there existed a God who wished us to be unsure whether or not He existed .\n",
      "\n",
      "Reviewers praise the second volume of Cook 's biography as well researched , thorough , and fascinating .\n",
      "[TN/But] Many also take it as a point of departure for talking about Hillary Clinton .\n",
      "\n",
      "This year 's first-prize winner , Before It 's Too Late , was created by a truck driver whose hobby is computer animation .\n",
      "[TN/But] A talking skeleton holding a LifeStyles condom in bony fingers confides that he never used a condom because he was `` too embarrassed to ask for them from behind the counter , '' and because he 'd `` feel awkward stopping in the middle of everything just to put this on . ''\n",
      "\n",
      "As well as looking down over the town , you can also see across into Slovakia .\n",
      "[TN/But] The Danube forms a natural boundary ; the bridge that up until World War I linked the two countries was symbolically left in ruins .\n",
      "\n",
      "How long shall a group be observed ?\n",
      "[TN/But] What is an acceptable chi-square value ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRUE NEGATIVES\n",
    "print(len(df[(df.y == 0) & (df.pred == 0)]))\n",
    "print()\n",
    "for idx, row in df[(df.y == 0) & (df.pred == 0)].sample(5).iterrows():\n",
    "    print(' '.join(row['sent1']))\n",
    "    print('[TN/' + term + '] ' + ' '.join(row['sent2']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test on Microsoft example text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The Azure portal is a web-based, unified console that provides an alternative to command-line tools. With the Azure portal, you can manage your Azure subscription using a graphical user interface. You can build, manage, and monitor everything from simple web apps to complex cloud deployments, create custom dashboards for an organized view of resources, and configure accessibility options for the best experience. The Azure portal is designed for resiliency and continuous availability. It has a presence in every Azure datacenter thereby making it resilient to individual datacenter failures and also avoids network slow-downs by being close to users. The Azure portal updates continuously and requires no downtime for maintenance activities.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Azure portal is a web-based, unified console that provides an alternative to command-line tools. With the Azure portal, you can manage your Azure subscription using a graphical user interface. You can build, manage, and monitor everything from simple web apps to complex cloud deployments, create custom dashboards for an organized view of resources, and configure accessibility options for the best experience. The Azure portal is designed for resiliency and continuous availability. It has a presence in every Azure datacenter thereby making it resilient to individual datacenter failures and also avoids network slow-downs by being close to users. The Azure portal updates continuously and requires no downtime for maintenance activities.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Azure portal is a web-based, unified console that provides an alternative to command-line tools.\n",
      "[Also/And] With the Azure portal, you can manage your Azure subscription using a graphical user interface.\n",
      "\n",
      "With the Azure portal, you can manage your Azure subscription using a graphical user interface.\n",
      "[But/Also/And] You can build, manage, and monitor everything from simple web apps to complex cloud deployments, create custom dashboards for an organized view of resources, and configure accessibility options for the best experience.\n",
      "\n",
      "You can build, manage, and monitor everything from simple web apps to complex cloud deployments, create custom dashboards for an organized view of resources, and configure accessibility options for the best experience.\n",
      "[But/And] The Azure portal is designed for resiliency and continuous availability.\n",
      "\n",
      "The Azure portal is designed for resiliency and continuous availability.\n",
      "[First/Now/And] It has a presence in every Azure datacenter thereby making it resilient to individual datacenter failures and also avoids network slow-downs by being close to users.\n",
      "\n",
      "It has a presence in every Azure datacenter thereby making it resilient to individual datacenter failures and also avoids network slow-downs by being close to users.\n",
      "[So/Also] The Azure portal updates continuously and requires no downtime for maintenance activities.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_from_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Azure portal is a web-based, unified console that provides an alternative to command-line tools.\n",
      "[And] With the Azure portal, you can manage your Azure subscription using a graphical user interface.\n",
      "\n",
      "You can build, manage, and monitor everything from simple web apps to complex cloud deployments, create custom dashboards for an organized view of resources, and configure accessibility options for the best experience.\n",
      "[And] The Azure portal is designed for resiliency and continuous availability.\n",
      "\n",
      "It has a presence in every Azure datacenter thereby making it resilient to individual datacenter failures and also avoids network slow-downs by being close to users.\n",
      "[So] The Azure portal updates continuously and requires no downtime for maintenance activities.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_from_text(text, 0.75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
