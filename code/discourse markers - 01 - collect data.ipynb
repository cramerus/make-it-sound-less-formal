{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves the purpose of extracting discourse markers/connectives from various corpora for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are used later in the notebook for various purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lines(loc):\n",
    "    # given file path, returns all lines in list form\n",
    "    with open(loc) as f:\n",
    "        return [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(loc):\n",
    "    # given file path\n",
    "    with open(loc) as f:\n",
    "        return f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_df(df, terms):\n",
    "    # count number of example sentences (unverified) for each example\n",
    "    terms_dict = defaultdict(list)\n",
    "    for item in tqdm(terms):\n",
    "        if item.strip() in df.columns:\n",
    "            continue\n",
    "        for sent in df['sent']:\n",
    "            sent = ' '.join(sent)\n",
    "            if item + ' ' in sent:\n",
    "                terms_dict[item.strip()].append(1)\n",
    "            else:\n",
    "                terms_dict[item.strip()].append(0)\n",
    "        df[item.strip()] = terms_dict[item.strip()]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sent_simple(sent, term):\n",
    "    # given a sentence and a term in one of the initial positions,\n",
    "    # removes the term and capitalizes the next appropriate word\n",
    "    # makes sure to remove commas as well\n",
    "    \n",
    "    copy = sent.copy()\n",
    "    \n",
    "    terms = nltk.word_tokenize(term)\n",
    "    if len(terms) == 1:\n",
    "        term = terms[0]\n",
    "    else:\n",
    "        temp_idx = -1\n",
    "        for term_idx in range(len(terms)):\n",
    "            if temp_idx == -1:\n",
    "                temp_idx = copy.index(terms[term_idx])\n",
    "            else:\n",
    "                try:\n",
    "                    assert copy.index(terms[term_idx]) == temp_idx + 1\n",
    "                except: # there's only 3\n",
    "                    print('skip')\n",
    "                    return np.nan\n",
    "                del copy[temp_idx + 1]\n",
    "        term = terms[0]\n",
    "    \n",
    "    idx = sent.index(term)\n",
    "    \n",
    "    if copy[idx + 1] == ',': # remove comma too\n",
    "        del copy[idx + 1]\n",
    "    elif copy[idx + 1] == '.': # make sure you're not removing entire sentence\n",
    "        print('end of sent')\n",
    "        return np.nan\n",
    "    elif copy[idx + 1] == \"''\": # end of quote\n",
    "        print('end of quote')\n",
    "        return np.nan\n",
    "        \n",
    "    copy[idx + 1] = copy[idx + 1].capitalize()\n",
    "    del copy[idx]\n",
    "        \n",
    "    return copy\n",
    "\n",
    "def clean_df(df):\n",
    "    # given a dataset with sentences\n",
    "    # provide 'clean' version of sentence, without the relevant term, using function above\n",
    "    # checks for capitalization and makes proper if necessary\n",
    "    \n",
    "    df['clean'] = [np.nan] * len(df)\n",
    "    df['clean'] = df['clean'].astype(object)\n",
    "    terms = df.columns[1:-1]\n",
    "    \n",
    "    for term in tqdm(terms):\n",
    "        for idx, row in df[df[term] == 1].iterrows():\n",
    "            df.at[idx, 'clean'] = clean_sent_simple(row['sent'], term)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sent(sent, term):\n",
    "    # given a sentence and a term in one of the initial positions,\n",
    "    # removes the term and capitalizes the next appropriate word\n",
    "    # makes sure to remove commas as well\n",
    "    # this version tailored to BNC corpus and gives feedback\n",
    "    \n",
    "    copy = sent.copy()\n",
    "    idx = copy.index(term)\n",
    "    copy[idx] = ''\n",
    "    \n",
    "    change = False\n",
    "    for rest_idx in range(idx, len(copy)):\n",
    "        if copy[rest_idx] == '':\n",
    "            continue\n",
    "        elif copy[rest_idx] == ',': # remove comma too\n",
    "            copy[rest_idx] = ''\n",
    "        elif copy[rest_idx] == '.':\n",
    "            print('ERROR: end of sent')\n",
    "            print(sent)\n",
    "            return np.nan\n",
    "        elif copy[rest_idx] == \"'\" or copy[rest_idx] == '\"':\n",
    "            print('ERROR: end of quote')\n",
    "            print(sent)\n",
    "            return np.nan\n",
    "        elif copy[rest_idx][0].isalpha():\n",
    "            copy[rest_idx] = copy[rest_idx].capitalize()\n",
    "            change = True\n",
    "            \n",
    "        if change == True:\n",
    "            break\n",
    "    \n",
    "    if change == False:\n",
    "        print('ERROR: no change possible')\n",
    "        print(sent)\n",
    "        return np.nan\n",
    "        \n",
    "    return list(filter(None, copy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sent(sent):\n",
    "    # ensure that sentence is proper, remove some dust, return\n",
    "    ends = ['”', '\"', \"'\", '.', ']', ')', '!', '?', '’', '…', ':']\n",
    "    sent = sent.strip(string.digits)\n",
    "    sent = sent.strip()\n",
    "    if sent == '':\n",
    "        return ''\n",
    "    if sent[0].isupper() and sent[-1] in ends: # try to filter out sentences that aren't complete\n",
    "        return sent\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_vector(sent, terms):\n",
    "    # given a dictionary of terms-to-idx, returns a vector of length len(terms + 1) showing which term is in the sentence\n",
    "    # the last class is the null class -- i.e. no term appears\n",
    "    # also returns the sentence, changed properly if a term does appear\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    \n",
    "    vec = [0] * (len(terms) + 1)\n",
    "    for term in terms:\n",
    "        if term in sent:\n",
    "            vec[terms[term]] = 1\n",
    "            sent = clean_sent(sent, term)\n",
    "            break            \n",
    "    \n",
    "    return vec, sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = [['Voice', 'over'], ['Male', 'speaker'], ['Female', 'speaker']]\n",
    "\n",
    "def parse_bnc_xml(path):\n",
    "    # takes path to BNC xml file\n",
    "    # returns description of texts\n",
    "    # returns list of documents, each of which is a list of sentences, each of which is a list of tokens\n",
    "    \n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    info = root[0][0][0][0].text\n",
    "    documents = []\n",
    "    \n",
    "    for div in root[1]:\n",
    "        doc = []\n",
    "        sent = []\n",
    "        for tag in div.iter():\n",
    "            if tag.text == '\\n':\n",
    "                sent = list(filter(None, sent))\n",
    "                if sent.count('.') > 1:\n",
    "                    temp = ' '.join(sent)\n",
    "                    temp = sent_tokenize(temp)\n",
    "                    for temp_sent in temp:\n",
    "                        temp_sent = word_tokenize(temp_sent)\n",
    "                        if len(sent) < 2:\n",
    "                            sent = []\n",
    "                        elif temp_sent in skip: #skip certain things\n",
    "                            sent = []\n",
    "                        elif any([piece.isupper() for piece in temp_sent]):\n",
    "                            sent = []\n",
    "                        else:\n",
    "                            doc.append(temp_sent)\n",
    "                elif len(sent) > 1:\n",
    "                    if sent in skip:\n",
    "                        sent = []\n",
    "                    elif any([piece.isupper() for piece in sent]):\n",
    "                        sent = []\n",
    "                    else:\n",
    "                        doc.append(sent)\n",
    "                sent = []\n",
    "            elif tag.text != None:\n",
    "                sent.append(tag.text.strip())\n",
    "        if len(doc) > 1:\n",
    "            documents.append(doc)\n",
    "    \n",
    "    return info, documents"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def parse_xml(text_path):\n",
    "    # given a path, find linguistic information given in OANC corpus using helper files\n",
    "    sent_path = text_path[:-4] + '-s.xml'\n",
    "    para_path = text_path[:-4] + '-logical.xml'\n",
    "    text = read_file(text_path)\n",
    "    tree = ET.parse(para_path)\n",
    "    root = tree.getroot()\n",
    "    for child in root:\n",
    "        print(child.tag)\n",
    "        print(child.attrib)\n",
    "        if 'anchors' in child.attrib:\n",
    "            anchors = [int(x) for x in child.attrib['anchors'].split(' ')]\n",
    "            print(text[anchors[0]-1 : anchors[1]])\n",
    "        print()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open American National Corpus (OANC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nature of the OANC documents meant that they had to be handled specially, without much use of the above functions. Some types of files were skipped (the spoken ones were difficult to parse into sentences, and the biomedical files not only had those segmentation issues but were also profoundly technical) and others had differing preprocessing needs that were discovered through some manual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = read_lines('/home/rebekah/Documents/make-it-sound-less-formal/data/discourse_markers/discourse_markers.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/rebekah/Documents/OANC-GrAF/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in glob(path + \"**/*.txt\", recursive=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d95dcb90d3c245c1af7e1a5d60982d3a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_paragraphs = []\n",
    "labels = []\n",
    "\n",
    "for file in tqdm(files):\n",
    "    if 'spoken' in file or 'biomed' in file:\n",
    "        continue\n",
    "    else:\n",
    "        text = read_file(file)\n",
    "        if 'eggan' in file:\n",
    "            text = text.replace(\"\\n\\t\\t\\t\\t\", ' ')\n",
    "            text = text.replace('\\t', '')\n",
    "            text = text.split('\\n')\n",
    "        elif 'slate' in file or 'verbatim' in file:\n",
    "            text = text.split('\\n\\n')\n",
    "            text = [x.replace('\\n', ' ').strip() for x in text]\n",
    "        elif 'verbatim' in file:\n",
    "            text = text.split('\\n\\n')\n",
    "            text = [x.replace('\\n', ' ').strip() for x in text]\n",
    "        elif 'icic' in file:\n",
    "            text = text.split('\\n\\n')\n",
    "            text = [x.replace('\\n', ' ').strip() for x in text]\n",
    "            text = [x.replace('       ', ' ').strip() for x in text]\n",
    "        elif 'OUP' in file:\n",
    "            text = [x.strip() for x in text.split('\\n')]\n",
    "        elif '911report' in file:\n",
    "            text = text.replace('            ', '')\n",
    "            text = text.split('\\n\\n')\n",
    "            text = [x.replace('\\n', ' ').strip() for x in text]\n",
    "            text = [x.replace('     ', ' ').strip() for x in text]\n",
    "        elif 'government' in file:\n",
    "            text = text.split('\\n\\n')\n",
    "            text = [x.replace('\\n', ' ').strip() for x in text]\n",
    "        elif 'plos' in file:\n",
    "            text = text.replace('\\n        ', ' ')\n",
    "            text = text.split('\\n')\n",
    "        elif 'berlitz1' in file:\n",
    "            text = text.replace('\\n        ', ' ')\n",
    "            text = text.split('\\n')\n",
    "        elif 'berlitz2' in file:\n",
    "            text = text.replace('\\n        ', ' ')\n",
    "            text = text.split('\\n')\n",
    "        for par in text:\n",
    "            if par == '':\n",
    "                continue\n",
    "            elif len(par) < 100:\n",
    "                continue\n",
    "            else:\n",
    "                all_paragraphs.append(par)\n",
    "                labels.append(file[49:-4])\n",
    "\n",
    "oanc_df = pd.DataFrame()\n",
    "oanc_df['text'] = all_paragraphs\n",
    "oanc_df['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b33c771b960d4ef4a1c89ecb212c5dc9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339631\n"
     ]
    }
   ],
   "source": [
    "sents = []\n",
    "count = 0\n",
    "for idx, row in tqdm(oanc_df.iterrows(), total = len(oanc_df)):\n",
    "    temp_sents = []\n",
    "    for sent in nltk.sent_tokenize(row['text']):\n",
    "        sent = check_sent(sent)\n",
    "        if sent != '':\n",
    "            temp_sents.append(sent)\n",
    "            count += 1\n",
    "    if len(temp_sents) > 0:\n",
    "        sents.append(temp_sents)\n",
    "    else:\n",
    "        sents.append(np.nan)\n",
    "oanc_df['sents'] = sents\n",
    "oanc_df = oanc_df.dropna()\n",
    "print(count) # number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>Computer layout systems also improve the quali...</td>\n",
       "      <td>non-fiction/OUP/Abernathy/ch8</td>\n",
       "      <td>[Computer layout systems also improve the qual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15792</th>\n",
       "      <td>Tenet, accompanied by his deputy director for ...</td>\n",
       "      <td>technical/911report/chapter-6</td>\n",
       "      <td>[Tenet, accompanied by his deputy director for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32416</th>\n",
       "      <td>The sociologist Nathan Glazer reviewed Slavery...</td>\n",
       "      <td>journal/slate/48/ArticleIP_9089</td>\n",
       "      <td>[The sociologist Nathan Glazer reviewed Slaver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15706</th>\n",
       "      <td>His one-day stopover on March 25, 2000, was th...</td>\n",
       "      <td>technical/911report/chapter-6</td>\n",
       "      <td>[His one-day stopover on March 25, 2000, was t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61409</th>\n",
       "      <td>The coverage emphasizes that the White House c...</td>\n",
       "      <td>journal/slate/7/Article247_821</td>\n",
       "      <td>[The coverage emphasizes that the White House ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10875</th>\n",
       "      <td>Internal Control 7.11 Auditors should obtain a...</td>\n",
       "      <td>technical/government/Gen_Account_Office/Govern...</td>\n",
       "      <td>[Internal Control 7.11 Auditors should obtain ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20306</th>\n",
       "      <td>And there Fish more or less stops. (Well, actu...</td>\n",
       "      <td>journal/slate/19/Article247_4257</td>\n",
       "      <td>[And there Fish more or less stops., Maddening...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64273</th>\n",
       "      <td>The remainder of Chapter 1 is mainly a catalog...</td>\n",
       "      <td>journal/verbatim/VOL15_2</td>\n",
       "      <td>[The remainder of Chapter 1 is mainly a catalo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29733</th>\n",
       "      <td>I love this in part because I am proud that I ...</td>\n",
       "      <td>journal/slate/37/ArticleIP_2561</td>\n",
       "      <td>[I love this in part because I am proud that I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28834</th>\n",
       "      <td>\"That was a miserable year, when I watched a g...</td>\n",
       "      <td>journal/slate/51/ArticleIP_34924</td>\n",
       "      <td>[Who watched whom go from 92 to 38 what?]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "896    Computer layout systems also improve the quali...   \n",
       "15792  Tenet, accompanied by his deputy director for ...   \n",
       "32416  The sociologist Nathan Glazer reviewed Slavery...   \n",
       "15706  His one-day stopover on March 25, 2000, was th...   \n",
       "61409  The coverage emphasizes that the White House c...   \n",
       "10875  Internal Control 7.11 Auditors should obtain a...   \n",
       "20306  And there Fish more or less stops. (Well, actu...   \n",
       "64273  The remainder of Chapter 1 is mainly a catalog...   \n",
       "29733  I love this in part because I am proud that I ...   \n",
       "28834  \"That was a miserable year, when I watched a g...   \n",
       "\n",
       "                                                   label  \\\n",
       "896                        non-fiction/OUP/Abernathy/ch8   \n",
       "15792                      technical/911report/chapter-6   \n",
       "32416                    journal/slate/48/ArticleIP_9089   \n",
       "15706                      technical/911report/chapter-6   \n",
       "61409                     journal/slate/7/Article247_821   \n",
       "10875  technical/government/Gen_Account_Office/Govern...   \n",
       "20306                   journal/slate/19/Article247_4257   \n",
       "64273                           journal/verbatim/VOL15_2   \n",
       "29733                    journal/slate/37/ArticleIP_2561   \n",
       "28834                   journal/slate/51/ArticleIP_34924   \n",
       "\n",
       "                                                   sents  \n",
       "896    [Computer layout systems also improve the qual...  \n",
       "15792  [Tenet, accompanied by his deputy director for...  \n",
       "32416  [The sociologist Nathan Glazer reviewed Slaver...  \n",
       "15706  [His one-day stopover on March 25, 2000, was t...  \n",
       "61409  [The coverage emphasizes that the White House ...  \n",
       "10875  [Internal Control 7.11 Auditors should obtain ...  \n",
       "20306  [And there Fish more or less stops., Maddening...  \n",
       "64273  [The remainder of Chapter 1 is mainly a catalo...  \n",
       "29733  [I love this in part because I am proud that I...  \n",
       "28834          [Who watched whom go from 92 to 38 what?]  "
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oanc_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2953f98cbde7419eacb858832d7cfd89"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'And': 6150, 'Recently': 108, 'First': 990, 'Yet': 759, 'So': 2115, 'Fortunately': 82, 'But': 11108, 'Unfortunately': 256, 'Finally': 457, 'Now': 1264, 'Also': 993, 'Yes': 356, 'Or': 888, 'Oh': 228, 'Well': 534, 'Again': 160, 'Alas': 78, 'Ah': 66, 'Plus': 60, 'Sure': 110, 'Admittedly': 23, 'Basically': 19, 'Hey': 83, 'Heck': 2, 'Seriously': 4, 'Okay': 17, 'Uh': 16, 'Anyway': 86, 'Anyhow': 8, 'Ok': 2})\n"
     ]
    }
   ],
   "source": [
    "terms_dict = defaultdict(int)\n",
    "\n",
    "for idx, row in tqdm(oanc_df.iterrows(), total = len(oanc_df)):\n",
    "    for sent in row.sents:\n",
    "        sent = nltk.word_tokenize(sent)\n",
    "        for item in disc:\n",
    "            if item in sent:\n",
    "                terms_dict[item] += 1\n",
    "\n",
    "print(terms_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Also': 4,\n",
       " 'And': 7,\n",
       " 'But': 8,\n",
       " 'First': 3,\n",
       " 'Now': 5,\n",
       " 'Or': 2,\n",
       " 'So': 6,\n",
       " 'Well': 0,\n",
       " 'Yet': 1}"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_items = sorted(terms_dict.items(), key=lambda x:x[1])[-9:]\n",
    "# take the ones with more than 500 occurrences - even with these this is not enough data\n",
    "\n",
    "chosen_terms = {}\n",
    "idx = 0\n",
    "for item in sorted_items:\n",
    "    chosen_terms[item[0]] = idx\n",
    "    idx += 1\n",
    "chosen_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90fba562928d42c5829b513cefa18f83"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of sent\n",
      "end of sent\n",
      "end of quote\n",
      "end of sent\n",
      "end of sent\n",
      "end of sent\n",
      "end of quote\n",
      "end of sent\n",
      "end of sent\n",
      "end of sent\n",
      "end of quote\n",
      "end of sent\n",
      "end of sent\n",
      "end of quote\n",
      "end of sent\n",
      "end of quote\n",
      "end of quote\n",
      "end of quote\n",
      "end of quote\n",
      "end of sent\n",
      "end of sent\n",
      "end of quote\n",
      "end of quote\n",
      "end of sent\n",
      "end of sent\n",
      "end of sent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "tokenized_par = []\n",
    "vecs_par = []\n",
    "\n",
    "for idx, row in tqdm(oanc_df.iterrows(), total = len(oanc_df)):\n",
    "    tokenized_sents = []\n",
    "    vecs = []\n",
    "    for sent in row.sents:\n",
    "        vec, tokenized_sent = return_vector(sent, chosen_terms)\n",
    "        tokenized_sents.append(tokenized_sent)\n",
    "        vecs.append(vec)\n",
    "    tokenized_par.append(tokenized_sents)\n",
    "    vecs_par.append(vecs)\n",
    "\n",
    "oanc_df['clean_and_tokenized'] = tokenized_par\n",
    "oanc_df['vectors'] = vecs_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>sents</th>\n",
       "      <th>clean_and_tokenized</th>\n",
       "      <th>vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In my three decades of teaching university cou...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[In my three decades of teaching university co...</td>\n",
       "      <td>[[In, my, three, decades, of, teaching, univer...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As a byproduct of those experiences, parents r...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[As a byproduct of those experiences, parents ...</td>\n",
       "      <td>[[As, a, byproduct, of, those, experiences, ,,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>•Bob and Sharon, parents of a 4-year-old: Our ...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[When we looked for a preschool, many programs...</td>\n",
       "      <td>[[When, we, looked, for, a, preschool, ,, many...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>•Angela, mother of a 4-year-old and 6-year-old...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[I’ve read that it’s the quality of time we sp...</td>\n",
       "      <td>[[I, ’, ve, read, that, it, ’, s, the, quality...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>•Talia, mother of a 7-year-old: My son Anselmo...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[His father ﬁrmly insists that he do it by him...</td>\n",
       "      <td>[[His, father, ﬁrmly, insists, that, he, do, i...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>•Noah and Suzanne, parents of a 2-year-old: Wh...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[Recently we read that how children turn out i...</td>\n",
       "      <td>[[Recently, we, read, that, how, children, tur...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Despite being well educated, intent on doing w...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[Despite being well educated, intent on doing ...</td>\n",
       "      <td>[[Despite, being, well, educated, ,, intent, o...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The reasons, I believe, are twofold. First, ra...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[The reasons, I believe, are twofold., First, ...</td>\n",
       "      <td>[[The, reasons, ,, I, believe, ,, are, twofold...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Over the past three decades, external forces i...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[Over the past three decades, external forces ...</td>\n",
       "      <td>[[Over, the, past, three, decades, ,, external...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Although many societal conditions heighten par...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[Although many societal conditions heighten pa...</td>\n",
       "      <td>[[Although, many, societal, conditions, height...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>the problem of child care.  In 1970, 30 percen...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[In 1970, 30 percent of mothers with pre-schoo...</td>\n",
       "      <td>[[In, 1970, ,, 30, percent, of, mothers, with,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Without a nationally regulated and generously ...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[Without a nationally regulated and generously...</td>\n",
       "      <td>[[Without, a, nationally, regulated, and, gene...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>the “time bind.”  Like many parents, Angela, w...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[When Angela and her husband, Tom, walk throug...</td>\n",
       "      <td>[[When, Angela, and, her, husband, ,, Tom, ,, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Angela and Tom represent a growing number of A...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[Angela and Tom represent a growing number of ...</td>\n",
       "      <td>[[Angela, and, Tom, represent, a, growing, num...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The expression “quality time” dates back to th...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[The expression “quality time” dates back to t...</td>\n",
       "      <td>[[The, expression, “, quality, time, ”, dates,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A close look at the research reveals that chil...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[A close look at the research reveals that chi...</td>\n",
       "      <td>[[A, close, look, at, the, research, reveals, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>In Angela and Tom’s case, sandwiching concentr...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[In Angela and Tom’s case, sandwiching concent...</td>\n",
       "      <td>[[In, Angela, and, Tom, ’, s, case, ,, sandwic...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Furthermore, the “time bind” stiﬂes an essenti...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[Furthermore, the “time bind” stiﬂes an essent...</td>\n",
       "      <td>[[Furthermore, ,, the, “, time, bind, ”, stiﬂe...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>In a recent provocative study, sociologist Arl...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[In a recent provocative study, sociologist Ar...</td>\n",
       "      <td>[[In, a, recent, provocative, study, ,, sociol...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>As homes become frenzied places in which work ...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[As homes become frenzied places in which work...</td>\n",
       "      <td>[[As, homes, become, frenzied, places, in, whi...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Fortunately, not all reports are as disturbing...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[Fortunately, not all reports are as disturbin...</td>\n",
       "      <td>[[Fortunately, ,, not, all, reports, are, as, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Although the precise extent of family–work con...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[Although the precise extent of family–work co...</td>\n",
       "      <td>[[Although, the, precise, extent, of, family–w...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Furthermore, long hours in child care during i...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[Furthermore, long hours in child care during ...</td>\n",
       "      <td>[[Furthermore, ,, long, hours, in, child, care...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>These ﬁndings are not an indictment of materna...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[These ﬁndings are not an indictment of matern...</td>\n",
       "      <td>[[These, ﬁndings, are, not, an, indictment, of...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Employed mothers of cognitively competent, wel...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[Employed mothers of cognitively competent, we...</td>\n",
       "      <td>[[Employed, mothers, of, cognitively, competen...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Fathers’ involvement in child rearing is an ad...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[Fathers’ involvement in child rearing is an a...</td>\n",
       "      <td>[[Fathers, ’, involvement, in, child, rearing,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>In sum, increasingly pressured adult lives hav...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[In sum, increasingly pressured adult lives ha...</td>\n",
       "      <td>[[In, sum, ,, increasingly, pressured, adult, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Probably because it reduces work overload, par...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[Probably because it reduces work overload, pa...</td>\n",
       "      <td>[[Probably, because, it, reduces, work, overlo...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Almost all parents—especially ﬁrst-time parent...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[Almost all parents—especially ﬁrst-time paren...</td>\n",
       "      <td>[[Almost, all, parents—especially, ﬁrst-time, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>The call for parenting advice has led to a pro...</td>\n",
       "      <td>non-fiction/OUP/Berk/ch1</td>\n",
       "      <td>[The call for parenting advice has led to a pr...</td>\n",
       "      <td>[[The, call, for, parenting, advice, has, led,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65097</th>\n",
       "      <td>Scots Gaelic dictionaries are heavily dependen...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[Scots Gaelic dictionaries are heavily depende...</td>\n",
       "      <td>[[Scots, Gaelic, dictionaries, are, heavily, d...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65098</th>\n",
       "      <td>The second dictionary, by MacLennan, is two-wa...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[The second dictionary, by MacLennan, is two-w...</td>\n",
       "      <td>[[The, second, dictionary, ,, by, MacLennan, ,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65099</th>\n",
       "      <td>Using Dwelly, MacLennan, and a list of plant n...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[Using Dwelly, MacLennan, and a list of plant ...</td>\n",
       "      <td>[[Using, Dwelly, ,, MacLennan, ,, and, a, list...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65100</th>\n",
       "      <td>GAELIC                 ENGLISH              LA...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[GAELIC                 ENGLISH              L...</td>\n",
       "      <td>[[GAELIC, ENGLISH, LATIN, fraoch, commom, heat...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65101</th>\n",
       "      <td>Dwelly also gives fraoch-an-ruinnse for the cr...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[Dwelly also gives fraoch-an-ruinnse for the c...</td>\n",
       "      <td>[[Dwelly, also, gives, fraoch-an-ruinnse, for,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65102</th>\n",
       "      <td>From W.A.K. Johnston we learn that fraochdearg...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[From W.A.K., Johnston we learn that fraochdea...</td>\n",
       "      <td>[[From, W.A.K, .], [Johnston, we, learn, that,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65103</th>\n",
       "      <td>Dwelly also gives fraoch nam curra bhitheag wi...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[Dwelly also gives fraoch nam curra bhitheag w...</td>\n",
       "      <td>[[Dwelly, also, gives, fraoch, nam, curra, bhi...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65104</th>\n",
       "      <td>Dwelly says, “See fraochan,” But fraochan can ...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[Dwelly says, “See fraochan,” But fraochan can...</td>\n",
       "      <td>[[Dwelly, says, ,, “, See, fraochan, ,, ”, Fra...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65105</th>\n",
       "      <td>Fraoch itself has other meanings which must go...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[Fraoch itself has other meanings which must g...</td>\n",
       "      <td>[[Fraoch, itself, has, other, meanings, which,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65106</th>\n",
       "      <td>So, golfers, the next time you are in Ireland ...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[So, golfers, the next time you are in Ireland...</td>\n",
       "      <td>[[Golfers, ,, the, next, time, you, are, in, I...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65107</th>\n",
       "      <td>I was surprised and dismayed at the content of...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[I was surprised and dismayed at the content o...</td>\n",
       "      <td>[[I, was, surprised, and, dismayed, at, the, c...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65108</th>\n",
       "      <td>If Mr. Mason is so distressed at the state of ...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[If Mr. Mason is so distressed at the state of...</td>\n",
       "      <td>[[If, Mr., Mason, is, so, distressed, at, the,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65109</th>\n",
       "      <td>Mr. Mason may find a crumb of comfort in reali...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[Mr. Mason may find a crumb of comfort in real...</td>\n",
       "      <td>[[Mr., Mason, may, find, a, crumb, of, comfort...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65110</th>\n",
       "      <td>Amy Stoller's letter [XVII,1] identifies Jerry...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[Amy Stoller's letter [XVII,1] identifies Jerr...</td>\n",
       "      <td>[[Amy, Stoller, 's, letter, [, XVII,1, ], iden...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65111</th>\n",
       "      <td>“So this,” I thought, “is a fire.  This is wha...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[This is what it is like to have the house on ...</td>\n",
       "      <td>[[This, is, what, it, is, like, to, have, the,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65112</th>\n",
       "      <td>“that last disappointment....  So this is the ...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[So this is the great experience--well, what o...</td>\n",
       "      <td>[[This, is, the, great, experience, --, well, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65113</th>\n",
       "      <td>VERBATIM, as we know, is about language.  It i...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[VERBATIM, as we know, is about language., It ...</td>\n",
       "      <td>[[VERBATIM, ,, as, we, know, ,, is, about, lan...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65114</th>\n",
       "      <td>It must be seen that there are many, many diff...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[It must be seen that there are many, many dif...</td>\n",
       "      <td>[[It, must, be, seen, that, there, are, many, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65115</th>\n",
       "      <td>I often question whether I am a writer.  If a ...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[I often question whether I am a writer., If a...</td>\n",
       "      <td>[[I, often, question, whether, I, am, a, write...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65116</th>\n",
       "      <td>My sentiments about my own writing alter rapid...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[My sentiments about my own writing alter rapi...</td>\n",
       "      <td>[[My, sentiments, about, my, own, writing, alt...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65117</th>\n",
       "      <td>There are fourteen essays in the collection, a...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[There are fourteen essays in the collection, ...</td>\n",
       "      <td>[[There, are, fourteen, essays, in, the, colle...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65118</th>\n",
       "      <td>Following this epicene epiphany of ephebic ero...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[Following this epicene epiphany of ephebic er...</td>\n",
       "      <td>[[Following, this, epicene, epiphany, of, ephe...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65119</th>\n",
       "      <td>One gets the distinct impression that Ober enj...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[One gets the distinct impression that Ober en...</td>\n",
       "      <td>[[One, gets, the, distinct, impression, that, ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65120</th>\n",
       "      <td>This seems an appropriate point to insert a pe...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[This seems an appropriate point to insert a p...</td>\n",
       "      <td>[[This, seems, an, appropriate, point, to, ins...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65121</th>\n",
       "      <td>As the reader can tell, both from these commen...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[As the reader can tell, both from these comme...</td>\n",
       "      <td>[[As, the, reader, can, tell, ,, both, from, t...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65122</th>\n",
       "      <td>If I find the space to treat this subject agai...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[If I find the space to treat this subject aga...</td>\n",
       "      <td>[[If, I, find, the, space, to, treat, this, su...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65123</th>\n",
       "      <td>“The family said they would try to bury him ag...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[Submitted by ]]</td>\n",
       "      <td>[[Submitted, by, ]]]</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65124</th>\n",
       "      <td>“Attractive, divorced Jewish woman 41.  Reuben...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[Reubenesque, professional., Submitted by  of ...</td>\n",
       "      <td>[[Reubenesque, ,, professional, .], [Submitted...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65125</th>\n",
       "      <td>“(The cyclist) hopes to survive the 2,020-mile...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[Submitted by ]                               ...</td>\n",
       "      <td>[[Submitted, by, ], “, Your, thumb, or, finger...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65126</th>\n",
       "      <td>“After much adieu, the TC by Masarati Sports C...</td>\n",
       "      <td>journal/verbatim/VOL17_2</td>\n",
       "      <td>[Submitted by ]]</td>\n",
       "      <td>[[Submitted, by, ]]]</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65127 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      In my three decades of teaching university cou...   \n",
       "1      As a byproduct of those experiences, parents r...   \n",
       "2      •Bob and Sharon, parents of a 4-year-old: Our ...   \n",
       "3      •Angela, mother of a 4-year-old and 6-year-old...   \n",
       "4      •Talia, mother of a 7-year-old: My son Anselmo...   \n",
       "5      •Noah and Suzanne, parents of a 2-year-old: Wh...   \n",
       "6      Despite being well educated, intent on doing w...   \n",
       "7      The reasons, I believe, are twofold. First, ra...   \n",
       "8      Over the past three decades, external forces i...   \n",
       "9      Although many societal conditions heighten par...   \n",
       "10     the problem of child care.  In 1970, 30 percen...   \n",
       "11     Without a nationally regulated and generously ...   \n",
       "12     the “time bind.”  Like many parents, Angela, w...   \n",
       "13     Angela and Tom represent a growing number of A...   \n",
       "14     The expression “quality time” dates back to th...   \n",
       "15     A close look at the research reveals that chil...   \n",
       "16     In Angela and Tom’s case, sandwiching concentr...   \n",
       "17     Furthermore, the “time bind” stiﬂes an essenti...   \n",
       "18     In a recent provocative study, sociologist Arl...   \n",
       "19     As homes become frenzied places in which work ...   \n",
       "20     Fortunately, not all reports are as disturbing...   \n",
       "21     Although the precise extent of family–work con...   \n",
       "22     Furthermore, long hours in child care during i...   \n",
       "23     These ﬁndings are not an indictment of materna...   \n",
       "24     Employed mothers of cognitively competent, wel...   \n",
       "25     Fathers’ involvement in child rearing is an ad...   \n",
       "26     In sum, increasingly pressured adult lives hav...   \n",
       "27     Probably because it reduces work overload, par...   \n",
       "28     Almost all parents—especially ﬁrst-time parent...   \n",
       "29     The call for parenting advice has led to a pro...   \n",
       "...                                                  ...   \n",
       "65097  Scots Gaelic dictionaries are heavily dependen...   \n",
       "65098  The second dictionary, by MacLennan, is two-wa...   \n",
       "65099  Using Dwelly, MacLennan, and a list of plant n...   \n",
       "65100  GAELIC                 ENGLISH              LA...   \n",
       "65101  Dwelly also gives fraoch-an-ruinnse for the cr...   \n",
       "65102  From W.A.K. Johnston we learn that fraochdearg...   \n",
       "65103  Dwelly also gives fraoch nam curra bhitheag wi...   \n",
       "65104  Dwelly says, “See fraochan,” But fraochan can ...   \n",
       "65105  Fraoch itself has other meanings which must go...   \n",
       "65106  So, golfers, the next time you are in Ireland ...   \n",
       "65107  I was surprised and dismayed at the content of...   \n",
       "65108  If Mr. Mason is so distressed at the state of ...   \n",
       "65109  Mr. Mason may find a crumb of comfort in reali...   \n",
       "65110  Amy Stoller's letter [XVII,1] identifies Jerry...   \n",
       "65111  “So this,” I thought, “is a fire.  This is wha...   \n",
       "65112  “that last disappointment....  So this is the ...   \n",
       "65113  VERBATIM, as we know, is about language.  It i...   \n",
       "65114  It must be seen that there are many, many diff...   \n",
       "65115  I often question whether I am a writer.  If a ...   \n",
       "65116  My sentiments about my own writing alter rapid...   \n",
       "65117  There are fourteen essays in the collection, a...   \n",
       "65118  Following this epicene epiphany of ephebic ero...   \n",
       "65119  One gets the distinct impression that Ober enj...   \n",
       "65120  This seems an appropriate point to insert a pe...   \n",
       "65121  As the reader can tell, both from these commen...   \n",
       "65122  If I find the space to treat this subject agai...   \n",
       "65123  “The family said they would try to bury him ag...   \n",
       "65124  “Attractive, divorced Jewish woman 41.  Reuben...   \n",
       "65125  “(The cyclist) hopes to survive the 2,020-mile...   \n",
       "65126  “After much adieu, the TC by Masarati Sports C...   \n",
       "\n",
       "                          label  \\\n",
       "0      non-fiction/OUP/Berk/ch1   \n",
       "1      non-fiction/OUP/Berk/ch1   \n",
       "2      non-fiction/OUP/Berk/ch1   \n",
       "3      non-fiction/OUP/Berk/ch1   \n",
       "4      non-fiction/OUP/Berk/ch1   \n",
       "5      non-fiction/OUP/Berk/ch1   \n",
       "6      non-fiction/OUP/Berk/ch1   \n",
       "7      non-fiction/OUP/Berk/ch1   \n",
       "8      non-fiction/OUP/Berk/ch1   \n",
       "9      non-fiction/OUP/Berk/ch1   \n",
       "10     non-fiction/OUP/Berk/ch1   \n",
       "11     non-fiction/OUP/Berk/ch1   \n",
       "12     non-fiction/OUP/Berk/ch1   \n",
       "13     non-fiction/OUP/Berk/ch1   \n",
       "14     non-fiction/OUP/Berk/ch1   \n",
       "15     non-fiction/OUP/Berk/ch1   \n",
       "16     non-fiction/OUP/Berk/ch1   \n",
       "17     non-fiction/OUP/Berk/ch1   \n",
       "18     non-fiction/OUP/Berk/ch1   \n",
       "19     non-fiction/OUP/Berk/ch1   \n",
       "20     non-fiction/OUP/Berk/ch1   \n",
       "21     non-fiction/OUP/Berk/ch1   \n",
       "22     non-fiction/OUP/Berk/ch1   \n",
       "23     non-fiction/OUP/Berk/ch1   \n",
       "24     non-fiction/OUP/Berk/ch1   \n",
       "25     non-fiction/OUP/Berk/ch1   \n",
       "26     non-fiction/OUP/Berk/ch1   \n",
       "27     non-fiction/OUP/Berk/ch1   \n",
       "28     non-fiction/OUP/Berk/ch1   \n",
       "29     non-fiction/OUP/Berk/ch1   \n",
       "...                         ...   \n",
       "65097  journal/verbatim/VOL17_2   \n",
       "65098  journal/verbatim/VOL17_2   \n",
       "65099  journal/verbatim/VOL17_2   \n",
       "65100  journal/verbatim/VOL17_2   \n",
       "65101  journal/verbatim/VOL17_2   \n",
       "65102  journal/verbatim/VOL17_2   \n",
       "65103  journal/verbatim/VOL17_2   \n",
       "65104  journal/verbatim/VOL17_2   \n",
       "65105  journal/verbatim/VOL17_2   \n",
       "65106  journal/verbatim/VOL17_2   \n",
       "65107  journal/verbatim/VOL17_2   \n",
       "65108  journal/verbatim/VOL17_2   \n",
       "65109  journal/verbatim/VOL17_2   \n",
       "65110  journal/verbatim/VOL17_2   \n",
       "65111  journal/verbatim/VOL17_2   \n",
       "65112  journal/verbatim/VOL17_2   \n",
       "65113  journal/verbatim/VOL17_2   \n",
       "65114  journal/verbatim/VOL17_2   \n",
       "65115  journal/verbatim/VOL17_2   \n",
       "65116  journal/verbatim/VOL17_2   \n",
       "65117  journal/verbatim/VOL17_2   \n",
       "65118  journal/verbatim/VOL17_2   \n",
       "65119  journal/verbatim/VOL17_2   \n",
       "65120  journal/verbatim/VOL17_2   \n",
       "65121  journal/verbatim/VOL17_2   \n",
       "65122  journal/verbatim/VOL17_2   \n",
       "65123  journal/verbatim/VOL17_2   \n",
       "65124  journal/verbatim/VOL17_2   \n",
       "65125  journal/verbatim/VOL17_2   \n",
       "65126  journal/verbatim/VOL17_2   \n",
       "\n",
       "                                                   sents  \\\n",
       "0      [In my three decades of teaching university co...   \n",
       "1      [As a byproduct of those experiences, parents ...   \n",
       "2      [When we looked for a preschool, many programs...   \n",
       "3      [I’ve read that it’s the quality of time we sp...   \n",
       "4      [His father ﬁrmly insists that he do it by him...   \n",
       "5      [Recently we read that how children turn out i...   \n",
       "6      [Despite being well educated, intent on doing ...   \n",
       "7      [The reasons, I believe, are twofold., First, ...   \n",
       "8      [Over the past three decades, external forces ...   \n",
       "9      [Although many societal conditions heighten pa...   \n",
       "10     [In 1970, 30 percent of mothers with pre-schoo...   \n",
       "11     [Without a nationally regulated and generously...   \n",
       "12     [When Angela and her husband, Tom, walk throug...   \n",
       "13     [Angela and Tom represent a growing number of ...   \n",
       "14     [The expression “quality time” dates back to t...   \n",
       "15     [A close look at the research reveals that chi...   \n",
       "16     [In Angela and Tom’s case, sandwiching concent...   \n",
       "17     [Furthermore, the “time bind” stiﬂes an essent...   \n",
       "18     [In a recent provocative study, sociologist Ar...   \n",
       "19     [As homes become frenzied places in which work...   \n",
       "20     [Fortunately, not all reports are as disturbin...   \n",
       "21     [Although the precise extent of family–work co...   \n",
       "22     [Furthermore, long hours in child care during ...   \n",
       "23     [These ﬁndings are not an indictment of matern...   \n",
       "24     [Employed mothers of cognitively competent, we...   \n",
       "25     [Fathers’ involvement in child rearing is an a...   \n",
       "26     [In sum, increasingly pressured adult lives ha...   \n",
       "27     [Probably because it reduces work overload, pa...   \n",
       "28     [Almost all parents—especially ﬁrst-time paren...   \n",
       "29     [The call for parenting advice has led to a pr...   \n",
       "...                                                  ...   \n",
       "65097  [Scots Gaelic dictionaries are heavily depende...   \n",
       "65098  [The second dictionary, by MacLennan, is two-w...   \n",
       "65099  [Using Dwelly, MacLennan, and a list of plant ...   \n",
       "65100  [GAELIC                 ENGLISH              L...   \n",
       "65101  [Dwelly also gives fraoch-an-ruinnse for the c...   \n",
       "65102  [From W.A.K., Johnston we learn that fraochdea...   \n",
       "65103  [Dwelly also gives fraoch nam curra bhitheag w...   \n",
       "65104  [Dwelly says, “See fraochan,” But fraochan can...   \n",
       "65105  [Fraoch itself has other meanings which must g...   \n",
       "65106  [So, golfers, the next time you are in Ireland...   \n",
       "65107  [I was surprised and dismayed at the content o...   \n",
       "65108  [If Mr. Mason is so distressed at the state of...   \n",
       "65109  [Mr. Mason may find a crumb of comfort in real...   \n",
       "65110  [Amy Stoller's letter [XVII,1] identifies Jerr...   \n",
       "65111  [This is what it is like to have the house on ...   \n",
       "65112  [So this is the great experience--well, what o...   \n",
       "65113  [VERBATIM, as we know, is about language., It ...   \n",
       "65114  [It must be seen that there are many, many dif...   \n",
       "65115  [I often question whether I am a writer., If a...   \n",
       "65116  [My sentiments about my own writing alter rapi...   \n",
       "65117  [There are fourteen essays in the collection, ...   \n",
       "65118  [Following this epicene epiphany of ephebic er...   \n",
       "65119  [One gets the distinct impression that Ober en...   \n",
       "65120  [This seems an appropriate point to insert a p...   \n",
       "65121  [As the reader can tell, both from these comme...   \n",
       "65122  [If I find the space to treat this subject aga...   \n",
       "65123                                   [Submitted by ]]   \n",
       "65124  [Reubenesque, professional., Submitted by  of ...   \n",
       "65125  [Submitted by ]                               ...   \n",
       "65126                                   [Submitted by ]]   \n",
       "\n",
       "                                     clean_and_tokenized  \\\n",
       "0      [[In, my, three, decades, of, teaching, univer...   \n",
       "1      [[As, a, byproduct, of, those, experiences, ,,...   \n",
       "2      [[When, we, looked, for, a, preschool, ,, many...   \n",
       "3      [[I, ’, ve, read, that, it, ’, s, the, quality...   \n",
       "4      [[His, father, ﬁrmly, insists, that, he, do, i...   \n",
       "5      [[Recently, we, read, that, how, children, tur...   \n",
       "6      [[Despite, being, well, educated, ,, intent, o...   \n",
       "7      [[The, reasons, ,, I, believe, ,, are, twofold...   \n",
       "8      [[Over, the, past, three, decades, ,, external...   \n",
       "9      [[Although, many, societal, conditions, height...   \n",
       "10     [[In, 1970, ,, 30, percent, of, mothers, with,...   \n",
       "11     [[Without, a, nationally, regulated, and, gene...   \n",
       "12     [[When, Angela, and, her, husband, ,, Tom, ,, ...   \n",
       "13     [[Angela, and, Tom, represent, a, growing, num...   \n",
       "14     [[The, expression, “, quality, time, ”, dates,...   \n",
       "15     [[A, close, look, at, the, research, reveals, ...   \n",
       "16     [[In, Angela, and, Tom, ’, s, case, ,, sandwic...   \n",
       "17     [[Furthermore, ,, the, “, time, bind, ”, stiﬂe...   \n",
       "18     [[In, a, recent, provocative, study, ,, sociol...   \n",
       "19     [[As, homes, become, frenzied, places, in, whi...   \n",
       "20     [[Fortunately, ,, not, all, reports, are, as, ...   \n",
       "21     [[Although, the, precise, extent, of, family–w...   \n",
       "22     [[Furthermore, ,, long, hours, in, child, care...   \n",
       "23     [[These, ﬁndings, are, not, an, indictment, of...   \n",
       "24     [[Employed, mothers, of, cognitively, competen...   \n",
       "25     [[Fathers, ’, involvement, in, child, rearing,...   \n",
       "26     [[In, sum, ,, increasingly, pressured, adult, ...   \n",
       "27     [[Probably, because, it, reduces, work, overlo...   \n",
       "28     [[Almost, all, parents—especially, ﬁrst-time, ...   \n",
       "29     [[The, call, for, parenting, advice, has, led,...   \n",
       "...                                                  ...   \n",
       "65097  [[Scots, Gaelic, dictionaries, are, heavily, d...   \n",
       "65098  [[The, second, dictionary, ,, by, MacLennan, ,...   \n",
       "65099  [[Using, Dwelly, ,, MacLennan, ,, and, a, list...   \n",
       "65100  [[GAELIC, ENGLISH, LATIN, fraoch, commom, heat...   \n",
       "65101  [[Dwelly, also, gives, fraoch-an-ruinnse, for,...   \n",
       "65102  [[From, W.A.K, .], [Johnston, we, learn, that,...   \n",
       "65103  [[Dwelly, also, gives, fraoch, nam, curra, bhi...   \n",
       "65104  [[Dwelly, says, ,, “, See, fraochan, ,, ”, Fra...   \n",
       "65105  [[Fraoch, itself, has, other, meanings, which,...   \n",
       "65106  [[Golfers, ,, the, next, time, you, are, in, I...   \n",
       "65107  [[I, was, surprised, and, dismayed, at, the, c...   \n",
       "65108  [[If, Mr., Mason, is, so, distressed, at, the,...   \n",
       "65109  [[Mr., Mason, may, find, a, crumb, of, comfort...   \n",
       "65110  [[Amy, Stoller, 's, letter, [, XVII,1, ], iden...   \n",
       "65111  [[This, is, what, it, is, like, to, have, the,...   \n",
       "65112  [[This, is, the, great, experience, --, well, ...   \n",
       "65113  [[VERBATIM, ,, as, we, know, ,, is, about, lan...   \n",
       "65114  [[It, must, be, seen, that, there, are, many, ...   \n",
       "65115  [[I, often, question, whether, I, am, a, write...   \n",
       "65116  [[My, sentiments, about, my, own, writing, alt...   \n",
       "65117  [[There, are, fourteen, essays, in, the, colle...   \n",
       "65118  [[Following, this, epicene, epiphany, of, ephe...   \n",
       "65119  [[One, gets, the, distinct, impression, that, ...   \n",
       "65120  [[This, seems, an, appropriate, point, to, ins...   \n",
       "65121  [[As, the, reader, can, tell, ,, both, from, t...   \n",
       "65122  [[If, I, find, the, space, to, treat, this, su...   \n",
       "65123                               [[Submitted, by, ]]]   \n",
       "65124  [[Reubenesque, ,, professional, .], [Submitted...   \n",
       "65125  [[Submitted, by, ], “, Your, thumb, or, finger...   \n",
       "65126                               [[Submitted, by, ]]]   \n",
       "\n",
       "                                                 vectors  \n",
       "0      [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "1      [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "2      [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "3      [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "4      [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "5      [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "6      [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "7      [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, ...  \n",
       "8      [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "9      [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "10     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "11     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "12     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "13     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "14     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "15     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "16     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "17     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "18     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "19     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "20     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "21     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "22     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "23     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "24     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "25     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "26     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "27     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, ...  \n",
       "28     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "29     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "...                                                  ...  \n",
       "65097  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "65098  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "65099  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "65100                   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]  \n",
       "65101  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "65102  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "65103  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "65104                   [[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]  \n",
       "65105  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "65106  [[0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "65107  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "65108  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "65109  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "65110  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "65111  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "65112  [[0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "65113  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "65114  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "65115  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "65116  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "65117  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "65118                   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]  \n",
       "65119  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "65120  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "65121  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "65122                   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]  \n",
       "65123                   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]  \n",
       "65124  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "65125  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, ...  \n",
       "65126                   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]  \n",
       "\n",
       "[65127 rows x 5 columns]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oanc_df = oanc_df.dropna()\n",
    "oanc_df = oanc_df.reset_index(drop = True)\n",
    "oanc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "oanc_df.to_pickle('data/discourse_markers/oanc_df.zip')\n",
    "\n",
    "with open('oanc_terms.pkl', 'wb') as f:\n",
    "    pickle.dump(chosen_terms, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# British National Corpus (BNC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BNC, while easier to handle than the OANC, still had some specialized preprocessing and segmentation issues to be handled. Here I extract the data, clean it and put it into a DataFrame for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/discourse_markers/oanc_terms.pkl', 'rb') as f:\n",
    "    terms_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/rebekah/Documents/BNC/Texts/'\n",
    "files = [f for f in glob(path + \"**/*.xml\", recursive=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dbe0746b00444c1bdd3fe3cff6342b5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: no change possible\n",
      "['Well', '!']\n",
      "ERROR: end of sent\n",
      "['Well', ',', '.']\n",
      "ERROR: no change possible\n",
      "['Well', '!']\n",
      "ERROR: no change possible\n",
      "['Yes', '.', 'So']\n",
      "ERROR: no change possible\n",
      "['Ah', '!', 'Well']\n",
      "ERROR: no change possible\n",
      "['Yeah', '.', 'Well']\n",
      "ERROR: end of sent\n",
      "['And', '.']\n",
      "ERROR: no change possible\n",
      "['So', '?']\n",
      "ERROR: no change possible\n",
      "['Ha', '!', 'Well']\n",
      "ERROR: end of sent\n",
      "['Well', '.']\n",
      "ERROR: end of sent\n",
      "['But', '.']\n",
      "ERROR: end of sent\n",
      "['So', '.']\n",
      "ERROR: end of sent\n",
      "['Now', '.']\n",
      "ERROR: no change possible\n",
      "['Right', '.', 'And']\n",
      "ERROR: no change possible\n",
      "['on', '.', 'And']\n",
      "ERROR: no change possible\n",
      "['No', '!', 'Well']\n",
      "ERROR: end of sent\n",
      "['Now', '.']\n",
      "ERROR: no change possible\n",
      "['So', '?']\n",
      "ERROR: no change possible\n",
      "['Or', '?']\n",
      "ERROR: no change possible\n",
      "['And', '?']\n",
      "ERROR: end of sent\n",
      "['So', '.']\n",
      "ERROR: end of sent\n",
      "['So', '.']\n",
      "ERROR: end of sent\n",
      "['And', '.']\n",
      "ERROR: no change possible\n",
      "['birthday', '.', 'Now']\n",
      "ERROR: no change possible\n",
      "['Liver', '.', 'And']\n",
      "ERROR: no change possible\n",
      "['Ah', '.', 'So']\n",
      "ERROR: no change possible\n",
      "['Well']\n",
      "ERROR: no change possible\n",
      "['Yeah', '!', 'So']\n",
      "ERROR: no change possible\n",
      "['Well']\n",
      "ERROR: end of sent\n",
      "['And', '.']\n",
      "ERROR: no change possible\n",
      "['Mm', '.', 'Well']\n",
      "ERROR: no change possible\n",
      "['Well', '!']\n",
      "ERROR: no change possible\n",
      "['Well', ',']\n",
      "ERROR: end of sent\n",
      "['So', '.']\n",
      "ERROR: no change possible\n",
      "['Beauty', 'ravaged', 'And']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '?']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '?']\n",
      "ERROR: end of sent\n",
      "['‘', 'Well', '.']\n",
      "ERROR: no change possible\n",
      "['Now', ',']\n",
      "ERROR: end of sent\n",
      "['Now', '.']\n",
      "ERROR: no change possible\n",
      "['mhm', 'So']\n",
      "ERROR: end of sent\n",
      "['Well', '.']\n",
      "ERROR: end of sent\n",
      "['Well', '.']\n",
      "ERROR: no change possible\n",
      "['So', '?']\n",
      "ERROR: no change possible\n",
      "['Now']\n",
      "ERROR: no change possible\n",
      "['Spelling', '.', 'Now']\n",
      "ERROR: no change possible\n",
      "['And', '?']\n",
      "ERROR: end of sent\n",
      "['‘', 'Well', '.']\n",
      "ERROR: end of sent\n",
      "['Now', '.']\n",
      "ERROR: end of sent\n",
      "['‘', 'Well', '.']\n",
      "ERROR: no change possible\n",
      "['Well', '!']\n",
      "ERROR: end of sent\n",
      "['‘', 'Well', '.', '’']\n",
      "ERROR: end of sent\n",
      "['‘', 'Well', '.', '’']\n",
      "ERROR: end of sent\n",
      "['‘', 'Well', '.', '’']\n",
      "ERROR: end of sent\n",
      "['Well', '.']\n",
      "ERROR: no change possible\n",
      "['‘', 'So', '?', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '?']\n",
      "ERROR: end of sent\n",
      "['Now', '.']\n",
      "ERROR: no change possible\n",
      "['Now', '!']\n",
      "ERROR: end of sent\n",
      "['But', '…', '.']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '?', '’']\n",
      "ERROR: end of sent\n",
      "['So', '.']\n",
      "ERROR: end of sent\n",
      "['‘', 'Well', '.']\n",
      "ERROR: no change possible\n",
      "['‘', 'Now', '!']\n",
      "ERROR: no change possible\n",
      "['‘', 'And', '?', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'But', '—', '‘']\n",
      "ERROR: end of sent\n",
      "['‘', 'So', '.']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '?', '’']\n",
      "ERROR: no change possible\n",
      "['But', '…', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '?', '‘']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '?', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'So', '?']\n",
      "ERROR: no change possible\n",
      "['‘', 'Now', '!']\n",
      "ERROR: no change possible\n",
      "['‘', 'And', '?', '’']\n",
      "ERROR: no change possible\n",
      "['Well', '—', '‘']\n",
      "ERROR: end of sent\n",
      "['Yet', '.']\n",
      "ERROR: end of sent\n",
      "['Yet', '.']\n",
      "ERROR: end of sent\n",
      "['Yet', '.']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '?']\n",
      "ERROR: end of sent\n",
      "['Now', '.']\n",
      "ERROR: end of sent\n",
      "['Now', '.']\n",
      "ERROR: no change possible\n",
      "['‘', 'But', '—', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'And', '?', '’']\n",
      "ERROR: no change possible\n",
      "['Well', '?']\n",
      "ERROR: no change possible\n",
      "['‘', 'So', '?', '’', '/']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '?', '’']\n",
      "ERROR: end of sent\n",
      "['‘', 'So', '.']\n",
      "ERROR: end of sent\n",
      "['‘', 'Well', '.']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '—', '‘']\n",
      "ERROR: end of sent\n",
      "['Now', '.']\n",
      "ERROR: no change possible\n",
      "['‘', 'But', '?', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'So', '?', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'So', '?', '’']\n",
      "ERROR: end of quote\n",
      "['\"', 'Well', '?', '\"']\n",
      "ERROR: end of sent\n",
      "['``', 'Or', '.']\n",
      "ERROR: end of quote\n",
      "['\"', 'Well', '?', '\"']\n",
      "ERROR: no change possible\n",
      "['‘', 'And', '?', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'And', '?', '’']\n",
      "ERROR: no change possible\n",
      "['Or', '…']\n",
      "ERROR: no change possible\n",
      "['‘', 'So', '?', '’']\n",
      "ERROR: no change possible\n",
      "['Or', '…']\n",
      "ERROR: no change possible\n",
      "['‘', 'But', '—', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '?']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '?']\n",
      "ERROR: no change possible\n",
      "['But', ':']\n",
      "ERROR: end of sent\n",
      "['Now', '.']\n",
      "ERROR: no change possible\n",
      "['‘', 'But', '—', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '?', '’']\n",
      "ERROR: end of sent\n",
      "['‘', 'So', '.']\n",
      "ERROR: no change possible\n",
      "['‘', 'So', '?']\n",
      "ERROR: end of sent\n",
      "['Yet', '.']\n",
      "ERROR: end of sent\n",
      "['‘', 'Now', '.']\n",
      "ERROR: end of sent\n",
      "['‘', 'Now', '.']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '?']\n",
      "ERROR: no change possible\n",
      "['‘', 'And', '?', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '?']\n",
      "ERROR: no change possible\n",
      "['‘', 'And', '?', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '?', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '?']\n",
      "ERROR: no change possible\n",
      "['‘', 'So', '?', '’']\n",
      "ERROR: no change possible\n",
      "['Or']\n",
      "ERROR: no change possible\n",
      "['‘', 'But', '—', '’']\n",
      "ERROR: no change possible\n",
      "['But', '…']\n",
      "ERROR: end of sent\n",
      "['Well', '.']\n",
      "ERROR: end of sent\n",
      "['Now', '.']\n",
      "ERROR: no change possible\n",
      "['So', '?']\n",
      "ERROR: no change possible\n",
      "['Now', '!']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '—']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '!', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '?', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '?', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '?', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'So', '?', '’']\n",
      "ERROR: end of sent\n",
      "['And', '.']\n",
      "ERROR: no change possible\n",
      "['‘', 'And', '?', '’']\n",
      "ERROR: no change possible\n",
      "['First', '8,000']\n",
      "ERROR: end of sent\n",
      "['‘', 'So', '.']\n",
      "ERROR: no change possible\n",
      "['‘', 'So', '?', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'So', '?', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '?']\n",
      "ERROR: no change possible\n",
      "['‘', 'So', '?', '’']\n",
      "ERROR: end of quote\n",
      "['\"', 'Well', '?', '\"', 'said', 'Gerald', '.']\n",
      "ERROR: end of sent\n",
      "['So', '.']\n",
      "ERROR: no change possible\n",
      "['Well', '?', '``']\n",
      "ERROR: end of quote\n",
      "['\"', 'Well', '?', '\"', 'he', 'said', '.']\n",
      "ERROR: end of sent\n",
      "['Now', '.']\n",
      "ERROR: no change possible\n",
      "['‘', 'So', '?', '’']\n",
      "ERROR: no change possible\n",
      "['Now', '?']\n",
      "ERROR: end of sent\n",
      "['Yet', '.']\n",
      "ERROR: end of sent\n",
      "['Also', ',', '.']\n",
      "ERROR: no change possible\n",
      "['So', '…', '’']\n",
      "ERROR: no change possible\n",
      "['Now', '…']\n",
      "ERROR: end of sent\n",
      "['‘', 'So', '.']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '?']\n",
      "ERROR: end of sent\n",
      "['So', '.']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '?', '’']\n",
      "ERROR: end of sent\n",
      "['Well', '.']\n",
      "ERROR: end of sent\n",
      "['‘', 'Well', '.']\n",
      "ERROR: no change possible\n",
      "['Now', '?']\n",
      "ERROR: no change possible\n",
      "['‘', 'And', '?', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '—', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '!']\n",
      "ERROR: end of quote\n",
      "['\"', 'Well', ',', '\"', 'Graham', 'said', 'slowly', ',', 'deciding', 'he', 'had', 'better', 'say', 'something', 'nice', ',', '\"', 'it', \"'s\", 'good', ',', 'but', 'perhaps', 'it', 'needs', 'a', 'little', 'work', '.', '\"']\n",
      "ERROR: end of sent\n",
      "['Well', '.', '``']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '?']\n",
      "ERROR: no change possible\n",
      "['‘', 'So', '?']\n",
      "ERROR: no change possible\n",
      "['But', '…']\n",
      "ERROR: no change possible\n",
      "['‘', 'And', '…', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'So', '?']\n",
      "ERROR: no change possible\n",
      "['So', '!']\n",
      "ERROR: no change possible\n",
      "['‘', 'But', '—', '’']\n",
      "ERROR: end of sent\n",
      "['Yet', '.']\n",
      "ERROR: no change possible\n",
      "['‘', 'So', '?', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '?', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'So', '?', '’']\n",
      "ERROR: end of sent\n",
      "['So', '.']\n",
      "ERROR: end of sent\n",
      "['So', '.']\n",
      "ERROR: no change possible\n",
      "['‘', 'And', '?', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'Now', '?', '’']\n",
      "ERROR: end of sent\n",
      "['So', '.']\n",
      "ERROR: end of sent\n",
      "['Now', '.']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '—', '‘']\n",
      "ERROR: end of sent\n",
      "['‘', 'Well', '.']\n",
      "ERROR: end of sent\n",
      "['‘', 'So', '.']\n",
      "ERROR: end of sent\n",
      "['‘', 'So', '.']\n",
      "ERROR: end of sent\n",
      "['‘', 'Now', '.']\n",
      "ERROR: end of quote\n",
      "['’', '’', 'Well', \"'\", ',', 'said', 'Macmillan', ',', '‘', 'he', 'is', 'not', 'going', 'to', 'be', 'my', 'headmaster', '.']\n",
      "ERROR: end of sent\n",
      "['Yet', '.']\n",
      "ERROR: end of sent\n",
      "['Well', '.']\n",
      "ERROR: no change possible\n",
      "['‘', 'Now', '?', '’']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: no change possible\n",
      "['‘', 'And', '…', '?', '’']\n",
      "ERROR: end of sent\n",
      "['Yet', '.']\n",
      "ERROR: end of sent\n",
      "['Now', '.']\n",
      "ERROR: no change possible\n",
      "['But', '…']\n",
      "ERROR: no change possible\n",
      "['‘', 'And', '?', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'But', '…', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'So', '?', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'So', '?', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'But', '…', '’']\n",
      "ERROR: no change possible\n",
      "['Well', '!']\n",
      "ERROR: no change possible\n",
      "['And', '—', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'So', '?']\n",
      "ERROR: end of sent\n",
      "['Well', '.']\n",
      "ERROR: no change possible\n",
      "['‘', 'And', '?', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '?', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'But', '?', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'But', '?', '’']\n",
      "ERROR: end of sent\n",
      "['Now', '.']\n",
      "ERROR: end of sent\n",
      "['‘', 'Now', '.']\n",
      "ERROR: end of sent\n",
      "['‘', 'But', '—', '.', '’']\n",
      "ERROR: no change possible\n",
      "['‘', 'Well', '?']\n",
      "ERROR: no change possible\n",
      "['‘', 'So', '?']\n",
      "ERROR: no change possible\n",
      "['Tart-with-a-Golden-Heart', 'Or']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label = []\n",
    "sent1 = []\n",
    "sent2 = []\n",
    "sent2_og = []\n",
    "y = []\n",
    "\n",
    "for file in tqdm(files):\n",
    "    info, docs = parse_bnc_xml(file)\n",
    "    for doc in docs:\n",
    "        for idx in range(len(doc))[1:]:\n",
    "            found = False\n",
    "            for word in doc[idx][:3]:\n",
    "                if word in terms_dict:\n",
    "                    clean = clean_sent(doc[idx], word)\n",
    "                    if type(clean) != list:\n",
    "                        break\n",
    "                    label.append(info)\n",
    "                    sent1.append(doc[idx-1])\n",
    "                    sent2.append(clean)\n",
    "                    sent2_og.append(doc[idx])\n",
    "                    this_y = np.zeros(len(terms_dict) + 1)\n",
    "                    this_y[terms_dict[word]] = 1\n",
    "                    y.append(this_y)\n",
    "                    found = True\n",
    "            if not found:\n",
    "                label.append(info)\n",
    "                sent1.append(doc[idx-1])\n",
    "                sent2.append(doc[idx])\n",
    "                sent2_og.append(None)\n",
    "                this_y = np.zeros(len(terms_dict) + 1)\n",
    "                this_y[9] = 1\n",
    "                y.append(this_y)\n",
    "        break\n",
    "                \n",
    "df = pd.DataFrame()\n",
    "df['label'] = label\n",
    "df['sent1'] = sent1\n",
    "df['sent2'] = sent2\n",
    "df['sent2_orig'] = sent2_og \n",
    "df['y'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sent1</th>\n",
       "      <th>sent2</th>\n",
       "      <th>sent2_orig</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>186625</th>\n",
       "      <td>The birdwatcher's handbook. Sample containing...</td>\n",
       "      <td>[There, we, all, stand, in, companionable, sil...</td>\n",
       "      <td>[More, folk, tramp, along, the, sandy, track, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258269</th>\n",
       "      <td>Introduction to politics. Sample containing ...</td>\n",
       "      <td>[The, people, recruited, into, official, posit...</td>\n",
       "      <td>[Although, the, United, States, Congress, is, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482174</th>\n",
       "      <td>Independent, electronic edition of 1989-10-0...</td>\n",
       "      <td>[Andy, Jones, ,, the, Charlton, striker, ,, ha...</td>\n",
       "      <td>[Ian, Dowie, ,, a, former, missile, engineer, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5274</th>\n",
       "      <td>19 conversations recorded by `Martin' (PS0KN...</td>\n",
       "      <td>[yeah, but, this, is, ,, this, is, different, ...</td>\n",
       "      <td>[cos, has, n't, Grantham, got, the, reputation...</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182921</th>\n",
       "      <td>A compass error. Sample containing about 361...</td>\n",
       "      <td>[‘, Her, mother, still, did, n't, tell, her, t...</td>\n",
       "      <td>[Constanza, did, ask, ,, Anna, simply, repeate...</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    label  \\\n",
       "186625   The birdwatcher's handbook. Sample containing...   \n",
       "258269    Introduction to politics. Sample containing ...   \n",
       "482174    Independent, electronic edition of 1989-10-0...   \n",
       "5274      19 conversations recorded by `Martin' (PS0KN...   \n",
       "182921    A compass error. Sample containing about 361...   \n",
       "\n",
       "                                                    sent1  \\\n",
       "186625  [There, we, all, stand, in, companionable, sil...   \n",
       "258269  [The, people, recruited, into, official, posit...   \n",
       "482174  [Andy, Jones, ,, the, Charlton, striker, ,, ha...   \n",
       "5274    [yeah, but, this, is, ,, this, is, different, ...   \n",
       "182921  [‘, Her, mother, still, did, n't, tell, her, t...   \n",
       "\n",
       "                                                    sent2 sent2_orig  \\\n",
       "186625  [More, folk, tramp, along, the, sandy, track, ...       None   \n",
       "258269  [Although, the, United, States, Congress, is, ...       None   \n",
       "482174  [Ian, Dowie, ,, a, former, missile, engineer, ...       None   \n",
       "5274    [cos, has, n't, Grantham, got, the, reputation...       None   \n",
       "182921  [Constanza, did, ask, ,, Anna, simply, repeate...       None   \n",
       "\n",
       "                                                        y  \n",
       "186625  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "258269  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "482174  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "5274    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "182921  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('data/discourse_markers/bnc_df.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [unused] Brown corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section takes the Brown corpus (used for initial tests, eventually discarded due to lack of paragraph/document boundaries) and the discovered discourse markers from the Acrolinx data and creates a DataFrame with the sentences, the discourse markers found in them if any, and the \"cleaned\" sentences if there were markers found."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sents = brown.sents()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "disc = read_lines('data/discourse_markers/discourse_markers.txt')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "brown_disc_df = pd.DataFrame()\n",
    "brown_disc_df['sent'] = sents"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "brown_disc_df = fill_df(brown_disc_df, disc)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "brown_disc_df = clean_df(brown_disc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_disc_df = pd.read_pickle('data/discourse_markers/brown_disc_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_disc_df.to_pickle('data/discourse_markers/brown_disc_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [unused] qualifiers/intensifiers: Brown corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was used initially to search for qualifiers and intensifiers, which were collected from the Acrolinx dataset, in the Brown corpus and see the contexts in which they appeared."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sents = brown.sents()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "qual = set(read_file('data/qualifiers_intensifiers/qual_intens_list.txt'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "brown_qual_df = pd.DataFrame()\n",
    "brown_qual_df['sent'] = sents"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "brown_qual_df = fill_df(brown_qual_df, [' ' + x for x in qual])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if ever\t3\n",
      "ultimately\t19\n",
      "far\t401\n",
      "all of\t153\n",
      "so-called\t31\n",
      "particularly\t141\n",
      "much\t878\n",
      "admittedly\t3\n",
      "please\t45\n",
      "exactly\t99\n",
      "perfectly\t31\n",
      "actually\t127\n",
      "little\t767\n",
      "big\t308\n",
      "apparently\t102\n",
      "a lot\t85\n",
      "alone\t190\n",
      "vast\t60\n",
      "all\t2611\n",
      "at all\t183\n",
      "in effect\t24\n",
      "clearly\t118\n",
      "extremely\t50\n",
      "generally\t119\n",
      "some kind of\t21\n",
      "quite\t269\n",
      "usually\t185\n",
      "right\t577\n",
      "too\t760\n",
      "sorely\t3\n",
      "also\t983\n",
      "of course\t234\n",
      "surely\t38\n",
      "importantly\t8\n",
      "ever\t328\n",
      "blatantly\t0\n",
      "such\t1124\n",
      "lots of\t26\n",
      "exact\t27\n",
      "honestly\t12\n",
      "at least\t272\n",
      "just\t742\n",
      "necessarily\t49\n",
      "really\t267\n",
      "probably\t232\n",
      "some\t1345\n",
      "even\t954\n",
      "occasionally\t32\n",
      "pretty\t98\n",
      "relatively\t84\n",
      "fully\t80\n",
      "ideally\t5\n",
      "absolutely\t27\n",
      "well\t744\n",
      "definitely\t21\n",
      "a bit\t56\n",
      "incredibly\t7\n",
      "sort of\t117\n",
      "maybe\t66\n",
      "certainly\t115\n",
      "literally\t26\n",
      "simply\t166\n",
      "possibly\t57\n",
      "so\t1641\n",
      "specifically\t36\n",
      "actively\t11\n",
      "then\t984\n",
      "very\t749\n",
      "completely\t109\n",
      "er\t0\n",
      "truly\t56\n"
     ]
    }
   ],
   "source": [
    "# count number of example sentences (unverified) for each example\n",
    "for col in brown_qual_df:\n",
    "    if brown_qual_df[col].dtype == 'int64':\n",
    "        print(col + '\\t' + str(sum(brown_qual_df[col])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
