{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Dropout, Concatenate, TimeDistributed\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/lexical_repl/embedding.pkl', 'rb') as f:\n",
    "    embedding = pickle.load(f)\n",
    "    \n",
    "with open('data/lexical_repl/idx2w.pkl', 'rb') as f:\n",
    "    idx2w = pickle.load(f)\n",
    "    \n",
    "with open('data/lexical_repl/w2idx.pkl', 'rb') as f:\n",
    "    w2idx = pickle.load(f)\n",
    "    \n",
    "assert 0 not in idx2w\n",
    "assert '\\t' in w2idx # SOS\n",
    "assert '\\n' in w2idx # EOS\n",
    "assert '[UNK]' in w2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_idx(string):\n",
    "    # turns sequence of tokens to sequence of indices\n",
    "    seq = word_tokenize(string)\n",
    "    idx = []\n",
    "    for word in seq:\n",
    "        if word in w2idx:\n",
    "            idx.append(w2idx[word])\n",
    "        else:\n",
    "            #print('unknown:\\t' + word)\n",
    "            idx.append(w2idx['[UNK]'])\n",
    "    return idx\n",
    "\n",
    "def tok_to_idx(seq):\n",
    "    # if already tokenized\n",
    "    idx = []\n",
    "    for word in seq:\n",
    "        if word in w2idx:\n",
    "            idx.append(w2idx[word])\n",
    "        else:\n",
    "            #print('unknown:\\t' + word)\n",
    "            idx.append(w2idx['[UNK]'])\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sents_df = pd.read_pickle('data/lexical_repl/sents-df-with-frags.zip') # complete\n",
    "sents_df = pd.read_pickle('data/lexical_repl/sents-df-restricted.pkl') # smaller, more balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>source</th>\n",
       "      <th>description</th>\n",
       "      <th>masks</th>\n",
       "      <th>words</th>\n",
       "      <th>phrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The, September-October, term, jury, had, been...</td>\n",
       "      <td>brown</td>\n",
       "      <td>None</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[investigate]</td>\n",
       "      <td>[[to, investigate, reports, of, possible, ``]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[The, grand, jury, commented, on, a, number, o...</td>\n",
       "      <td>brown</td>\n",
       "      <td>None</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[operated, purchasing]</td>\n",
       "      <td>[[purchasing], [well, operated, and, follow]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[However, ,, the, jury, said, it, believes, ``...</td>\n",
       "      <td>brown</td>\n",
       "      <td>None</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[administration]</td>\n",
       "      <td>[[administration]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[It, urged, that, the, next, Legislature, ``, ...</td>\n",
       "      <td>brown</td>\n",
       "      <td>None</td>\n",
       "      <td>[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[provide]</td>\n",
       "      <td>[[that, the, next, Legislature, ``, provide, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[The, jury, also, commented, on, the, Fulton, ...</td>\n",
       "      <td>brown</td>\n",
       "      <td>None</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[administrators]</td>\n",
       "      <td>[[administrators]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sent source description  \\\n",
       "2   [The, September-October, term, jury, had, been...  brown        None   \n",
       "6   [The, grand, jury, commented, on, a, number, o...  brown        None   \n",
       "8   [However, ,, the, jury, said, it, believes, ``...  brown        None   \n",
       "12  [It, urged, that, the, next, Legislature, ``, ...  brown        None   \n",
       "18  [The, jury, also, commented, on, the, Fulton, ...  brown        None   \n",
       "\n",
       "                                                masks                   words  \\\n",
       "2   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...           [investigate]   \n",
       "6   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [operated, purchasing]   \n",
       "8   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        [administration]   \n",
       "12  [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...               [provide]   \n",
       "18  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        [administrators]   \n",
       "\n",
       "                                              phrases  \n",
       "2      [[to, investigate, reports, of, possible, ``]]  \n",
       "6       [[purchasing], [well, operated, and, follow]]  \n",
       "8                                  [[administration]]  \n",
       "12  [[that, the, next, Legislature, ``, provide, e...  \n",
       "18                                 [[administrators]]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec199805beca4b3496b9fcd403ceb4a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7146), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b05716cab3d4992b5a882a61ca09fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7146), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "for x in tqdm(sents_df.sent):\n",
    "    X.append(tok_to_idx(x))\n",
    "y_span = sents_df.masks\n",
    "\n",
    "X = pad_sequences(X, maxlen = 50, value = 0, padding = 'post').astype('int64')\n",
    "y_span = pad_sequences(y_span, maxlen = 50, value = 0, padding = 'post').astype('int64')\n",
    "\n",
    "y_span_cat = np.zeros((y_span.shape[0], y_span.shape[1], 2))\n",
    "for idx_1 in tqdm(range(y_span.shape[0])):\n",
    "    for idx_2 in range(y_span.shape[1]):\n",
    "        y_span_cat[idx_1][idx_2] = to_categorical(y_span[idx_1][idx_2], num_classes = 2)\n",
    "        \n",
    "X, y_span_cat = shuffle(X, y_span_cat)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_span_cat, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# specifically for acrolinx mini-dataset\n",
    "\n",
    "acro_df = pd.read_pickle('data/lexical_repl/acro_test_df.pkl')\n",
    "\n",
    "X = [tok_to_idx(t) for t in acro_df.sent]\n",
    "y_span = list(acro_df.mark)\n",
    "\n",
    "X = pad_sequences(X, value = 0, padding = 'post').astype('int64')\n",
    "y_span = pad_sequences(y_span, value = 0, padding = 'post').astype('int64')\n",
    "\n",
    "y_span_cat = np.zeros((y_span.shape[0], y_span.shape[1], 2))\n",
    "for idx_1 in tqdm(range(y_span.shape[0])):\n",
    "    for idx_2 in range(y_span.shape[1]):\n",
    "        y_span_cat[idx_1][idx_2] = to_categorical(y_span[idx_1][idx_2], num_classes = 2)\n",
    "        \n",
    "X, y_span_cat = shuffle(X, y_span_cat)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_span_cat, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# extract data to arrays from df, add POST-padding\n",
    "X = pad_sequences(df.idx, maxlen = 50, value = 0, padding = 'post').astype('int64')\n",
    "y_span = pad_sequences(df.binary, maxlen = 50, value = 0, padding = 'post').astype('int64')\n",
    "\n",
    "y_span_cat = np.zeros((y_span.shape[0], y_span.shape[1], 2))\n",
    "for idx_1 in tqdm(range(y_span.shape[0])):\n",
    "    for idx_2 in range(y_span.shape[1]):\n",
    "        y_span_cat[idx_1][idx_2] = to_categorical(y_span[idx_1][idx_2], num_classes = 2)\n",
    "        \n",
    "X, y_span_cat = shuffle(X, y_span_cat)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_span_cat, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# extract data to arrays from df, add POST-padding\n",
    "X_orig = pad_sequences(df['x_orig'], value = pad, padding = 'post').astype('int64')\n",
    "dec_input = pad_sequences(df['dec_input'], value = pad, padding = 'post').astype('int64')\n",
    "dec_target = pad_sequences(df['dec_target'], value = pad, padding = 'post').astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model: find spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_find_model(input_len, embedding, num_units, dropout_rate):\n",
    "    K.clear_session()\n",
    "    \n",
    "    main_input = Input(shape = (input_len,), dtype = 'int64', name = 'main_input')\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        embedding_layer = Embedding(input_dim = embedding.shape[0],\n",
    "                              output_dim = embedding.shape[1],\n",
    "                              weights = [embedding],\n",
    "                              trainable = False, \n",
    "                              mask_zero = True,\n",
    "                              name = 'embedding_layer')\n",
    "        input_embed = embedding_layer(main_input)\n",
    "        \n",
    "    bi_lstm = Bidirectional(LSTM(return_sequences = True, units = num_units), name='bi-lstm')(input_embed)\n",
    "    dropout_lstm = Dropout(rate = dropout_rate, name = 'dropout_lstm')(bi_lstm)\n",
    "    dense = TimeDistributed(Dense(num_units, activation = 'relu'), name = 'dense')(dropout_lstm)\n",
    "    dropout_dense = Dropout(rate = dropout_rate, name = 'dropout_dense')(dense)\n",
    "    # is timedistributed even needed anymore? dense can handle 3D input now?\n",
    "    output = TimeDistributed(Dense(2, activation = 'softmax'), name = 'output')(dropout_dense)\n",
    "    \n",
    "    model = Model(inputs = main_input, outputs = output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_layer (Embedding)  (None, 50, 100)           14830500  \n",
      "_________________________________________________________________\n",
      "bi-lstm (Bidirectional)      (None, 50, 128)           84480     \n",
      "_________________________________________________________________\n",
      "dropout_lstm (Dropout)       (None, 50, 128)           0         \n",
      "_________________________________________________________________\n",
      "dense (TimeDistributed)      (None, 50, 64)            8256      \n",
      "_________________________________________________________________\n",
      "dropout_dense (Dropout)      (None, 50, 64)            0         \n",
      "_________________________________________________________________\n",
      "output (TimeDistributed)     (None, 50, 2)             130       \n",
      "=================================================================\n",
      "Total params: 14,923,366\n",
      "Trainable params: 92,866\n",
      "Non-trainable params: 14,830,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "find_model = build_find_model(input_len = X.shape[1], \n",
    "                         embedding = embedding, \n",
    "                         num_units = 64, \n",
    "                         dropout_rate = 0.25)\n",
    "find_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n",
      "6431/6431 [==============================] - 38s 6ms/step - loss: 0.4298 - binary_accuracy: 0.8279\n"
     ]
    }
   ],
   "source": [
    "find_model.compile(optimizer = 'adam',\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics = ['binary_accuracy'])\n",
    "\n",
    "find_history = find_model.fit(X_train, y_train, epochs = 1, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model: suggest replacements for spans"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% conda activate thesis\n",
    "\n",
    "% python3 OpenNMT-py/preprocess.py -train_src make-it-sound-less-formal/code/data/lexical_repl_models/src-train.txt -train_tgt make-it-sound-less-formal/code/data/lexical_repl_models/tgt-train.txt -valid_src make-it-sound-less-formal/code/data/lexical_repl_models/src-val.txt -valid_tgt make-it-sound-less-formal/code/data/lexical_repl_models/tgt-val.txt -save_data make-it-sound-less-formal/code/data/lexical_repl_models/OpenNMTData -dynamic_dict\n",
    "\n",
    "% python3 OpenNMT-py/tools/embeddings_to_torch.py -emb_file_both \"make-it-sound-less-formal/code/data/lexical_repl_models/w2v_vectors.txt\" -dict_file \"make-it-sound-less-formal/code/data/lexical_repl_models/OpenNMTData.vocab.pt\" -output_file \"make-it-sound-less-formal/code/data/lexical_repl_models/embeddings\"\n",
    "\n",
    "% python3 OpenNMT-py/train.py -save_model make-it-sound-less-formal/code/data/lexical_repl_models/model -data make-it-sound-less-formal/code/data/lexical_repl_models/OpenNMTData -word_vec_size 100 -pre_word_vecs_enc \"make-it-sound-less-formal/code/data/lexical_repl_models/embeddings.enc.pt\" -pre_word_vecs_dec \"make-it-sound-less-formal/code/data/lexical_repl_models/embeddings.dec.pt\" -copy_attn\n",
    "\n",
    "% python3 OpenNMT-py/translate.py -model make-it-sound-less-formal/code/data/lexical_repl_models/model -src make-it-sound-less-formal/code/data/lexical_repl_models/src-test.txt -output make-it-sound-less-formal/code/data/lexical_repl_models/pred.txt -replace_unk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unused: manual implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I realized that already being familiar with OpenNMT, it made more sense to use their system - as educative an experience as developing my own was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_suggest_model(orig_len, repl_len, embedding, num_units, dropout_rate):\n",
    "    K.clear_session()\n",
    "    \n",
    "    orig_input = Input(shape = (orig_len,), dtype = 'int64', name = 'orig_input')\n",
    "    repl_input = Input(shape = (repl_len,), dtype = 'int64', name = 'repl_input')\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        embedding_layer = Embedding(input_dim = embedding.shape[0],\n",
    "                              output_dim = embedding.shape[1],\n",
    "                              weights = [embedding],\n",
    "                              trainable = False, \n",
    "                              mask_zero = True,\n",
    "                              name = 'embedding_layer')\n",
    "        orig_embed = embedding_layer(orig_input)\n",
    "        repl_embed = embedding_layer(repl_input)\n",
    "        \n",
    "    ### feed encoder input (main_input), decoder input (repl_input) and sliced replacement text to enc-dec system\n",
    "\n",
    "    # these should change later to some sort of context-based or conditional model\n",
    "    # also with attention\n",
    "\n",
    "    encoder = Bidirectional(LSTM(return_state = True, units = num_units), name = \"encoder\")\n",
    "    decoder = LSTM(return_sequences = True, return_state = True, name = \"decoder\", units = 2 * num_units)\n",
    "\n",
    "    enc_output, enc_h_forward, enc_c_forward, enc_h_backward, enc_c_backward = encoder(orig_embed)\n",
    "    enc_h = Concatenate()([enc_h_forward, enc_h_backward])\n",
    "    enc_c = Concatenate()([enc_c_forward, enc_c_backward])\n",
    "    dec_output, _, _ = decoder(repl_embed, initial_state = [enc_h, enc_c])\n",
    "\n",
    "    # Dropout?\n",
    "    # between enc-dec\n",
    "\n",
    "    dense = TimeDistributed(Dense(num_units, activation = 'relu'), name = 'dense_layer')\n",
    "    dec_tdd = TimeDistributed(Dense(embedding.shape[0], activation='softmax'), name = 'dense_output')\n",
    "\n",
    "    dec_dense = dense(dec_output)\n",
    "    repl_output = dec_tdd(dec_dense)\n",
    "    \n",
    "    model = Model(inputs = [orig_input, repl_input], outputs = repl_output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_generator_train(data, batch_size):\n",
    "    X_orig_whole = data[0]\n",
    "    dec_input_whole = data[1]\n",
    "    dec_target_whole = data[2]\n",
    "    \n",
    "    X_orig_whole, dec_input_whole, dec_target_whole = shuffle(X_orig_whole, \n",
    "                                                              dec_input_whole, \n",
    "                                                              dec_target_whole)\n",
    "    \n",
    "    i = 0\n",
    "            \n",
    "    while True:\n",
    "        if i + batch_size > len(X_orig_whole):\n",
    "            X_orig_batch = X_orig[i:]\n",
    "            dec_input_batch = dec_input_whole[i:]\n",
    "            dec_target_batch = dec_target_whole[i:]\n",
    "            i = 0\n",
    "        else:\n",
    "            X_orig_batch = X_orig[i:i+batch_size]\n",
    "            dec_input_batch = dec_input_whole[i:i+batch_size]\n",
    "            dec_target_batch = dec_target_whole[i:i+batch_size]\n",
    "            i += batch_size\n",
    "        \n",
    "        inputs = [X_orig_batch, dec_input_batch]\n",
    "        targets = np.array([to_categorical(x, num_classes = embedding.shape[0]) for x in dec_target_batch])\n",
    "        \n",
    "        yield inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggest_model = build_suggest_model(orig_len = X_orig.shape[1],\n",
    "                                   repl_len = dec_input.shape[1],\n",
    "                                   embedding = embedding,\n",
    "                                   num_units = 128,\n",
    "                                   dropout_rate = 0.25)\n",
    "\n",
    "suggest_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggest_model.compile(optimizer = 'adam',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "suggest_history = suggest_model.fit_generator(\n",
    "                            suggest_generator_train([X_orig, dec_input, dec_target], \n",
    "                                                    batch_size),\n",
    "                              steps_per_epoch=len(X_orig) // batch_size,\n",
    "                              epochs = 5,b\n",
    "                              verbose = 1)#,\n",
    "                              #validation_data = (x_val, y_val),\n",
    "                              #use_multiprocessing = True,\n",
    "                              #workers = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "#model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine encoder model: takes original input, outputs the states\n",
    "enc_model = Model(inputs = orig_input, outputs = [enc_h, enc_c])\n",
    "\n",
    "# define the states to input into the decoder (this is what you get from the encoder)\n",
    "inf_dec_h_input = Input(shape=(num_units * 2,)) #enc_h\n",
    "inf_dec_c_input = Input(shape=(num_units * 2,)) #enc_c\n",
    "inf_dec_states_input = [inf_dec_h_input, inf_dec_c_input]\n",
    "\n",
    "# these are the outputs you get when you run the decoder, set them up matching the original model\n",
    "# repl_embed is more of a placeholder - of course you won't actually have the answer when you infer\n",
    "inf_dec_main, inf_dec_h, inf_dec_c = decoder(repl_embed, initial_state = inf_dec_states_input)\n",
    "inf_dec_states = [inf_dec_h, inf_dec_c]\n",
    "inf_dec_dense = dense(inf_dec_main)\n",
    "inf_dec_output = dec_tdd(inf_dec_dense)\n",
    "\n",
    "# define decoder model\n",
    "dec_model = Model([repl_input] + inf_dec_states_input, [inf_dec_output] + inf_dec_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(input_seq):\n",
    "    # takes input sequence in form of seq of token indices\n",
    "    states = enc_model.predict(input_seq)\n",
    "    \n",
    "    # begin output sequence, use start character\n",
    "    target_seq = np.zeros((1, repl_len))\n",
    "    target_seq[0, 0] = w2idx['\\t']\n",
    "    \n",
    "    # using batch_size = 1, sample in a loop\n",
    "    stop = False\n",
    "    decoded = []\n",
    "    while not stop:\n",
    "        output_tok, h, c = dec_model.predict([target_seq] + states)\n",
    "        states = [h, c] # update states\n",
    "        \n",
    "        # sample a token\n",
    "        sample_idx = np.argmax(output_tok[0, -1, :]) # takes the last one in output\n",
    "        sample_tok = idx2w[sample_idx]\n",
    "        \n",
    "        # exit if maxlen is reached or stop character is found\n",
    "        if (sample_tok == '\\n' or len(decoded) > repl_len):\n",
    "            stop = True\n",
    "        else:\n",
    "            # update target_seq\n",
    "            decoded.append(sample_tok)\n",
    "            target_seq = np.zeros((1, repl_len))\n",
    "            target_seq[0, 0] = sample_idx\n",
    "    \n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'is not'\n",
    "\n",
    "decode(pad_sequences([seq_to_idx(test)], value = pad, padding = 'post', maxlen = orig_len).astype('int64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in np.random.choice(len(X_orig), 10):\n",
    "    sent = ''\n",
    "    for x in X_orig[idx]:\n",
    "        if x != 0:\n",
    "            sent += idx2w[x] + ' '\n",
    "    print(sent)\n",
    "    print(decode(np.array([X_orig[idx]])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
