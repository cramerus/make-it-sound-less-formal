{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from glob import glob\n",
    "import math\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models.callbacks import CallbackAny2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was used to train word embeddings using the gathered data from the Brown, OANC, BNC corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train word2vec embedding using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(pd.read_pickle('data/lexical_repl/sents_df.zip')['sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f793451bb346309334ef7435810aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3818246), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_words = 10000\n",
    "num_words = 0\n",
    "for item in tqdm(sentences):\n",
    "    num_words += len(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochMonitor(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "    \n",
    "    def __init__(self, num_words, batch_words):\n",
    "        self.batch_words = batch_words\n",
    "        self.num_words = num_words\n",
    "        self.num_batches = math.ceil(self.num_words / self.batch_words)\n",
    "        self.epoch = 1\n",
    "        self.current_batch = 1\n",
    "        \n",
    "    def on_batch_begin(self, model):\n",
    "        if self.current_batch % int(self.num_batches / 5) == 0:\n",
    "            print(\"{0:.0%}\".format(self.current_batch/self.num_batches))\n",
    "        \n",
    "    def on_batch_end(self, model):\n",
    "        self.current_batch += 1\n",
    "    \n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "        self.current_batch = 1\n",
    "    \n",
    "    def on_epoch_end(self, model):\n",
    "        self.epoch += 1\n",
    "        \n",
    "epoch_monitor = EpochMonitor(num_words, batch_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 start\n",
      "20%\n",
      "40%\n",
      "60%\n",
      "80%\n",
      "100%\n",
      "Epoch #2 start\n",
      "20%\n",
      "40%\n",
      "60%\n",
      "80%\n",
      "100%\n",
      "Epoch #3 start\n",
      "20%\n",
      "40%\n",
      "60%\n",
      "80%\n",
      "100%\n",
      "Epoch #4 start\n",
      "20%\n",
      "40%\n",
      "60%\n",
      "80%\n",
      "100%\n",
      "Epoch #5 start\n",
      "20%\n",
      "40%\n",
      "60%\n",
      "80%\n",
      "100%\n",
      "Epoch #6 start\n",
      "20%\n",
      "40%\n",
      "60%\n",
      "80%\n",
      "100%\n",
      "Epoch #7 start\n",
      "20%\n",
      "40%\n",
      "60%\n",
      "80%\n",
      "100%\n",
      "Epoch #8 start\n",
      "20%\n",
      "40%\n",
      "60%\n",
      "80%\n",
      "100%\n",
      "Epoch #9 start\n",
      "20%\n",
      "40%\n",
      "60%\n",
      "80%\n",
      "100%\n",
      "Epoch #10 start\n",
      "20%\n",
      "40%\n",
      "60%\n",
      "80%\n",
      "100%\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(sentences, \n",
    "                 size = 100, \n",
    "                 window = 5, \n",
    "                 min_count = 5, \n",
    "                 workers = 12, \n",
    "                 sg = 0, \n",
    "                 iter = 10, \n",
    "                 batch_words = batch_words,\n",
    "                 callbacks = [epoch_monitor]) \n",
    "#sg=0: CBOW > skipgram\n",
    "# most default parameters kept\n",
    "model.save('data/lexical_repl/w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148301"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 0.7727294564247131),\n",
       " ('knight', 0.7580963373184204),\n",
       " ('prince', 0.7514700889587402),\n",
       " ('warrior', 0.7270703315734863),\n",
       " ('chieftain', 0.7142376899719238),\n",
       " ('protector', 0.7099010348320007),\n",
       " ('grandson', 0.7018693685531616),\n",
       " ('vassal', 0.6943684816360474),\n",
       " ('princess', 0.6851438283920288),\n",
       " ('Everqueen', 0.6839128136634827)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive = ['queen', 'man'], negative = ['woman'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare embedding for use in model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make:\n",
    "<br>- word2index dictionary\n",
    "<br>- index2word dictionary\n",
    "<br>- embedding matrix\n",
    "\n",
    "Keep in mind:\n",
    "<br>- we will use the mask_zero functionality, so we must be sure the 0th position in the embedding matrix is a random vector/all zeroes/it doesn't particularly matter and that the 0th position doesn't lead to anything in either dictionary; it will be used for padding\n",
    "<br>- we need to add the start and end of sentence markers\n",
    "<br>- we also must add a randomly initialized UNK token, to insert a placeholder for words unknown to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aede59e5aa274da1a5dd7e760d629212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=148301), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(model.wv.vocab) + 1, 100))\n",
    "w2idx = {}\n",
    "idx2w = {}\n",
    "\n",
    "for i in tqdm(range(1, len(model.wv.vocab) + 1)):\n",
    "    word = model.wv.index2word[i - 1]\n",
    "    \n",
    "    w2idx[word] = i\n",
    "    idx2w[i] = word\n",
    "    \n",
    "    embedding_vector = model.wv[word]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOS start-of-sequence tag is '\\t'\n",
    "np.random.seed(47)\n",
    "idx2w[embedding_matrix.shape[0]] = '\\t'\n",
    "w2idx['\\t'] = embedding_matrix.shape[0]\n",
    "embedding_matrix = np.append(embedding_matrix, np.random.rand(1, 100), axis=0)\n",
    "\n",
    "# EOS end-of-sequence tag is '\\n'\n",
    "np.random.seed(42)\n",
    "idx2w[embedding_matrix.shape[0]] = '\\n'\n",
    "w2idx['\\n'] = embedding_matrix.shape[0]\n",
    "embedding_matrix = np.append(embedding_matrix, np.random.rand(1, 100), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(26)\n",
    "idx2w[embedding_matrix.shape[0]] = '[UNK]'\n",
    "w2idx['[UNK]'] = embedding_matrix.shape[0]\n",
    "embedding_matrix = np.append(embedding_matrix, np.random.rand(1, 100), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/lexical_repl/embedding.pkl', 'wb') as f:\n",
    "    pickle.dump(embedding_matrix, f)\n",
    "    \n",
    "with open('data/lexical_repl/idx2w.pkl', 'wb') as f:\n",
    "    pickle.dump(idx2w, f)\n",
    "    \n",
    "with open('data/lexical_repl/w2idx.pkl', 'wb') as f:\n",
    "    pickle.dump(w2idx, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare for use with openNMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load('data/lexical_repl/w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148301"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('data/lexical_repl_models/w2v_vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
