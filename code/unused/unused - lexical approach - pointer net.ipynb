{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tqdm_pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from nltk import word_tokenize\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras.backend as K\n",
    "from keras import initializers, regularizers\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Activation, Embedding, RNN, LSTM, LSTMCell, Dense, Dropout, Concatenate\n",
    "from keras.layers import TimeDistributed, Bidirectional, Lambda, Layer\n",
    "from keras.layers import concatenate\n",
    "from keras.layers.recurrent import Recurrent\n",
    "from keras.layers.core import Reshape\n",
    "from keras.activations import tanh, softmax\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import metrics, optimizers\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure gpu is available\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import embedding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# pretrained GloVe embeddings unpacked\n",
    "\n",
    "file = 'glove.6B.300d.txt'\n",
    "embed_dim = 300\n",
    "\n",
    "w2idx = {}\n",
    "w2vec = {}\n",
    "\n",
    "with open(file) as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        line = [part.strip() for part in line.split()]\n",
    "        word = line[0]\n",
    "        vec = np.asarray(line[1 : embed_dim + 1], dtype='float32')\n",
    "        \n",
    "        w2idx[word] = idx\n",
    "        w2vec[word] = vec\n",
    "        \n",
    "# include empty character for padding - put last\n",
    "w2idx[''] = len(w2idx)\n",
    "w2vec[''] = np.zeros(embed_dim)\n",
    "\n",
    "# create embedding matrix\n",
    "embeddings = np.zeros((len(w2idx), embed_dim))\n",
    "\n",
    "for word, idx in w2idx.items():\n",
    "    embeddings[idx] = np.array(w2vec[word])\n",
    "    \n",
    "# save\n",
    "with open('glv_embed_matrix.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "    \n",
    "with open('glv_w2idx.pkl', 'wb') as f:\n",
    "    pickle.dump(w2idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries, pretrained embeddings\n",
    "with open('data/glv_w2idx.pkl', 'rb') as f:\n",
    "    w2idx = pickle.load(f)\n",
    "with open('data/glv_embed_matrix.pkl', 'rb') as f:\n",
    "    embedding = pickle.load(f)\n",
    "    \n",
    "# need to append BOS ('\\t') and EOS ('\\n') tokens to embeddings\n",
    "# give (consistently) random initialization since they don't actually mean anything\n",
    "# padding already exists as '' at the end of the embedding\n",
    "\n",
    "pad = len(w2idx) - 1\n",
    "\n",
    "w2idx['\\t'] = embedding.shape[0]\n",
    "np.random.seed(1)\n",
    "embedding = np.append(embedding, np.random.rand(1, 300), axis=0)\n",
    "\n",
    "w2idx['\\n'] = embedding.shape[0]\n",
    "np.random.seed(2)\n",
    "embedding = np.append(embedding, np.random.rand(1, 300), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# placeholder dataset\n",
    "\n",
    "df = pd.DataFrame({'Sentence': [\"I do not know what to say.\", \n",
    "                                \"The girl will not go to bed.\",\n",
    "                               \"He will not come tomorrow night.\",\n",
    "                               \"They would not want you to do that.\",\n",
    "                               \"We can not believe that this happened.\",\n",
    "                               \"I could not handle the truth.\"], \n",
    "                  'Original': [\"do not\", \"will not\", \"will not\", \"would not\", \"can not\", \"could not\"],\n",
    "                  'Replacement': [\"don't\", \"won't\", \"won't\", \"wouldn't\", \"can't\", \"couldn't\"]})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Original</th>\n",
       "      <th>Replacement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Imagine that you have just written what you be...</td>\n",
       "      <td>you have</td>\n",
       "      <td>you 've</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are ready to get it off your plate and sen...</td>\n",
       "      <td>You are</td>\n",
       "      <td>You 're</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Before you hit the publish button , are you po...</td>\n",
       "      <td>you have</td>\n",
       "      <td>you 've</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After all , you have probably worked hard to c...</td>\n",
       "      <td>you have</td>\n",
       "      <td>you 've</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maybe you are already asking yourself some of ...</td>\n",
       "      <td>you are</td>\n",
       "      <td>you 're</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Original Replacement\n",
       "0  Imagine that you have just written what you be...  you have     you 've\n",
       "1  You are ready to get it off your plate and sen...   You are     You 're\n",
       "2  Before you hit the publish button , are you po...  you have     you 've\n",
       "3  After all , you have probably worked hard to c...  you have     you 've\n",
       "4  Maybe you are already asking yourself some of ...   you are     you 're"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('data/acrolinx_blog/acrolinx_blog_annotated_df.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "# change from text to indices\n",
    "\n",
    "# NOTE: there is word lowering in this because the pretrained word vectors, GloVe, only include\n",
    "# lowercase tokens\n",
    "\n",
    "def preprocess(df):\n",
    "    x_token = []\n",
    "    y_idx_start = []\n",
    "    y_idx_end = []\n",
    "    y_repl = []\n",
    "    x_orig = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total = df.shape[0]):\n",
    "        \n",
    "        # Converting sentence strings to lists of indices.\n",
    "        # Adding BOS (\"\\t\") and EOS (\"\\n\") markers to all.\n",
    "        \n",
    "        sent = word_tokenize(row['Sentence'])\n",
    "        # add start-of-sequence ('\\t') and end-of-sequence ('\\n') markers to all texts\n",
    "        #sent = ['\\t'] + sent + ['\\n']\n",
    "        sent_indices = []\n",
    "        for word in sent:\n",
    "            word = word.lower()\n",
    "            if word in w2idx:\n",
    "                sent_indices.append(w2idx[word])\n",
    "            #else:\n",
    "            #    sent_indices.append(pad) # This adds padding instead of unknown. Fix?         \n",
    "        if len(sent_indices) == 0:\n",
    "            x_token.append(np.nan)\n",
    "            y_idx_start.append(np.nan)\n",
    "            y_idx_end.append(np.nan)\n",
    "            y_repl.append(np.nan)\n",
    "            x_orig.append(np.nan)\n",
    "            print('Empty sentence: ' + row['Sentence'])\n",
    "            continue\n",
    "        x_token.append(sent_indices)\n",
    "        \n",
    "        # Finding original segment locations in sentence.\n",
    "        \n",
    "        orig = word_tokenize(row['Original'])\n",
    "        orig_indices = []\n",
    "        for word in orig:\n",
    "            word = word.lower()\n",
    "            if word in w2idx:\n",
    "                orig_indices.append(w2idx[word])\n",
    "            #else:\n",
    "            #    orig_indices.append(pad) # This adds padding instead of unknown. Fix?         \n",
    "        if len(orig_indices) == 0:    \n",
    "            y_idx_start.append(np.nan)\n",
    "            y_idx_end.append(np.nan)\n",
    "            y_repl.append(np.nan)\n",
    "            x_orig.append(np.nan)\n",
    "            print('Empty fragment: ' + row['Original'])\n",
    "            continue\n",
    "        x_orig.append(orig_indices)\n",
    "                \n",
    "        # take indices and find the 1st occurrence of the slice in the whole sentence\n",
    "        starts = [i for i, x in enumerate(sent_indices) if x == orig_indices[0]]\n",
    "        y_s = np.nan\n",
    "        y_e = np.nan\n",
    "        for potential_start in starts:\n",
    "            potential_slice = sent_indices[potential_start : potential_start + len(orig_indices)]\n",
    "            if (potential_slice == np.array(orig_indices)).all():\n",
    "                y_s = potential_start\n",
    "                y_e = potential_start + len(orig_indices) + 1\n",
    "                break\n",
    "        if np.isnan(y_s) or np.isnan(y_e):\n",
    "            print('Original not found in sentence.')\n",
    "            print(row['Sentence'])\n",
    "            print(row['Original'])\n",
    "        y_idx_start.append(y_s)\n",
    "        y_idx_end.append(y_e)\n",
    "                \n",
    "        # Tokenize, put through w2idx, add BOS/EOS markers to replacement text.\n",
    "        \n",
    "        repl = word_tokenize(row['Replacement'])\n",
    "        # add start-of-sequence ('\\t') and end-of-sequence ('\\n') markers to these\n",
    "        # this, the replacement/target text, will be used in the decoder step of training only\n",
    "        repl = ['\\t'] + repl + ['\\n']\n",
    "        repl_indices = []\n",
    "        for word in repl:\n",
    "            word = word.lower()\n",
    "            if word in w2idx:\n",
    "                repl_indices.append(w2idx[word])\n",
    "            #else:\n",
    "            #    repl_indices.append(pad) # This adds padding instead of unknown. Fix?\n",
    "        y_repl.append(repl_indices)\n",
    "                \n",
    "    df['x_token'] = x_token\n",
    "    df['y_idx_start'] = y_idx_start\n",
    "    df['y_idx_end'] = y_idx_end\n",
    "    df['y_repl'] = y_repl\n",
    "    df['x_orig'] = x_orig\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d567e3a0a44a4524b6e470c9f974d911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4848), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original not found in sentence.\n",
      "Now you have seen all of the problem words that we have collectedd .\n",
      "that we have collected\n",
      "Original not found in sentence.\n",
      "Yet the separation of technical content creation makes it difficultr to ensure consistency , both across all technical output and compared with a company 's other branded content .\n",
      "makes it difficult\n",
      "Original not found in sentence.\n",
      "It does not mean… We 're rigid or uptight .\n",
      "It does not mean\n",
      "Empty fragment: Acrolinx\n",
      "Original not found in sentence.\n",
      "It is a great event and one that you should definitely check out if you have not't before ( by the way , you can still register for it by clicking here ) .\n",
      "have not\n",
      "Original not found in sentence.\n",
      "Thmay be conference is all about being smarter with your content — whether you 're a marketer or in tech docs — and following the lead of pioneering companies such as Google , IBM , and Cisco Systems .\n",
      "This\n",
      "Original not found in sentence.\n",
      "Thmay be conference is all about being smarter with your content — whether you 're a marketer or in tech docs — and following the lead of pioneering companies such as Google , IBM , and Cisco Systems .\n",
      "may be\n",
      "Original not found in sentence.\n",
      "Considering that Americans eat over ten billion doughnut every year ( that 's 31 donuts per person if you 're wondering ) , it 's a food that many of us seem to be familiarr with .\n",
      "familiar\n",
      "Original not found in sentence.\n",
      "`` Content Connections is a great event because it not only packs a real punch in terms of ideas , takeaways , and insight , it 's alvery so convenient , '' continues Bredenkamp .\n",
      "very\n",
      "Original not found in sentence.\n",
      "That is because that websites finally gave us a place to put all of the collateral we had been creating about our companies ' products and services .\n",
      "that we\n",
      "Original not found in sentence.\n",
      "You will get a sense of achievement , and it is often easierr to edit what is there than to start creating from scratch .\n",
      "easier\n",
      "Original not found in sentence.\n",
      "This handy rethereforeurce gathers all of the most established style guides for you , so you can choose the best one for your brand of writing .\n",
      "therefore\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = preprocess(df)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Original</th>\n",
       "      <th>Replacement</th>\n",
       "      <th>x_token</th>\n",
       "      <th>y_idx_start</th>\n",
       "      <th>y_idx_end</th>\n",
       "      <th>y_repl</th>\n",
       "      <th>x_orig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4121</th>\n",
       "      <td>The result is high-impact content that is on-b...</td>\n",
       "      <td>that is</td>\n",
       "      <td>that 's</td>\n",
       "      <td>[0, 712, 14, 157821, 2768, 12, 14, 5, 1, 3023,...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>[400001, 12, 9, 400002]</td>\n",
       "      <td>[12, 14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>We are sure we will get back to the surface wi...</td>\n",
       "      <td>We are</td>\n",
       "      <td>We 're</td>\n",
       "      <td>[53, 32, 1085, 53, 43, 169, 137, 4, 0, 2283, 1...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[400001, 53, 267, 400002]</td>\n",
       "      <td>[53, 32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2149</th>\n",
       "      <td>If you do not start out with a truly good batt...</td>\n",
       "      <td>do not</td>\n",
       "      <td>do n't</td>\n",
       "      <td>[83, 81, 88, 36, 465, 66, 17, 7, 4702, 219, 12...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[400001, 88, 70, 400002]</td>\n",
       "      <td>[88, 36]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>He has the highest Acrolinx Score and therefor...</td>\n",
       "      <td>He has</td>\n",
       "      <td>He 's got</td>\n",
       "      <td>[18, 31, 0, 1240, 1654, 5, 2317, 0, 1240, 1506...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[400001, 18, 9, 405, 400002]</td>\n",
       "      <td>[18, 31]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>Four hundred years on , take time to show a li...</td>\n",
       "      <td>it is</td>\n",
       "      <td>probably take</td>\n",
       "      <td>[133, 3079, 82, 13, 1, 190, 79, 4, 273, 7, 333...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>[400001, 965, 190, 400002]</td>\n",
       "      <td>[20, 14]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentence Original  \\\n",
       "4121  The result is high-impact content that is on-b...  that is   \n",
       "1922  We are sure we will get back to the surface wi...   We are   \n",
       "2149  If you do not start out with a truly good batt...   do not   \n",
       "1645  He has the highest Acrolinx Score and therefor...   He has   \n",
       "1909  Four hundred years on , take time to show a li...    it is   \n",
       "\n",
       "        Replacement                                            x_token  \\\n",
       "4121        that 's  [0, 712, 14, 157821, 2768, 12, 14, 5, 1, 3023,...   \n",
       "1922         We 're  [53, 32, 1085, 53, 43, 169, 137, 4, 0, 2283, 1...   \n",
       "2149         do n't  [83, 81, 88, 36, 465, 66, 17, 7, 4702, 219, 12...   \n",
       "1645      He 's got  [18, 31, 0, 1240, 1654, 5, 2317, 0, 1240, 1506...   \n",
       "1909  probably take  [133, 3079, 82, 13, 1, 190, 79, 4, 273, 7, 333...   \n",
       "\n",
       "      y_idx_start  y_idx_end                        y_repl    x_orig  \n",
       "4121          5.0        8.0       [400001, 12, 9, 400002]  [12, 14]  \n",
       "1922          0.0        3.0     [400001, 53, 267, 400002]  [53, 32]  \n",
       "2149          2.0        5.0      [400001, 88, 70, 400002]  [88, 36]  \n",
       "1645          0.0        3.0  [400001, 18, 9, 405, 400002]  [18, 31]  \n",
       "1909         20.0       23.0    [400001, 965, 190, 400002]  [20, 14]  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data to arrays from df, add PRE-padding\n",
    "\n",
    "X = pad_sequences(df['x_token'], value = pad, padding = 'post').astype('int64')\n",
    "X_orig = pad_sequences(df['x_orig'], value = pad, padding = 'post').astype('int64')\n",
    "y_repl = pad_sequences(df['y_repl'], value = pad, padding = 'post').astype('int64')\n",
    "\n",
    "# set up target data from output sequence, 1 timestep off from y_rep\n",
    "y_idx_s = np.array(df['y_idx_start'])\n",
    "y_idx_start = to_categorical(np.array(df['y_idx_start']), num_classes = X.shape[1], dtype = 'int64')\n",
    "y_idx_end = to_categorical(np.array(df['y_idx_end']), num_classes = X.shape[1], dtype = 'int64')\n",
    "#y_repl_cat = np.array([to_categorical(x, num_classes = embedding.shape[0]) for x in y_repl]) \n",
    "\n",
    "input_len = X.shape[1]\n",
    "orig_len = X_orig.shape[1]\n",
    "repl_len = y_repl.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function taken from https://github.com/datalogue/keras-attention/issues/15\n",
    "def _time_distributed_dense(x, w, b=None, dropout=None,\n",
    "                        input_dim=None, output_dim=None,\n",
    "                        timesteps=None, training=None):\n",
    "    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        w: weight matrix.\n",
    "        b: optional bias vector.\n",
    "        dropout: wether to apply dropout (same dropout mask\n",
    "            for every temporal slice of the input).\n",
    "        input_dim: integer; optional dimensionality of the input.\n",
    "        output_dim: integer; optional dimensionality of the output.\n",
    "        timesteps: integer; optional number of timesteps.\n",
    "        training: training phase tensor or boolean.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    if not input_dim:\n",
    "        input_dim = K.shape(x)[2]\n",
    "    if not timesteps:\n",
    "        timesteps = K.shape(x)[1]\n",
    "    if not output_dim:\n",
    "        output_dim = K.shape(w)[1]\n",
    "\n",
    "    if dropout is not None and 0. < dropout < 1.:\n",
    "        # apply the same dropout pattern at every timestep\n",
    "        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
    "        dropout_matrix = K.dropout(ones, dropout)\n",
    "        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
    "        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n",
    "\n",
    "    # collapse time dimension and batch dimension together\n",
    "    x = K.reshape(x, (-1, input_dim))\n",
    "    x = K.dot(x, w)\n",
    "    if b is not None:\n",
    "        x = K.bias_add(x, b)\n",
    "        \n",
    "    # reshape to 3D tensor\n",
    "    if K.backend() == 'tensorflow':\n",
    "        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n",
    "        x.set_shape([None, None, output_dim])\n",
    "    else:\n",
    "        x = K.reshape(x, (-1, timesteps, output_dim))\n",
    "        \n",
    "    return x\n",
    "\n",
    "# pointer network implementation\n",
    "class PointerNet(LSTM):\n",
    "    def __init__(self, units, *args, **kwargs):\n",
    "        super().__init__(units, *args, **kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # immediately set variables for later use\n",
    "        # keep same number as units as encoder LSTM by default\n",
    "        self.num_units = input_shape[2]\n",
    "        self.seq_len = input_shape[1]\n",
    "        \n",
    "        # add trainable attention weights\n",
    "        self.W1 = self.add_weight(name=\"W1\",\n",
    "                                  shape=(self.num_units, 1),\n",
    "                                  initializer=\"uniform\",\n",
    "                                  trainable=True)\n",
    "        self.W2 = self.add_weight(name=\"W2\",\n",
    "                                  shape=(self.num_units, 1),\n",
    "                                  initializer=\"uniform\",\n",
    "                                  trainable=True)\n",
    "        self.vt = self.add_weight(name=\"vt\",\n",
    "                                  shape=(self.seq_len, 1),\n",
    "                                  initializer='uniform',\n",
    "                                  trainable=True)\n",
    "        \n",
    "        super(PointerNet, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        initial_state = self.get_initial_state(x)\n",
    "                \n",
    "        pointer, _, _ = K.rnn(self.step, x, initial_state, \n",
    "                              constants = [x], input_length = self.seq_len)\n",
    "        \n",
    "        return pointer # only need 1 pointer for whole sequence, so h/c don't matter for this task\n",
    "    \n",
    "    def step(self, x_input, states):\n",
    "        # x_input = original input at current time stamp (batch_size, num_units)\n",
    "        # states = 3 tensors:\n",
    "        # states[0] = h hidden state (batch_size, num_units)\n",
    "        # states[1] = c cell state/memory (batch_size, num_units)\n",
    "        # states[2] = x next word input (batch_size, seq_len, num_units)        \n",
    "        encoded = states[2]\n",
    "        _, [h, c] = self.cell.call(x_input, states[0:2])\n",
    "        decoded = K.repeat(h, self.seq_len)\n",
    "\n",
    "        # vt*tanh(W1*e+W2*d)\n",
    "        W1_eij = _time_distributed_dense(encoded, self.W1, output_dim=1)\n",
    "        W2_dij = _time_distributed_dense(decoded, self.W2, output_dim=1)\n",
    "        U = self.vt * tanh(W1_eij + W2_dij)\n",
    "        U = K.squeeze(U, 2) # removes a 1-dimension at 2nd axis\n",
    "\n",
    "        # softmax over U to get probability distribution over input length\n",
    "        pointer = softmax(U)\n",
    "        return pointer, [h, c]\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # input shape should be (batch_size, seq_len, units)\n",
    "        # output shape should be (batch_size, seq_len)\n",
    "        return (input_shape[0], input_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "main_input = Input(shape = (input_len,), dtype = 'int64', name = 'main_input')\n",
    "orig_input = Input(shape = (orig_len,), dtype = 'int64', name = 'orig_input')\n",
    "repl_input = Input(shape = (repl_len,), dtype = 'int64', name = 'repl_input')\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    # note for later: can use mask_zero parameter in embedding layer, but would need to go back and change some indices\n",
    "    embedding_layer = Embedding(input_dim = embedding.shape[0],\n",
    "                          output_dim = embedding.shape[1],\n",
    "                          weights = [embedding],\n",
    "                          trainable = False, \n",
    "                          name = 'embedding_layer')\n",
    "    input_embed = embedding_layer(main_input)\n",
    "    orig_embed = embedding_layer(orig_input)\n",
    "    repl_embed = embedding_layer(repl_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_units = 128\n",
    "\n",
    "lstm = LSTM(return_sequences = True, units = num_units, name = 'lstm')(input_embed)\n",
    "y_start = PointerNet(units = num_units, activation=\"softmax\", input_shape = (input_len, num_units), name = 'y_start')(lstm)\n",
    "y_end = PointerNet(units = num_units, activation=\"softmax\", input_shape = (input_len, num_units), name = 'y_end')(lstm)\n",
    "y_start_sparse = Lambda(lambda x: K.argmax(x, axis=1), name='y_start_sparse')(y_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "### feed encoder input (main_input), decoder input (repl_input) and sliced replacement text to enc-dec system\n",
    "\n",
    "# these should change later to some sort of context-based or conditional model\n",
    "# also with attention\n",
    "\n",
    "# decoder given 2*units to accept bidirectional outputs\n",
    "encoder = Bidirectional(LSTM(return_state = True, units = num_units), name = \"encoder\")\n",
    "decoder = LSTM(return_sequences = True, return_state = True, name = \"decoder\", units = 2 * num_units)\n",
    "\n",
    "# sequence is unnecessary for the encoder - just states, to start the decoder correctly\n",
    "# state and sequence for decoder will be necessary in inference, but not right now\n",
    "enc_output, enc_h_forward, enc_c_forward, enc_h_backward, enc_c_backward = encoder(orig_embed)\n",
    "enc_h = Concatenate()([enc_h_forward, enc_h_backward])\n",
    "enc_c = Concatenate()([enc_c_forward, enc_c_backward])\n",
    "dec_output, _, _ = decoder(repl_embed, initial_state = [enc_h, enc_c])\n",
    "\n",
    "# Dropout?\n",
    "\n",
    "dec_tdd = TimeDistributed(Dense(embedding.shape[0], activation='softmax'), \n",
    "                               name = 'y_repl_output')\n",
    "y_repl_output = dec_tdd(dec_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Pointer - find indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "4836/4836 [==============================] - 50s 10ms/step - loss: 4.2053 - acc: 0.0196\n"
     ]
    }
   ],
   "source": [
    "pointer_model = Model(inputs = main_input, outputs = [y_start])\n",
    "#pointer_opt = optimizers.Adam(lr=.1, decay=0.0)\n",
    "pointer_model.compile(optimizer = 'adam',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "pointer_history = pointer_model.fit(X, [y_idx_start], epochs = 1, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 68)                0         \n",
      "_________________________________________________________________\n",
      "embedding_layer (Embedding)  multiple                  120000900 \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 68, 128)           219648    \n",
      "_________________________________________________________________\n",
      "y_start (PointerNet)         (None, 68)                131584    \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None,)                   0         \n",
      "=================================================================\n",
      "Total params: 120,352,132\n",
      "Trainable params: 351,232\n",
      "Non-trainable params: 120,000,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pointer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'y_start_acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-c41ccf5af445>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpointer_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_start_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'y_start'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpointer_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_end_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'y_end'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'y_start_acc'"
     ]
    }
   ],
   "source": [
    "plt.plot(pointer_history.history['y_start_acc'], label='y_start')\n",
    "#plt.plot(pointer_history.history['y_end_acc'], label='y_end')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f71e5dd6f28>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8lNX5///XlWSyb2xhScImyKpsAVHQSrWC1n5EbdVWsYrrr7Zqaz+tttZuH/21tVW7K637Wlu02loVrfsCGhZliSyySFjDEpKQhSRzvn/MBGNIyCTMzD2ZvJ+Pxzzumfs+c9/XDYEr55z7nGPOOURERNqT4HUAIiLSNShhiIhISJQwREQkJEoYIiISEiUMEREJiRKGiIiERAlDRERCooQh0glmttHMTvU6DpFoUsIQEZGQKGGIhJGZXWFm68xsj5k9a2YDgvvNzO40s51mVmFmy81sbPDYGWa2yswqzWyLmX3X27sQaZ0ShkiYmNnngf8fOA/oD2wCnggePg04CTgayAmW2R08di9wlXMuCxgLvBLFsEVCluR1ACJx5ELgPufcEgAzuwnYa2aDgXogCxgJvOecK2n2vXpgtJl94JzbC+yNatQiIVINQyR8BhCoVQDgnKsiUIvId869AvwB+COw08zmmVl2sOi5wBnAJjN73cyOj3LcIiFRwhAJn63AoKYPZpYB9AK2ADjnfuecmwSMJtA09b/B/e87584C8oB/Ak9GOW6RkChhiHSez8xSm17A48ClZjbezFKA24BFzrmNZjbZzI4zMx+wH6gF/GaWbGYXmlmOc64eqAD8nt2RyGEoYYh03n+Ammavk4EfAfOBbcBRwAXBstnAXwj0T2wi0FR1e/DYHGCjmVUAVxPoCxGJOaYFlEREJBSqYYiISEgiljDMrNDMXg0OSFppZte1UuZCM/swOIjpHTMb1+zYLDNbHRwEdWOk4hQRkdBErEnKzPoD/Z1zS8wsC1gMzHbOrWpW5gSgxDm318xOB37inDvOzBKBNcAXgFLgfeCrzb8rIiLRFbEahnNuW9MAJudcJVAC5Lco805woBLAQqAg+H4KsM45t945d4DAaNmzIhWriIi0LyojvYMjXScAiw5T7DLg+eD7fGBzs2OlwHFtnPtK4EqAjIyMSSNHjjzCaEVEuo/Fixfvcs71CaVsxBOGmWUSeMzweudcRRtlZhBIGNM7en7n3DxgHkBRUZErLi4+gmhFRLoXM9vUfqmAiCaM4CCl+cCjzrmn2ihzLPBX4HTnXNNkbFuAwmbFCoL7RETEI5F8SsoIzMJZ4py7o40yA4GngDnOuTXNDr0PDDezIWaWTGDw07ORilVERNoXyRrGNAIjWJeb2bLgvh8AAwGcc3cDtxCYa+dPgfxCg3OuyDnXYGbfBF4EEgnMALoygrGKiEg7IpYwnHNvAdZOmcuBy9s49h8CUy+IiHRIfX09paWl1NbWeh1KzEhNTaWgoACfz9fpc2g9DBGJO6WlpWRlZTF48GCCrRfdmnOO3bt3U1paypAhQzp9Hk0NIiJxp7a2ll69eilZBJkZvXr1OuIalxKGiMQlJYvPCsefR7dPGPWNfu55/WMWb9KqmCIih9PtE8aBBj8PvLORm/+5goZGrVsjIrHhtttu69T37rrrLqqrq8McTUC3TxgZKUn86MzRlGyr4OGFIQ94FBGJqM4kjMbGRiWMSDt9bD9OHN6bOxasYWeFHsMTkSN3yy23cNdddx38/MMf/pDf/va3h5Tbtm0bJ510EuPHj2fs2LG8+eab3HjjjdTU1DB+/HguvDCwAOPs2bOZNGkSY8aMYd68eQe/n5mZyQ033MC4ceO49dZb2bp1KzNmzGDGjBlhv6e4WnHvSOaS2rBrPzPvfIMzjunHXRdMCHNkIhJNJSUljBo1CoCf/mslq7a2Oo1dp40ekM2PvzTmsGU2btzIOeecw5IlS/D7/QwfPpz33nuPXr16fabcb37zG2pra/nhD39IY2Mj1dXVZGVlkZmZSVVV1cFye/bsoWfPntTU1DB58mRef/31g0+C/e1vf+O8884DYPDgwRQXF9O7d+9DYmr+59LEzBY754pCuW+Nwwga0juDqz43lN+/so4Lpgxk6tBe7X9JRKQNgwcPplevXixdupQdO3YwYcKEQ5IFwOTJk5k7dy719fXMnj2b8ePHt3q+3/3udzz99NMAbN68mbVr19KrVy8SExM599xzI3ovTZQwmvnGycN4eukWbnlmBc9deyK+RLXYiXR17dUEIunyyy/ngQceYPv27cydO7fVMieddBJvvPEGzz33HJdccgnf+c53uPjiiz9T5rXXXuPll1/m3XffJT09nZNPPvngmIrU1FQSExMjfi+gPozPSEtO5CdfGsOaHVXc//YGr8MRkS7u7LPP5oUXXuD9999n5syZrZbZtGkTffv25YorruDyyy9nyZIlAPh8Purr6wHYt28fPXr0ID09nY8++oiFCxe2ec2srCwqKyvDfzOohnGIU0f35dRRedz18lq+NG4A/XPSvA5JRLqo5ORkZsyYQW5ubpu1gNdee43bb78dn89HZmYmDz30EABXXnklxx57LBMnTuS+++7j7rvvZtSoUYwYMYKpU6e2ec0rr7ySWbNmMWDAAF599dWw3o86vVuxeU81p97xOqeO6ssfL5wYhshEJJpa69z1gt/vZ+LEifz9739n+PDhXodzxJ3eapJyDta+BDtLDu4q7JnON2cM47nl23hjTZmHwYlIV7Vq1SqGDRvGKaecEhPJIhzUJGUGT14MRXNh5q0Hd1/5uaHMX1LKbf8p4cThvTUvjYh0yOjRo1m/fv3Bz8uXL2fOnDmfKZOSksKiRYuiHVqnKWEApOZAbflndqUkJXL1547ixqeWs2jDHj1mKyJH5JhjjmHZsmXtF4xhapICSM2FmvJDdp81Pp/cdB8PvrMx+jGJiMQYJQyAtFyo3Xfo7uREzp9cyIsrt7OlvMaDwEREYocSBgRqGLWH1jAA5kwdBMAjmphQRLo5JQwI1DBqDq1hABT0SOcLo/vy+HufUFvfGOXARERihxIGtNrp3dwlJwyhvLqeZ5dtjWJQIiKxRQkDAk1SdRXgb70GMXVoT0b0zeL+dzYSTwMdRaTr27hxI2PHjo3KtZQwINAkBa12fENgLdxLpg2mZFsF72/UUq4i0j1pHAYEahgQaJZK79lqkdnj8/nF8x/xwDsbmDKk9TIiEoOevxG2Lw/vOfsdA6f/4rBFbrnlFnr27Mn1118PBBZQysvL47rrrjuk7O23386TTz5JXV0dZ599Nj/96U/ZuHEjp59+OtOnT+edd94hPz+fZ555hrS0NBYvXnxw9tvTTjstvPd2GKphwKc1jFbGYhwskpzIBZMLeXHlDrbqEVsRacfcuXMPTiTo9/t54oknuOiiiw4pt2DBAtauXct7773HsmXLWLx4MW+88QYAa9eu5ZprrmHlypXk5uYyf/58AC699FJ+//vf88EHH0TvhlANIyA1J7A9TMc3wEVTB/GXN9fzyMJNfG/WyCgEJiJHrJ2aQKSEuoDSggULWLBgARMmBFb6rKqqYu3atQwcOJAhQ4YcXFBp0qRJbNy4kfLycsrLyznppJMAmDNnDs8//3xU7kkJAz5tkjpMDQMCkxKeOirwiO21pwwn1RedRUtEpGsKZQEl5xw33XQTV1111Wf2b9y4kZSUlIOfExMTqanxtnVDTVLQbqd3c5ecMJi91fU8+4EesRWRwwtlAaWZM2dy3333HVy/e8uWLezcubPNc+bm5pKbm8tbb70FwKOPPhr+wNugGgZ8ttO7Hccf1Yuj+2by4DsbOa+oMMKBiUhXFsoCSqeddholJSUcf/zxAGRmZvLII48cdtnV+++/n7lz52JmUe301gJKEFgT4//yYOo34As/bbf4/W9v4Kf/WsV/b/gcR/XJ7ESkIhJJWkCpdVpAKRzM2h3t3dzMMf0AeHHl9khGJSJdmBZQimdtTHHemgG5aYwryOHFFdv5xsnDIhyYiHRFWkApnrUxxXlbZo7tx69eWM3W8hoG5KZFMDAR6QznXEytlOn1Akrh6H5Qk1STw0xx3pqmZqkFapYSiTmpqans3r1bc78FOefYvXs3qampR3Qe1TCapOXC7nUhFz+qTybD8zJ5ceUOLpk2JIKBiUhHFRQUUFpaSllZmdehxIzU1FQKCgqO6BxKGE060OndZOaYfvzptXXs2X+AnhnJEQpMRDrK5/MxZIh+kQu3iDVJmVmhmb1qZqvMbKWZHTLjlpmNNLN3zazOzL7b4th1ZrYi+N3rIxXnQanBPgy/P+SvzBrbD7+Dl0t2RDAwEZHYEMk+jAbgBufcaGAqcI2ZjW5RZg9wLfDr5jvNbCxwBTAFGAecaWaRfRwpLRecHw5UhfyVMQOyyc9N48UV6scQkfgXsYThnNvmnFsSfF8JlAD5LcrsdM69D9S3+PooYJFzrto51wC8DpwTqViBDo32bmJmzBzTjzfX7aKqriFCgYmIxIaoPCVlZoOBCUCoDxyvAE40s15mlg6cAbQ6D4eZXWlmxWZWfEQdXCFMcd6amWP6cqDBz+ur1bkmIvEt4gnDzDKB+cD1zrmKUL7jnCsBfgksAF4AlgGtrp/qnJvnnCtyzhX16dOn84GGOMV5S0WDe9IrI5kX9HitiMS5iCYMM/MRSBaPOuee6sh3nXP3OucmOedOAvYCayIR40EhTnHeUmKC8YXRfXn1o53UNbS+JriISDyI5FNSBtwLlDjn7ujE9/OC24EE+i8eC2+ELXRgivOWZo7tR1VdA++s2x3moEREYkckx2FMA+YAy82saTz8D4CBAM65u82sH1AMZAP+4OOzo4NNV/PNrBeBDvFrnHMd+9W/ozrR6d3khKN6kZmSxIsrtzNjZF6YAxMRiQ0RSxjOubeAw07k4pzbDrQ69NA5d2Ik4mpTShZYQoebpABSkhKZMTKPl1bt4NazHYkJsTN/jYhIuGguqSYdnOK8pVlj+rF7/wGKN+4Jc2AiIrFBCaO5Dkxx3tLJI/qQnJTAiys16ltE4pMSRnMdnOK8uYyUJE4a3psXV27XDJkiEpeUMJrr4BTnLc0c048t5TV8UNq5pCMiEsuUMJpL63yTFAQer01OSuCfS7eEMSgRkdighNHcEXR6A2Sn+vjC6L48+8FWDjSEPuutiEhXoITRXFOn9xH0QZw7MZ89+w/w+hrNLSUi8UUJo7m0XPDXQ31Np09x4vA+9M5M5qklpWEMTETEe0oYzR3BaO8mvsQE/mdcPv8t2Ul59YEwBSYi4j0ljOY6OcV5S+dMzOdAo59/f7gtDEGJiMQGJYzmOjnFeUtjBmRzdN9MntbTUiISR5Qwmkvt/Iy1zZkZ50wsYPGmvWzctT8MgYmIeE8Jo7kwNUkBzB6fjxk8pVqGiMQJJYzmwtDp3aRfTirTh/XmqSWl+P2aKkREuj4ljOaa+jDCUMOAQOd36d4aijftDcv5RES8pITRXEIipGSHpYYBgbml0pMTNSZDROKCEkZLqZ2fsbal9OQkZo3tx3MfbqO2Xut9i0jXpoTRUlpO2JqkAM6dWEBlXQMvl2idDBHp2pQwWjrCKc5bmjq0F/1zUnlqiZ6WEpGuTQmjpSOc4rylxATjrPH5vL6mjLLKurCdV0Qk2pQwWjrCKc5bc+7EfBr9jocXbgrreUVEokkJo6Uwdno3Gd43izOO6cdf3ljPjorasJ5bRCRalDBaSsuF+mpoCO9Ms9+fNZIGv5/fLFgd1vOKiESLEkZLYRzt3dygXhl8/fjB/H1xKau2VoT13CIi0aCE0VJaj8A2jB3fTb75+WFkp/q47T8luCNY1U9ExAtKGC2FaYrz1uSmJ3PtKcN5a90uXtMSriLSxShhtBSmKc7bMmfqIAb1Sue250poaPRH5BoiIpGghNFSGKc4b01yUgI3zhrJ2p1VPFmsOaZEpOtQwmgpQp3ezc0a24/Jg3twx0urqapriNh1RETCSQmjpTBPcd4aM+MHZ4xiV9UB7n7t44hdR0QknJQwWkpKBl96RGsYABMG9uBL4wbwlzfXs7W8JqLXEhEJByWM1oR5AsK2fG/mCBxwyzMr9ZitiMQ8JYzWhHkCwrYU9kznezNH8HLJDv7y5vqIX09E5EgoYbQmAvNJteWy6UOYNaYfv3xhNe9t2BOVa4qIdIYSRmtSw7uI0uGYGb/6yrEU9kjjm48t0RToIhKzlDBakxadPowm2ak+/nThJPbV1HPt40tp9Ks/Q0RiT8QShpkVmtmrZrbKzFaa2XWtlBlpZu+aWZ2ZfbfFsW8Hv7fCzB43s9RIxXqIKDZJNRk9IJufzx7Lu+t3c+dLa6J6bRGRUESyhtEA3OCcGw1MBa4xs9EtyuwBrgV+3XynmeUH9xc558YCicAFEYz1s9Jyoa4C/I1RuyTAeUWFnFdUwB9eXccrH2kNcBGJLRFLGM65bc65JcH3lUAJkN+izE7n3PtAfSunSALSzCwJSAe2RirWQ0R4PqnD+dlZYxnVP5tv/+0DSvdWR/36IiJtiUofhpkNBiYAi0Ip75zbQqDW8QmwDdjnnFvQxrmvNLNiMysuKwvTDLAHR3vvDc/5OnJpXyJ/vnAifr/j6kcWU3MgurUcEZG2RDxhmFkmMB+43jkX0spBZtYDOAsYAgwAMszsotbKOufmOeeKnHNFffr0CU/QaZGfT+pwBvfO4M7zx7NyawXfeXIZfnWCi0gMiGjCMDMfgWTxqHPuqQ589VRgg3OuzDlXDzwFnBCJGFvlYZNUk1NH9+WHZ4zi+RXb+bWWdRWRGJAUqRObmQH3AiXOuTs6+PVPgKlmlg7UAKcAxWEOsW0RnuI8VJdNH8LHZVX86bWPGdonky9PKvA0HhHp3iKWMIBpwBxguZktC+77ATAQwDl3t5n1I5AIsgG/mV0PjHbOLTKzfwBLCDxttRSYF8FYPysKU5yHwsz42Vlj2bS7mpue+pCBPdOZMqSnpzGJSPcVsYThnHsLsHbKbAda/bXZOfdj4McRCK19UZjiPFS+xAT+fOEkzv7z21z1cDFPf2Mag3tneB2WiHRDGundGl8aJCZ72ofRXE66j/u+PhkHzH3wffZVt/YUsohIZClhtMYsalOch2pw7wzuuWgSm/dUc81jS6jXeuAiEmVKGG2J0hTnHXHc0F7cdvYxvLVuFz/9l9bQEJHoimSnd9cWYzWMJl8pKmRdWRX3vL6e4XlZfP2EwV6HJCLdhGoYbYniFOcd9b2ZIzl1VF9++q+VvLEmTKPbRUTaoYTRlrToz1gbqsQE47cXjOfovllc89gS1u2s9DokEekGlDDaEqNNUk0yUpK495LJpCQlcNmDxezdf8DrkEQkzoWUMMzsOjPLtoB7zWyJmZ0W6eA81VTD8Mfu00j5uWncM6eIbftqufqRxRxoiN1YRaTrC7WGMTc4ceBpQA8CI7h/EbGoYkFqLjg/HIjt5p5Jg3rwq3OPZdGGPfz42RV6ckpEIibUp6SaRmyfATzsnFsZnCsqfjUf7d30PkbNnpDPup1V/OHVdQzPy2Lu9CFehyQicSjUGsZiM1tAIGG8aGZZQHy3f6R5P2NtR3znC0dz2ui+3PqfEt5et8vrcEQkDoWaMC4DbgQmO+eqAR9wacSiigUxMgFhqBISjDvOH89RfTL4xqNL2LR7v9chiUicCTVhHA+sds6VBxcyuhnoGr96d1aMTHHeEZkpSfz14smYwRUPFVNV1+B1SCISR0JNGH8Gqs1sHHAD8DHwUMSiigVdrIbRZGCvdP74tYl8XLafb/9Nq/WJSPiEmjAaXODxm7OAPzjn/ghkRS6sGBBDU5x31LRhvbn5i6N4adUO7np5jdfhiEicCPUpqUozu4nA47QnmlkCgX6M+JWSBZbYZTq9W7rkhMGUbKvgd6+sY0S/bL54bH+vQxKRLi7UGsb5QB2B8RhNix7dHrGoYoFZoJbRxZqkmpgZP589lokDc/nu3z9g1dYKr0MSkS4upIQRTBKPAjlmdiZQ65yL7z4MiMkpzjsiJSmRu+dMIifNxxUPFbNH04eIyBEIdWqQ84D3gK8A5wGLzOzLkQwsJsT4fFKhyMtKZd7FkyirquOaR7Xwkoh0XqhNUj8kMAbj6865i4EpwI8iF1aMyOgNVTu9juKIHVuQyy/OOYZ31+/m1udKvA5HRLqoUBNGgnOu+f+cuzvw3a4rpxD2bfY6irA4Z2IBl00fwgPvbOTJ4vi4JxGJrlCfknrBzF4EHg9+Ph/4T2RCiiE5BVCzF+qqICXT62iO2E2nj2T19kpufnoFw/MymTCwh9chiUgXEmqn9/8C84Bjg695zrnvRzKwmJA7MLDdV+ptHGGSlJjA7786gb45KVz9yGJ2VtR6HZKIdCEhNys55+Y7574TfD0dyaBiRk5hYBsnzVIAPTKS+cvFRVTWNnDVI4upa2j0OiQR6SIOmzDMrNLMKlp5VZpZ/D/YnxtMGOWfeBtHmI3sl80d541j6Sfl3Py01tAQkdAcNmE457Kcc9mtvLKcc9nRCtIzmX0hISmuahhNZo3tz7WnDOfvi0u5/+2NXocjIl1A/D/pdCQSEiE7P276MFq6/pThzBzTl/97bhVvrCnzOhwRiXFKGO3JHQjl8VfDgOAaGueN5+i+WXzzsSWsL6vyOiQRiWFKGO2Jo7EYrclISeIvFxeRlJjA5Q8VU1Fb73VIIhKjlDDak1MAldugMX7/Iy3smc6fL5zIJ7urufbxpTRqDQ0RaYUSRntyC8H5oWKr15FE1HFDe/Gzs8by2uoyfvnCR16HIyIxSAmjPXE4FqMtXztuIBcfP4h5b6xn/uL47OgXkc5TwmhP02jvOO34bulHZ47m+KG9uOmp5by/cY/X4YhIDFHCaE92fmDbDWoYAL7EBP580UTye6Rx1cOL+WR3tdchiUiMUMJojy8VMvK6TcIAyE1P5r5LJtPod1z6wHvsq4nfDn8RCZ0SRihyC7tNk1STIb0zuGfOJD7ZU62Fl0QEiGDCMLNCM3vVzFaZ2Uozu66VMiPN7F0zqzOz7zbbP8LMljV7VZjZ9ZGKtV1xPhajLVOH9uK2s4/hrXW7+PGzKzXnlEg3F+p6GJ3RANzgnFtiZlnAYjN7yTm3qlmZPcC1wOzmX3TOrQbGA5hZIrAF8G6G3JwCWPMCOAdmnoXhha8UFbJ+137+/NrHDO2dweUnDvU6JBHxSMRqGM65bc65JcH3lUAJkN+izE7n3PvA4RrJTwE+ds5tilSs7codCA21sH+XZyF46X9PG8HpY/tx639KeGnVDq/DERGPRKUPw8wGAxOARZ34+gV8utJfa+e+0syKzay4rCxCE+gdHIsRX9Och6ppzqlj8nO47omlfFha7nVIIuKBiCcMM8sE5gPXO+c6tIaGmSUD/wP8va0yzrl5zrki51xRnz59jizYthxcF6P79WM0SUtO5K8XF9EjPZm5D7yvx21FuqGIJgwz8xFIFo86557qxClOB5Y457xtB+lGo70PJy87lQfnTqa+0XHJ/e+xZ/8Br0MSkSiK5FNSBtwLlDjn7ujkab7KYZqjoiY1B5Kz4nZdjI4YlpfFvV8vorS8hssffJ+aA1riVaS7iGQNYxowB/h8s8djzzCzq83sagAz62dmpcB3gJvNrNTMsoPHMoAvAJ2pmYSXWbcci9GWosE9+d0F41m6uZzrntDstiLdRcQeq3XOvQUc9hlU59x2oKCNY/uBXhEIrXNyCrttp3drZo3tz4/PHM1P/rWKnzy7kp+dNQbrZo8ci3Q3kRyHEV9yC2FzZx7yil+XTBvCtn213PPGevrnpvKNk4d5HZKIRJASRqhyCqC2HOoqISXL62hixvdnjWTbvlp+9cJq+mSm8JWiQq9DEpEI0VxSoTr4pJQ6vptLSDBu/8qxnDi8N9+f/yEvrNjmdUgiEiFKGKHqZutidERKUiL3zJnEhIE9+NbjS3ljTYQGUIqIp5QwQtXNR3u3Jz05ifsumcywvCyuengxizdp8SWReKOEEarMvpDgUw3jMHLSfDw0dwr9clK55P73WbW1QwP7RSTGKWGEKiEBcvLVh9GOPlkpPHL5cWSlJHHxfYtYX1bldUgiEiZKGB3RTdfF6Kj83DQevvw4nIOL/rqILeU1XockImGghNERuQPVJBWio/pk8tBlU6isa+CCee9SuleTFYp0dUoYHZFTAJXboEGT7oVizIAcHrnsOPZV13P+PQvZvEdJQ6QrU8LoiJxCwEHlVq8j6TLGFeby2BVTqapr4Px73mXT7v1ehyQinaSE0RFaF6NTxubn8NgVx1FT38j59yxkwy4lDZGuSAmjI7QuRqeNGZDDY1dM5UCjn/PveZeP9fSUSJejhNER2cElyVXD6JRR/bN5/Iqp+J3j/HsWsnZHpdchiUgHKGF0hC81MIBPNYxOG9Evi8evmArAV+55l/c2aES4SFehhNFRGotxxIb3zWL+/3c8PdOTueivi3hm2RavQxKREChhdJRW3guLQb0yeOobJzC+MJfrnljGH19dh3NauU8klilhdFROQWB6EL/f60i6vNz0ZB6+fApnjR/A7S+u5qanllPfqD9XkVilBZQ6KmcgNNZB9S7IzPM6mi4vJSmRu84fz8Ce6fz+lXVsKa/hTxdOJCvV53VoItKCahgdpbEYYWdm3HDaCH517rG8+/FuzvnTO6zerieoRGKNEkZHaV2MiDlvciEPzp3C3uoDfOkPb/HA2xvUryESQ5QwOiqnILBVDSMipg3rzQvXn8T0Yb35yb9Wccn977OzstbrsEQEJYyOS8uFlGw9WhtBvTNTuPfrRfz8rDEsXL+b0+96k/+W7PA6LJFuTwmjM3IKtZBShJkZc44fzL+/NZ287FQue7CYm/+5nH019V6HJtJtKWF0hsZiRM3wvln885oTuHz6EB5d9Amfu/1V7n1rA3UNjV6HJtLtKGF0Rk4hlG/SWIwoSUlK5OYzR/Pvb03nmPwcfv7vVZx6x+v864Ot6hQXiSIljM4oKIK6Cti2zOtIupUxA3J4+LLjeGjuFDKSk/jW40uZ/ce3Wbh+t9ehiXQLShidMfw0sARY/bzXkXRLJx3dh+cM8cdmAAASlElEQVSuPZFff2UcOyvruGDeQs798zs8+8FWjRQXiSCLpyp9UVGRKy4ujs7F7jsdDlTC1W9F53rSqtr6Rh5d9AkPvbuRTburyctK4cLjBvHV4wrJy0r1OjyRmGdmi51zRaGUVQ2js0bMgu3L9bSUx1J9iVw2fQiv3nAy918ymVH9s7nz5TVM+8UrXPfEUhat343fHz+/FIl4SXNJddbRp8NLtwSapaZc4XU03V5CgjFjZB4zRuaxvqyKh97dxD8Wl/LMsq0U9EjjnAn5nD2xgCG9M7wOVaTLUpNUZzkHv58IPYfCRfOjc03pkOoDDSxYuYP5S0p5e90u/A4mDszl7IkFfOnY/uSmJ3sdoojnOtIkpYRxJF78Ibw3D763AVIyo3dd6bDt+2p5ZtkW5i8pZc2OKnyJxowReZwzMZ8ZI/NISUr0OkQRTyhhRMuGN+HBM+H8R2DUl6J3Xek05xwrt1bw9NItPLNsK7uq6shJ8/HFY/tzzoR8Jg3qgZl5HaZI1ChhREtjPdx+FIw8E2b/KXrXlbBoaPTz1rpdPL10Cy+u3E5tvZ+BPdP58qQCzp1UQH5umtchikRcRxKGOr2PRKIvMCZjzYvgb4QENWt0JUmJCZw8Io+TR+RRVdfAiyu2M39JKXe8tIY7X17D9GG9+fKkAmaO6UeqT3+3IhF7rNbMCs3sVTNbZWYrzey6VsqMNLN3zazOzL7b4liumf3DzD4ysxIzOz5SsR6Ro2cFVt/bstjrSOQIZKYkce6kAh67Yipvfm8G135+OOvL9nPdE8uYcuvL3PzP5azYss/rMEU8FbEmKTPrD/R3zi0xsyxgMTDbObeqWZk8YBAwG9jrnPt1s2MPAm865/5qZslAunOu/HDXjHqTFEBNeaBZ6oRvwak/ie61JaL8fsfC9bt5sngzz6/YTl2Dn3GFuXxtSiFfGjeA9GRV0KXri8k+DDN7BviDc+6lVo79BKhqShhmlgMsA4a6DgToScIAeOBM2L8LrlkY/WtLVOyrrueppaU8tugT1u6sIislidkT8vnqlIGMHpDtdXginRZzI73NbDAwAVgU4leGAGXA/Wa21Mz+amatjrgysyvNrNjMisvKysISb4eNOAPKSmDvRm+uLxGXk+7j0mlDWPDtk/j71cdz6ui+/K14M2f87s2D81gdaNA8VhLfIp4wzCwTmA9c75yrCPFrScBE4M/OuQnAfuDG1go65+Y554qcc0V9+vQJS8wdNmJWYLv6BW+uL1FjZkwe3JM7zx/Pez84hZu/OIpdVXVc+/hSpv3yFe58aQ07KrSkrMSniCYMM/MRSBaPOuee6sBXS4FS51xTjeQfBBJIbOo5FHqPgNX/8ToSiaLc9GQuP3FoYB6rSyczdkA2v/3vWqb94hW+9fhSFm/ao/U6JK5ErNfOAqOf7gVKnHN3dOS7zrntZrbZzEY451YDpwCr2vuep0acDu/+AWr3QWqO19FIFCUkBEaNzxiRx8Zd+3l44SaeLN7Mvz7YyriCHC6dNoQzjulPcpLm+pSuLZJPSU0H3gSWA02Nuz8ABgI45+42s35AMZAdLFMFjHbOVZjZeOCvQDKwHrjUObf3cNf0rNMb4JOFcN9M+PL9MPYcb2KQmFF9oIH5S7Zw/9sbWF+2n7ysFC4+fhBfO24QPTM0h5XEjph8SioaPE0Y/kb49XA46hQ49y/exCAxx+93vL62jPve2sCba3eRkpTA7PH5zDl+EGPzVRMV72mktxcSEgOjvlc/D40NkKg/Wvlsc9XaHZXc/85Gnl6yhb8Vb2bSoB5cfPwgTh+r5irpGvRTGk6jvgS15fDu772ORGLQ8L5Z3Hb2MSz8wSn86MzR7K6q47onlnHCL17hNwtWs21fjdchihyWmqTCyTn4x6Ww6hmY808Y+jnvYpGY5/c73ly3i4fe2cgrq3diwLRhvTl7Qj4zx/QjI0W1VIk89WF4qa4K/vL5wPxSV70BOQXexiNdwuY91TxZvJmnl26hdG8N6cmJzBzTj7Mn5DNtWG8SEzTlukSGEobXytYEkkafo+HS5yEpxeuIpIvw+x3Fm/by9NJS/v3hNiprG8jLSuGUUXl87ug8pg/vTaZqHhJGShixYNWz8OQcKJoLZ97pdTTSBdXWN/LqRzv514dbeXPNLirrGvAlGkWDenLyiD7MGJnH8LxMLfgkR0QJI1a8dAu8/Vs4608w4UKvo5EurL7Rz+JNe3ltdRmvrd7JR9srAeiZkczEgT2YNKgHRYN7cEx+jtbukA5RwogVjQ3wyNmw+T24bAH0H+d1RBIntu2r4fXVZRRv2suSTXtZv2s/AL5EY8yAHMYX5jJmQDZj83MYlpeJL1EPRErrlDBiSVUZzPtcYJzG3Bche4DXEUkc2l1Vx9JPyln8yV4Wb9rLii37qD7QCEByUgIj+2UxZkAOYwZkM6p/NiP7ZekpLAGUMLwO41ClxfDAF8ES4Phr4IRrIVVrKEjk+P2ODbv3s3JrBSu37GPF1n2s2FLBvpp6AMxgcK8MRvfPZlT/LEYPyGZ0/xz6ZqeoT6SbUcKIRXvWwyv/ByvmQ3ovOOl7gQ7xJM0rJNHhnGNLeQ0l2ypZtbWCVdv2UbKtkk/2VB8s0ysjOZg8sg9uh/TOIElNWnFLCSOWbVkCL/8YNrwBuYPglFtgzDmQoH+Q4o3K2npKtlVSsq0imEgqWL29kgONgTlDU5ISGNk/kDzGDAgkkpH9srREbZxQwoh1zsHH/4WXfgI7lkNGHxh8Igw5KfDqOTTQZiDikfpGP+vL9rNy676DSWTl1k+btBIMBvduatL6tEaSl6Umra5GCaOr8Pth1T9hzQuBGkfltsD+7IJA4igoCiSPnkMDI8YT9LikeMc5x9Z9tazcso+VWyso2VZByfYKNu/5dA6snhnJjOyXxdF9sxjRL/AanpdJVqrPw8jlcJQwuiLnYPc62PB6IHlseBNq9nx6PMEHuQMDySN3YKBWktE70B/StE3vDSlZ4EtTDUWipqK2no+2VbJqa6BP5KMdlazZXklNfePBMvm5aQzvm8lRfTIZ2ieDob0zOapPBn1UI/GcEkY88Puhcivs2RDoMN+7IfB+7wYo3ww1e4E2/u4sEVIyITkrkEBSMgNJJCkNfKmBbVJKcF8KJKZAYnKgAz6x+csHCUmBV6IvkLQSg58t8dNjCQnN9iUGtmafvk9IDDwhZsFtQkLw8+Fe+k+kK/P7HaV7a1i9o5I1OypZvT2w3bBrP3UN/oPlslKSGNIng8Ke6QwMvgp7BLb9c1M1fiQKlDC6g8aGQNKo3gX7dwW21buhrjIwAWJdJRwIbusqob4GGmqgvrbZthYa6qCxzuu7aYN9mjws4TCfW5TFWpSxQ7ftlWvzu7R9zlCuC4f/bmv30aHvtijf5j7aLnfwOm2VCfV987/KwD7njKoDDZRXN1BecyC4raeitoGK2gb8zuH49FwZKUlkpCSRmeojs9k2I8VHRnIi6Sm+4Foidsi1Pr0XWnxuefwIPodctq3YQvjzDCUmXyqMOZvO0AJK3UFiEmT2CbyOlHPgbwgmjwPNXvWBlQT99cH3DYGtawy89zcGXw2Blwt+dv7Ay9946L7PHAu+xzU75gLHcIH3h5QJbqHFZ3f497hghaytY/5m71vuo5Vj7W1b+a6/5TUPsz0YHx28bnD7mZhpZV9r5Zp+eWyx/7Dv2yh/8PNnjxuQhSMLKGx+3DmcL/DeNb/nRqDaQTUktFWjFsjI63TC6AglDAn8tpLoC7xEPNLid+ZDOOeoqG1g274adlbUUVZRS1lVHWWVteyqrGNnZS17qurYs/8A5TUH8DuwYJL59NwOX6KRk+ojN91Hj/QkslN9ZKcmkZ0W2Oak+chKTSQn1UdmaiLZKYHPWWk+Mn0JJCRYKwn0M4E2venA57beh3gei07TnRKGiHQJZkZOmo+cNB8j+x2+rN/v2FdTz+79B9hbfYDdVQfYV3OA8up6ymvqKa8Ovq+uZ1NFPft2VFNRU09lXUM7MUBmchJZqUlkpfqC2+bvm+9LIivFR2bwfXZTs1paUpftm1HCEJG4k5Bg9MhIpkdGx2ZSaPQ7Kmvr2VdTT2VtAxU19VTU1lNR0xDcBvpbKmsbqKwNlCmrqmP9rv0H99U3tt90lupL+DS5pAQSTWZKILFkNiWeFp8zU5LIbvY5Izkx6k+YKWGIiAQlJhi56cnkpnd+yp7a+sbPJJTK2gaq6pq/b3asroGqYNmdlbUHy+w/0HBIS1dLCUYwyfgYkJvK368+odMxh0oJQ0QkjFJ9iaT6EumT1fmVNv1+x/4DDZ9JOE01m6pmyaiqLlDzSY5SE5cShohIjElIsGCTVWw9iNI1e15ERCTqlDBERCQkShgiIhISJQwREQmJEoaIiIRECUNEREKihCEiIiFRwhARkZDE1XoYZlYGbOrk13sDu8IYTleh++5edN/dSyj3Pcg5F9I6CXGVMI6EmRWHuohIPNF9dy+67+4l3PetJikREQmJEoaIiIRECeNT87wOwCO67+5F9929hPW+1YchIiIhUQ1DRERCooQhIiIh6fYJw8xmmdlqM1tnZjd6HU8kmdl9ZrbTzFY029fTzF4ys7XBbQ8vYww3Mys0s1fNbJWZrTSz64L74/q+Acws1czeM7MPgvf+0+D+IWa2KPgz/zcz6/x6pDHKzBLNbKmZ/Tv4Oe7vGcDMNprZcjNbZmbFwX1h+1nv1gnDzBKBPwKnA6OBr5rZaG+jiqgHgFkt9t0I/Nc5Nxz4b/BzPGkAbnDOjQamAtcE/47j/b4B6oDPO+fGAeOBWWY2FfglcKdzbhiwF7jMwxgj5TqgpNnn7nDPTWY458Y3G38Rtp/1bp0wgCnAOufceufcAeAJ4CyPY4oY59wbwJ4Wu88CHgy+fxCYHdWgIsw5t805tyT4vpLAfyL5xPl9A7iAquBHX/DlgM8D/wjuj7t7N7MC4IvAX4OfjTi/53aE7We9uyeMfGBzs8+lwX3dSV/n3Lbg++1AXy+DiSQzGwxMABbRTe472DSzDNgJvAR8DJQ75xqCReLxZ/4u4HuAP/i5F/F/z00csMDMFpvZlcF9YftZTzrS6CR+OOecmcXlc9ZmlgnMB653zlUEfukMiOf7ds41AuPNLBd4GhjpcUgRZWZnAjudc4vN7GSv4/HAdOfcFjPLA14ys4+aHzzSn/XuXsPYAhQ2+1wQ3Ned7DCz/gDB7U6P4wk7M/MRSBaPOueeCu6O+/tuzjlXDrwKHA/kmlnTL4vx9jM/DfgfM9tIoIn588Bvie97Psg5tyW43UngF4QphPFnvbsnjPeB4cEnKJKBC4BnPY4p2p4Fvh58/3XgGQ9jCbtg+/W9QIlz7o5mh+L6vgHMrE+wZoGZpQFfINCH8yrw5WCxuLp359xNzrkC59xgAv+eX3HOXUgc33MTM8sws6ym98BpwArC+LPe7Ud6m9kZBNo8E4H7nHO3ehxSxJjZ48DJBKY83gH8GPgn8CQwkMDU8Oc551p2jHdZZjYdeBNYzqdt2j8g0I8Rt/cNYGbHEujkTCTwy+GTzrmfmdlQAr999wSWAhc55+q8izQygk1S33XOndkd7jl4j08HPyYBjznnbjWzXoTpZ73bJwwREQlNd2+SEhGREClhiIhISJQwREQkJEoYIiISEiUMEREJiRKGiIfM7OSmGVVFYp0ShoiIhEQJQyQEZnZRcG2JZWZ2T3BSvyozuzO41sR/zaxPsOx4M1toZh+a2dNN6w+Y2TAzezm4PsUSMzsqePpMM/uHmX1kZo8GR6djZr8IruPxoZn92qNbFzlICUOkHWY2CjgfmOacGw80AhcCGUCxc24M8DqBkfMADwHfd84dS2CEedP+R4E/BtenOAFomkF0AnA9gTVZhgLTgqNzzwbGBM/zf5G9S5H2KWGItO8UYBLwfnCq8FMI/MfuB/4WLPMIMN3McoBc59zrwf0PAicF5/jJd849DeCcq3XOVQfLvOecK3XO+YFlwGBgH1AL3Gtm5wBNZUU8o4Qh0j4DHgyuYjbeOTfCOfeTVsp1dp6d5nMaNQJJwbUbphBY9OdM4IVOnlskbJQwRNr3X+DLwTUGmtZIHkTg30/TDKhfA95yzu0D9prZicH9c4DXg6v9lZrZ7OA5Uswsva0LBtfvyHHO/Qf4NjAuEjcm0hFaQEmkHc65VWZ2M4GVzBKAeuAaYD8wJXhsJ4F+DghMIX13MCGsBy4N7p8D3GNmPwue4yuHuWwW8IyZpRKo4XwnzLcl0mGarVakk8ysyjmX6XUcItGiJikREQmJahgiIhIS1TBERCQkShgiIhISJQwREQmJEoaIiIRECUNERELy/wDyPMlQCs2NoAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pointer_history.history['y_start_loss'], label='y_start')\n",
    "plt.plot(pointer_history.history['y_end_loss'], label='y_end')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "#model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECOND: turn index into slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Original</th>\n",
       "      <th>Replacement</th>\n",
       "      <th>x_token</th>\n",
       "      <th>y_idx_start</th>\n",
       "      <th>y_idx_end</th>\n",
       "      <th>y_repl</th>\n",
       "      <th>x_orig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Imagine that you have just written what you be...</td>\n",
       "      <td>you have</td>\n",
       "      <td>you 've</td>\n",
       "      <td>[4779, 12, 81, 33, 120, 982, 102, 81, 733, 14,...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[400001, 81, 462, 400002]</td>\n",
       "      <td>[81, 33]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are ready to get it off your plate and sen...</td>\n",
       "      <td>You are</td>\n",
       "      <td>You 're</td>\n",
       "      <td>[81, 32, 1188, 4, 169, 20, 138, 392, 4364, 5, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[400001, 81, 267, 400002]</td>\n",
       "      <td>[81, 32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Before you hit the publish button , are you po...</td>\n",
       "      <td>you have</td>\n",
       "      <td>you 've</td>\n",
       "      <td>[106, 81, 416, 0, 6231, 6910, 1, 32, 81, 1335,...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>[400001, 81, 462, 400002]</td>\n",
       "      <td>[81, 33]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After all , you have probably worked hard to c...</td>\n",
       "      <td>you have</td>\n",
       "      <td>you 've</td>\n",
       "      <td>[49, 64, 1, 81, 33, 965, 762, 605, 4, 1210, 12...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[400001, 81, 462, 400002]</td>\n",
       "      <td>[81, 33]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maybe you are already asking yourself some of ...</td>\n",
       "      <td>you are</td>\n",
       "      <td>you 're</td>\n",
       "      <td>[1881, 81, 32, 411, 2619, 4961, 77, 3, 101, 1,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[400001, 81, 267, 400002]</td>\n",
       "      <td>[81, 32]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Original Replacement  \\\n",
       "0  Imagine that you have just written what you be...  you have     you 've   \n",
       "1  You are ready to get it off your plate and sen...   You are     You 're   \n",
       "2  Before you hit the publish button , are you po...  you have     you 've   \n",
       "3  After all , you have probably worked hard to c...  you have     you 've   \n",
       "4  Maybe you are already asking yourself some of ...   you are     you 're   \n",
       "\n",
       "                                             x_token  y_idx_start  y_idx_end  \\\n",
       "0  [4779, 12, 81, 33, 120, 982, 102, 81, 733, 14,...          2.0        5.0   \n",
       "1  [81, 32, 1188, 4, 169, 20, 138, 392, 4364, 5, ...          0.0        3.0   \n",
       "2  [106, 81, 416, 0, 6231, 6910, 1, 32, 81, 1335,...         11.0       14.0   \n",
       "3  [49, 64, 1, 81, 33, 965, 762, 605, 4, 1210, 12...          3.0        6.0   \n",
       "4  [1881, 81, 32, 411, 2619, 4961, 77, 3, 101, 1,...          1.0        4.0   \n",
       "\n",
       "                      y_repl    x_orig  \n",
       "0  [400001, 81, 462, 400002]  [81, 33]  \n",
       "1  [400001, 81, 267, 400002]  [81, 32]  \n",
       "2  [400001, 81, 462, 400002]  [81, 33]  \n",
       "3  [400001, 81, 462, 400002]  [81, 33]  \n",
       "4  [400001, 81, 267, 400002]  [81, 32]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:\t\tI do not know what to say.\n",
      "Predicted index:\t[7]\n",
      "Actual index:\t\t1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, row in df.sample(1).iterrows():\n",
    "    val = pad_sequences([row['x_token']], value = pad, maxlen = input_len, padding = 'post').astype('int64')\n",
    "    pred = pointer_model.predict(val, batch_size = 1)\n",
    "    point = np.argmax(pred, axis = 1)\n",
    "    \n",
    "    print('Sentence:\\t\\t' + row['Sentence'])\n",
    "    print('Predicted index:\\t' + str(point))\n",
    "    print('Actual index:\\t\\t' + str(int(row['y_idx_start'])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for item in X:\n",
    "\n",
    "y_idx_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESET PARAMETER ORIG_LEN HERE BASED ON PADDING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: NMT for finding suggestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently does NOT include whole context of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[1024,400003] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node training_6/Adam/Variable_20/Assign (defined at /home/users/rcramerus/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:402) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'training_6/Adam/Variable_20/Assign', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 781, in inner\n    self.run()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-30-e1439941a5e1>\", line 10, in <module>\n    nmt_history = nmt_model.fit([X_orig, y_repl], y_repl_cat, epochs = epochs, batch_size = batch_size)\n  File \"/home/users/rcramerus/.local/lib/python3.5/site-packages/keras/engine/training.py\", line 1010, in fit\n    self._make_train_function()\n  File \"/home/users/rcramerus/.local/lib/python3.5/site-packages/keras/engine/training.py\", line 509, in _make_train_function\n    loss=self.total_loss)\n  File \"/home/users/rcramerus/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/users/rcramerus/.local/lib/python3.5/site-packages/keras/optimizers.py\", line 488, in get_updates\n    vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n  File \"/home/users/rcramerus/.local/lib/python3.5/site-packages/keras/optimizers.py\", line 488, in <listcomp>\n    vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n  File \"/home/users/rcramerus/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 704, in zeros\n    return variable(v, dtype=dtype, name=name)\n  File \"/home/users/rcramerus/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 402, in variable\n    v = tf.Variable(value, dtype=tf.as_dtype(dtype), name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 213, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 176, in _variable_v1_call\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 155, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2495, in default_variable_creator\n    expected_shape=expected_shape, import_scope=import_scope)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 217, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 1395, in __init__\n    constraint=constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 1547, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/state_ops.py\", line 223, in assign\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 64, in assign\n    use_locking=use_locking, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1024,400003] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node training_6/Adam/Variable_20/Assign (defined at /home/users/rcramerus/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:402) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1024,400003] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training_6/Adam/Variable_20/Assign}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-e1439941a5e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m              metrics = ['accuracy'])\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnmt_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnmt_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_repl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_repl_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2697\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2699\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1024,400003] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node training_6/Adam/Variable_20/Assign (defined at /home/users/rcramerus/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:402) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'training_6/Adam/Variable_20/Assign', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 781, in inner\n    self.run()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-30-e1439941a5e1>\", line 10, in <module>\n    nmt_history = nmt_model.fit([X_orig, y_repl], y_repl_cat, epochs = epochs, batch_size = batch_size)\n  File \"/home/users/rcramerus/.local/lib/python3.5/site-packages/keras/engine/training.py\", line 1010, in fit\n    self._make_train_function()\n  File \"/home/users/rcramerus/.local/lib/python3.5/site-packages/keras/engine/training.py\", line 509, in _make_train_function\n    loss=self.total_loss)\n  File \"/home/users/rcramerus/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/users/rcramerus/.local/lib/python3.5/site-packages/keras/optimizers.py\", line 488, in get_updates\n    vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n  File \"/home/users/rcramerus/.local/lib/python3.5/site-packages/keras/optimizers.py\", line 488, in <listcomp>\n    vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n  File \"/home/users/rcramerus/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 704, in zeros\n    return variable(v, dtype=dtype, name=name)\n  File \"/home/users/rcramerus/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 402, in variable\n    v = tf.Variable(value, dtype=tf.as_dtype(dtype), name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 213, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 176, in _variable_v1_call\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 155, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2495, in default_variable_creator\n    expected_shape=expected_shape, import_scope=import_scope)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 217, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 1395, in __init__\n    constraint=constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 1547, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/state_ops.py\", line 223, in assign\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 64, in assign\n    use_locking=use_locking, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1024,400003] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node training_6/Adam/Variable_20/Assign (defined at /home/users/rcramerus/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:402) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "# other parameters\n",
    "# https://keras.io/examples/lstm_seq2seq/\n",
    "\n",
    "nmt_model = Model(inputs = [orig_input, repl_input], outputs = y_repl_output)\n",
    "\n",
    "nmt_model.compile(optimizer = 'adam',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "nmt_history = nmt_model.fit([X_orig, y_repl], y_repl_cat, epochs = epochs, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "repl_input (InputLayer)         (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "orig_input (InputLayer)         (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer (Embedding)     multiple             120000900   orig_input[0][0]                 \n",
      "                                                                 repl_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Bidirectional)         [(None, 1024), (None 3330048     embedding_layer[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1024)         0           encoder[0][1]                    \n",
      "                                                                 encoder[0][3]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1024)         0           encoder[0][2]                    \n",
      "                                                                 encoder[0][4]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder (LSTM)                  [(None, 4, 1024), (N 5427200     embedding_layer[2][0]            \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "y_rep_output (TimeDistributed)  (None, 4, 400003)    410003075   decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 538,761,223\n",
      "Trainable params: 418,760,323\n",
      "Non-trainable params: 120,000,900\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nmt_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nmt_history.history['acc'], label='accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nmt_history.history['loss'], label='loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference mode for NMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor Tensor(\"orig_input:0\", shape=(?, 4), dtype=int64) at layer \"orig_input\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-7388e6f7fae0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0minf_dec_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_tdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minf_dec_main\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0minf_enc_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmain_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0menc_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0minf_dec_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrepl_input\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minf_dec_states_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minf_dec_output\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minf_dec_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         nodes, nodes_by_depth, layers, layers_by_depth = _map_graph_network(\n\u001b[0;32m--> 231\u001b[0;31m             self.inputs, self.outputs)\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                                          \u001b[0;34m'The following previous layers '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                          \u001b[0;34m'were accessed without issue: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                          str(layers_with_complete_input))\n\u001b[0m\u001b[1;32m   1444\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m                     \u001b[0mcomputable_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor Tensor(\"orig_input:0\", shape=(?, 4), dtype=int64) at layer \"orig_input\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "inf_dec_h_input = Input(shape=(num_units * 2,))\n",
    "inf_dec_c_input = Input(shape=(num_units * 2,))\n",
    "inf_dec_states_input = [inf_dec_h_input, inf_dec_c_input]\n",
    "\n",
    "inf_dec_main, inf_dec_h, inf_dec_c = decoder(repl_embed, initial_state = inf_dec_states_input)\n",
    "inf_dec_states = [inf_dec_h, inf_dec_c]\n",
    "inf_dec_output = dec_tdd(inf_dec_main)\n",
    "\n",
    "inf_enc_model = Model(inputs = [orig_input], outputs = [enc_h, enc_c])\n",
    "inf_dec_model = Model([repl_input] + inf_dec_states_input, [inf_dec_output] + inf_dec_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(input_seq):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
