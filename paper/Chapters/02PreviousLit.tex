\chapter{Related Work}

\label{Chapter02}

\section{Formal and Informal Registers}

Central to this thesis is the question of what formality in language actually \textit{is}. Although it can be considered one of the most important linguistic registers, the literature on the topic contains varied definitions for formality (\cite{heylighen1999formality}). Similar in concept to, but somehow more opaque than, difficult and simple language, formal and informal tone have been a frequent topic of both empirical studies and theoretical exploration. However, there is a relative lack of experimental work on the subject as compared to text simplification.

The literature varies in which aspects of formality are focused on, and it is difficult to pin down a consensus on its exact makeup (\cite{pavlick2016empirical}). Still, there are some lexical, syntactic and semantic characteristics which most papers agree on.

\subsection{Definition} \label{2.1.1}

\begin{quote}
\small{FORMAL:} \quad\quad Those recommendations were unsolicited and undesirable. \linebreak
\small{INFORMAL:} \quad that's the stupidest suggestion EVER. 
\linebreak \mkcitation{\cite{pavlick2016empirical}}
\end{quote}

Perhaps the one thing the literature can agree on when it comes to formality, is that it cannot agree. A couple of decades ago, \cite{heylighen1999formality}, considered now to be a formative work in the field, posited that the ``lack of a good definition'' of formal language and for stylistic language more generally was holding sociolinguistic research back. More recently, empirical studies done on formal vs. informal language have noted poor inter-annotator agreement and the lack of a clear consensus on what formality is by both academics and the average human (\cite{lahiri2011informality}) (\cite{pavlick2016empirical}).

\cite{lahiri2011informality} note poor agreement on shared data by two annotators who were meant to label documents as informal or formal. They attribute this disparity to a certain arbitrariness when it comes to formality among the annotators. Similarly, \cite{pavlick2016empirical} build a bottom-up approach to human annotation, assuming that each judge has their own standards for formality. Even then, in the end they conclude that their human annotators have at best a ``reasonably coherent'' concept of what formality is.

These difficulties can stem from various aspects of formal language: for example, a judge might perceive the content of a text (talking about candy, or economics) as one level of formality, but the style (using slang, or archaic words) as another, and have to reconcile these differences. Not only that, but whether or not a document is judged as formal can be influenced by its domain, as, for example, an informally labeled news article might be more formal than an informally labeled blog post (\cite{pavlick2016empirical}).

Although there have been studies looking at formality as a binary class, for the most part the literature agrees that it lies more on a continuum, similar to other characteristics such as politeness and emotiveness (\cite{pavlick2016empirical}) (\cite{lahiri2011informality}). Importantly, while it may be a continuum, it is difficult to say that any sentence is \textit{absolutely} formal or \textit{absolutely} informal --- it is more useful to give text formality on a relative scale (\cite{heylighen1999formality}). That is, it may be more helpful to say whether a series of texts is more formal than another series of texts, rather than giving them absolute scores. \cite{pavlick2016empirical} found that it was difficult for human judges to give absolute scores of formality consistently, and that certain sentences would get values that were all across the board. Here is one example:

\begin{quote}
Governor, if this was intentionally done, whoever did it has at least one vote to go to hell.
\linebreak \mkcitation{\cite{pavlick2016empirical}}
\end{quote}

This sentence received judgments ranging from ``very formal'' to ``very informal'', and one can implicitly understand why: from the start the formal title of ``Goveror'' could clash with the informal directive to ``go to hell''. The task of labeling a text formal or informal, then, is not so straightforward.

\subsection{Context and Implicature}

Pragmatic concepts have been connected to the ideas behind formality since the early literature. Context is central to the definition that \cite{heylighen1999formality} posit: from the start they state that formal language is notable for its ``detachment, accuracy, rigidity and heaviness'' whereas informal language is more ``flexible, direct, implicit and involved'', but less informative (\cite{heylighen1999formality}). The authors go on to outline their primary theory for two levels of linguistic formality: ``deep'' and ``surface''.

Deep formality requires \textit{avoidance of ambiguity}. Specifically, this is achieved by minimizing context-dependence (the sentence or text should not need outside references to be understandable) and what the authors call ``fuzziness''. By giving precise and explicit descriptions of any context-related elements, one can disambiguate their speech and therefore achieve a higher level of formality. Some examples of terms to be avoided due to their context-dependence would be pronouns (whose antecedents could be ambiguous), words like ``here'' or ``there'', or, similarly, relative expressions of time such as ``before'' or ``tomorrow''. Fuzziness, relatedly, refers to when a word does not have a universally agreed-upon definition, or may vary in denotation among people. Abstract concepts like ``love'' may fall into this category, as may descriptors such as ``tall'' --- people may imagine different things when presented with that word (\cite{heylighen1999formality}). 

On the other hand, surface formality refers to non-context-based linguistic differences, such as replacing a colloquial word with a more official counterpart merely for the sake of form itself (\cite{heylighen1999formality}).

In sum, Heylighen and Dewaele argue against a definition of formality as following linguistic standards for those standards' sake alone. That is covered by surface formality, with its need for more ceremonious language. Deep formality, on the other hand, is attention to form so that an audience can have a precise and unambiguous understanding of the given content. It attempts to maximize the amount of information being given while still following Grice's maxims of language. They note that more formality can often be found when the distance in space, time or background between a speaker and an audience increases, likely because the lesser shared knowledge requires less context-dependent language (\cite{heylighen1999formality}).

However, evaluating how context-dependent a piece of text is can be a difficult task, even for humans. \cite{lahiri2015squinky} created a small corpus of sentences rated by human judges for formality, informativeness and implicature (essentially context-dependence). This was an ordinal rather than a binary scale, with ratings from 1-5. Among their findings was a note that implicature annotation was varied and generally unreliable among judges --- possibly because it is poorly understood (\cite{lahiri2015squinky}). If Heylighen and Dewaele's theory that context-independence is at the true heart of formality is accurate, this could pose a problem for all research looking into the subject.

In the context of this paper, it should also be noted that identifying and replacing deep formality as described here would likely require a different approach from surface formality. The former would require semantic and pragmatic information, probably across sentence boundaries, whereas the latter would be more bound to syntactic and lexical qualities.

\subsection{Other Pragmatic Aspects of Formality}

In \cite{pavlick2016empirical} the authors note that formality has a range of different dimensions. There is the serious versus the trivial (the content you are discussing), the polite versus the casual (how you might be discussing it), and the level of shared knowledge between speakers. Through their analysis of language in internet forum discussions, they theorize that formality is higher when the shared context is low, as speakers give more information in their conversations; on the other hand, when the shared context is high, conversationalists can default to more vague or imprecise language, assured that their partner will understand regardless. Additionally, they reiterate that formality is often higher when speakers dislike one another, and that, generally, people are able to adapt to one another and meet at a shared level of formality (\cite{pavlick2016empirical}).

\subsection{Word Class Frequencies (F-Score)} \label{fscore}

One of the major contributions of \cite{heylighen1999formality} was the proposal of the F-score, a simple formula taking frequencies of different word classes as input. 

Having established that words in formal sentences are more context-independent, whereas those in informal sentences are context-dependent, the authors split part-of-speech (POS) tags accordingly. Nouns, adjectives, articles and prepositions are in the context-independent (formal) category, whereas pronouns, adverbs, verbs and interjections are in the context-dependent (informal) category.

Criticism of the F-score and constraints on its usefulness have showed up in later work. Notably, it has been described as being specific to genre-level classification and not usable for shorter documents such as sentences (\cite{pavlick2016empirical}). Not only that, but while the F-score may be able to classify documents, it is opaque in its reasoning and cannot provide an explanation as to why a document is at that level of formality.

On the other hand, \cite{lahiri2011informality} explored the correlation between F-score and formality at the sentence level, using both human annotations and corpus-level F-score trends to compare. They found that the sentence-level F-score results follow the trends of the corpus-level scores, and also that the human annotations at the sentence level had a high degree of correlation with the sentence-level F-score. Thus, they argue that the F-score can, in fact, be used at the sentence level.

Perhaps inspired by Heylighen and Dewaele, \cite{fang2009adjective} undertook their own study on the relationship between word class frequency and formality. They focus specifically on adjective density, and use the large British National Corpus for their tests (\cite{bnc}). As an example, they show that academic prose has the most adjectives (9.63\%) and conversational the least (3.9\%). Their conclusion is that adjective density alone can be used to differentiate academic and non-academic writing; however, this is not necessarily the same divide as formal and informal language. The study involved human annotators ranking eight categories of text from most to least formal, and then comparing it to an automatic ranking of the categories using adjective density. Although the study was small and with only these eight classifications, it could be used as justification to include adjective density as a feature in future work.

\subsection{Lexicons}

Many works which have experimented with labeling formality have focused heavily on word lists to classify texts. While some studies have found significant success using lexicons, it also happens to be one of the easier ways to access information in a sentence or text. \cite{heylighen1999formality} note that their approach, focusing on the word-level, avoids phonetics, syntax, semantics and pragmatics, which seemed at the time too difficult to tackle (\cite{heylighen1999formality}).

\cite{dempsey2007using}, in their study of phrasal verbs across different registers of language, found that the 397 most frequently used phrasal verbs can be used to distinguish formal from informal texts (\cite{dempsey2007using}). At least to some degree, then, word lists are useful in determining text formality.

Beyond classification, \cite{sheikha2011generation} focus on text generation: their work was to generate sentences in an informal or formal style. Given a constrained input and certain information about the features of the sentence, the system would produce an output sentence in the correct style. They used two parameters in the construction of the sentence: passive or active voice (passive for formal, active for informal), and lexical choice based on the varied lexicons. There was a list for contractions and one for abbreviations, both of which were included as informal word lists. Then, there were separate lexicons for informal and formal (\cite{sheikha2011generation}).

The informal word list was composed of pronouns, short or simple words, phrasal verbs, words which express affection (for example, ``brother'' or ``buddy''), and generally other words judged to be of a subjective style (i.e. expressing emotion or opinion). Vague (more context-dependent or ``fuzzy'') expressions and slang language were also included (\cite{sheikha2011generation}). On the formal side, impersonal, objective or complex words were included. Business or technical vocabulary, words associated with politeness, context-independent or unambiguous phrases were all among the formal word lists (\cite{sheikha2011generation}).

When evaluating their results with human judgment, they found that the correct category (informal vs. formal) was chosen for their generated sentences to a very high accuracy, supporting the premise that lexicons can take one at least part of the way when it comes to formal language generation (\cite{sheikha2011generation}).

\subsection{Distribution of Formal Edits}

Given this discussion on what the major specific features which go into formal and informal language, it is also interesting to know how frequently each of these categories appear in translation. \cite{pavlick2016empirical} collected formal rewrites of sentences from online debate forums, then selected 100 of them to categorize all edits made.

\begin{table}[h]
\centering
 \begin{tabular}{||c | r | l | l ||} 
 \hline
 Category & \% & Informal Example & Formal Example \\ [0.3ex] 
 \hline\hline
 Capitalization & 50\% & walmart. & Walmart. \\ 
 \hline
 Punctuation & 39\% & more like a 30!!!!! & more like 30! \\
 \hline
 Paraphrase & 33\% & awesome & very nice \\
 \hline
 Delete fillers & 19\% & well it depends & It depends \\
 \hline
 Completion & 17\% & looks good & It looks good \\
 \hline
 Add context & 16\% & that guy & Osama Bin Laden \\
 \hline
 Contractions & 16\% & don't & do not \\
 \hline
 Spelling & 10\% & iwth & with \\
 \hline
 Normalization & 8\% & ur & your \\
 \hline
 Slang/idioms & 8\% & that's a big no. & I do not agree. \\
 \hline
 Politeness & 7\% & uh, more details? & more details, please? \\
 \hline
 Split sentences & 4\% & like high school & It's like high school \\
 \hline
 Relativizers & 3\% & sorry I'm not & Sorry that I am not \\ 
 \hline
\end{tabular}
\caption{The distribution of edits from 100 sentences rewritten for \cite{pavlick2016empirical}. Some edits fell into multiple categories.}
\label{pavlick2016table}
\end{table}

The results of this effort are shown in Table \ref{pavlick2016table}. Though this is specific to their domain of internet forum discussions, the resulting distribution still provides insight on what may actually make up formality.

\subsection{Specific Approaches}

Considering that, as discussed in \ref{2.1.1}, it is a clearer task to give relative (rather than absolute) labels to formal or informal sentences, \cite{pavlick2016empirical} developed a statistical model whose task was pairwise classification. Instead of labeling a sentence as absolutely informal or informal, the model would take two sentences and output which of the two is \textit{more} formal. Their features are notable in a survey of previous literature, as they state that to their knowledge five of their features were unused in previous approaches to the formal classification problem. These previously unused features were \textit{n-grams} (unigrams, bigrams and trigrams), \textit{word embeddings} (averaging pretrained word vectors over the sentence, skipping unknown words), \textit{named entities}, \textit{dependency features}, and \textit{parse tree features} (namely, the depth of the constituency parse tree, normalized by the length of the sentence). These were supplemented with the more familiar features of sentence length (and Flesch-Kincaid score), part of speech tags, repeated punctuation, capitalization features, lexical features (contractions, word length, word frequency) and subjectivity features (passive voice, personal pronouns, and use of varied lexicons). Using these features, they achieved an accuracy of 88\% with their model. However, they noted that this accuracy could degrade by anywhere from 5 to 23\% when tested on out-of-domain data (\cite{pavlick2016empirical}).

\section{Neural Machine Translation}

Neural machine translation (NMT) first appeared in the literature in 2014, offering a departure from statistical phrase-based translation systems; within a couple of years, most of the premier machine translation systems in the world had replaced their models with neural networks (\cite{sutskever2014sequence}). 

Previously, deep neural networks had been unable to tackle the problem of mapping one sequence to another, such as a sentence in one language to a translation in another. The LSTM encoder-decoder system introduced in \cite{sutskever2014sequence} could handle this task, even on longer sentences. Transformers, introduced by \cite{vaswani2018attention}, further improved on those results.

An NMT system consists of an encoder and a decoder. The encoder transforms the input sentence into a context vector of a fixed length, which the decoder then unrolls to generate an output sequence. Neural network machine translation systems are able to learn representations that are sensitive to syntactic structures and word order. As they are currently the standard, with new variants and improvements being constantly introduced in the literature, this section will review some main addendums, strategies and tools associated with NMT.

\subsection{Statistical Phrase-Based Machine Translation}

Before continuing with neural networks, however: as the previous standard, the statistical approaches (commonly referred to as Phrase-Based Machine Translation or PBMT) deserve a mention in the context of this work. In the beginning, phrase-based statistical methods were proven to be, unsurprisingly, a long step forward from mere single word-based experiments. Still, early research seemed to suggest that attempting to incorporate syntax in straightforward ways such as mapping constituents were actually to the detriment of models (\cite{koehn2003statistical}). Instead, \cite{koehn2003statistical} found their best performing models were those which included phrases of up to only three words.

A few years later, syntactic elements were introduced to PBMT with greater success in \cite{chiang2007hierarchical}. Hierarchical phrase-based translation could integrate ideas from phrase-based translation and syntax-based translation both; hierarchical phrases could encompass phrase ordering systems (for example, prepositional phrases modifying verb phrases by being ordered before them in some languages and after them in others). Not only that, but these could be learned from parallel text without any syntactic annotations needed (\cite{chiang2007hierarchical}).

To this day PBMT models are still known to outperform neural networks when there is a lack of labeled data, as while neural network models generalize better when data are abundant due to the large number of parameters, they are prone to overfit when the data are scarce (\cite{lample2018unsupervised}).

\subsection{Evaluation}

One of the most important decisions to make after training a machine translation system is naturally how to evaluate it in a coherent and consistent manner. Human evaluation of such a system might be ideal, but involve a lot of labor potentially spread out over a small group of people since workers who speak both languages fluently would need to be found. Data would need to be evaluated by multiple people --- the more the better, since, as discussed, interpretations of what formality is can vary among any group of people. Yet automatic evaluation presents its own problems, particularly if it is not language-independent and must be reformulated for each pair of languages.

The BLEU (Bilingual Evaluation Understudy) scoring system was developed as an alternative to human judgment that would be quick, language-independent, and correlate highly enough to humans that it would be usable as a substitute (\cite{papineni2002bleu}). By now, BLEU has become a standard for evaluation of machine translation systems, so that it is rare to not see it included in the evaluation section of current literature. BLEU essentially compares n-grams of a potential translation with those of a (or more than one) reference translation, and evaluates the target output thereon. 

Perplexity is also often included in evaluating machine translation models. Perplexity measures how well the probability distribution of a model can predict a sample (or many samples); a lower perplexity score indicates that the distribution is more likely to have predicted the sample.

\subsection{Corpora}

Of course, for both phrase-based approaches and for today's more common neural network systems, a parallel corpus, as large as possible, of texts is presumably necessary (or at the very least --- strongly preferred). Some language pairs are ``resource-rich'' or ``high-resource'', with large corpora available for pairs like English and German or English and French. For other ``resource-poor'' or ``low-resource'' language pairs, the problem of translation will often center around the building or acquisition of an appropriate corpus. Various strategies to avoid or deal with this problem have been proposed, and this section will cover the more relevant ones, as the task of formality translation is currently a low-resource problem and the question of data is addressed frequently over the course of this thesis.

\subsubsection{Backtranslation}

In many cases, for languages which lack a large bilingual and parallel corpus (sometimes referred to as \textit{bitext}), there exist at least some monolingual data. This disparity has often been taken advantage of: researchers use monolingual data to improve the performance of their translation models. One method for which monolingual data are used is backtranslation (BT). Methods similar to backtranslation, ways of producing synthetic data, come up throughout this work.

Backtranslation is a semi-supervised setup wherein bilingual data are supplemented with synthetic source data. These data are created by first training an intermediate system on the bitext, which then translates monolingual target text into the source language. This synthetic corpus comprising MT-output source data and human-written target data is then added to the original bitext to train the final system. It has been proven to improve performance in PBMT, NMT and unsupervised MT (\cite{edunov2018understanding}).

\cite{edunov2018understanding} takes on a large scale investigation of backtranslation, specifically in neural machine translation, by adding millions of backtranslated sentences to their bitext. The authors present different methods of selecting synthetic output, analyze results using each one, and demonstrate that the choice between these options does matter.

The most typical method of creating data through backtranslation uses greedy search or beam search to generate sentences. These are approximate algorithms to identify the maximum a posteriori (MAP) output of the model --- the sentences with the highest estimated probability given the selected input. Greedy search takes the most probable word at every step through the sentence; beam search expands in a restricted set and selects the most probable sentence over that set. In the case of any ambiguity, these would always favor the most likely option. Problematically, both beam and greedy search would therefore lead to less rich translations and a less diverse synthetic dataset overall (\cite{edunov2018understanding}).

Instead, the authors propose three additional methods of generating sentences from the intermediate model distribution. First, unrestricted sampling from the distribution, which would provide much more diverse output that may also be highly unlikely (referred to as the \textit{sampling} approach). Next, restricted sampling from the \textit{k} most likely tokens at each step (referred to as the \textit{top10} approach since \textit{k} = 10). And lastly, applying noise to beam search outputs, which comes in three steps: deleting random words at a probability of 0.1, replacing random words with a filler token at a probability of 0.1, and randomly swapping words no more than three positions apart (referred to as \textit{beam + noise}) (\cite{edunov2018understanding}).

Given these five methods of backtranslation, \cite{edunov2018understanding} then generated five supplementary synthetic datasets to the original bitext, in order to train six final translation models (including one on the unsupplemented data) to compare. All experiments were done with strong baseline models, and the original bitext data were already very large.

\begin{table}[h]
\centering
 \begin{tabular}{||c | r ||} 
 \hline
 Method & BLEU \\ [0.3ex] 
 \hline\hline
 bitext-only & 31.00 \\ 
 \hline
 beam & 31.78 \\ 
 \hline
 greedy & 31.96 \\ 
 \hline
 top10 & 32.94 \\ 
 \hline
 beam + noise & 33.43 \\ 
 \hline
 sampling & 33.51 \\
 \hline
\end{tabular}
\caption{The average results over 5 datasets for backtranslation methods, from \cite{edunov2018understanding}.}
\label{edunov2018results}
\end{table}

The average results from the experiments are shown in Table \ref{edunov2018results}. All models trained on the data supplemented with synthetic examples outperformed the bitext-only option, with the sampling and beam + noise methods eclipsing the typically used beam and greedy searches. \cite{edunov2018understanding} suggests that this is because these methods, which do not search for the MAP output, have a stronger training signal. Estimating MAP reduces diversity and richness in generated data, whereas adding noise to the synthetic source sentences has the effect of making it more difficult for the model to predict the target. Somewhat counterintuitively, this helps the model learn. Similarly, unrestricted sampling can better approximate the actual distribution of the data, which results in richer data than a pure argmax-focused method like the beam or greedy searches.

\begin{table}[h]
\centering
 \begin{tabular}{||c | r ||} 
 \hline
 Method & Perplexity \\ [0.3ex] 
 \hline\hline
 human & 75.34 \\ 
 \hline
 beam & 72.42 \\ 
 \hline
 sampling & 500.15 \\ 
 \hline
 top10 & 87.15 \\ 
 \hline
 beam + noise & 2823.73 \\ 
 \hline
\end{tabular}
\caption{Perplexity for backtranslation methods taken from trained language models, from \cite{edunov2018understanding}.}
\label{edunov2018perplex}
\end{table}

Perplexity scores, as shown in Table \ref{edunov2018perplex}, support this claim. The output of beam search has a lower perplexity score, and is thus more predictable, even than the human-written data. On the other hand, the data with the noise added are far less predictable. This noise allows a model trained on this data to avoid the pitfall of only confirming what the intermediate model had already learned, and instead it can generalize further and thus improve performance (\cite{edunov2018understanding}).

It should be reiterated that these experiments started with strong baselines that could produce good quality translations: what about in resource-poor settings, where the bitext is a small or poor quality dataset? \cite{edunov2018understanding} explore this question, as well, and find that adding backtranslated data still improves accuracy, but as opposed to the resource-rich setting, here beam search is more effective than sampling, likely because the additional noise is too difficult to work through for the already resource-poor model (\cite{edunov2018understanding}).

Another valuable piece of insight from this work is in relation to upsampling: often, when synthetic data are added to a genuine parallel dataset, upsampling is used to make sure the model ``sees'' the genuine data more often than the generated data. For example, an upsampling rate of 2 means the model visits the bitext at double the frequency than it visits the synthetic data. \cite{edunov2018understanding} argues that this is unnecessary with the sampling and beam + noise methods of backtranslation, as the synthetic data is already hard enough to fit to.

The final experiments done in this report compare the results of adding more human-written data to a corpus to supplementing with synthetic data. The authors demonstrate that the addition of synthetic data can sometimes match the accuracy gained when adding real bitext (\cite{edunov2018understanding}). Overall the work stands as a summary of possible methods to use when generating backtranslated data and when to use each one, as well as an assertion of the validity of backtranslation as a technique for data augmentation.

\subsubsection{Unsupervised Approaches with Monolingual Data}

For many, in fact, likely for the majority of language pairs in the world, there may exist no bitext at all (and this is not even considering research focused on intralingual translation, such as this thesis). One may have access to only a large, but monolingual, corpus in each of the languages. While it has become more and more common to use monolingual data in a semi-supervised manner to improve performance of machine translation models, \cite{lample2018unsupervised} take a different approach, developing entirely unsupervised models (both NMT and PBMT) which learn to translate without ever seeing original bitext.

The authors posit three essential characteristics of their models, which enable them to learn in an unsupervised manner. The first is a careful initialization of model parameters, which serves as a prior --- a way to establish from the start the space of reasonable solutions to the problem. This can be done using a bilingual dictionary, for example, or unsupervised inferred dictionaries. The argument is that this way some of the semantics are preserved regardless of how training proceeds (\cite{lample2018unsupervised}).

The second important task is to train language models on both source and target languages, using the large monolingual corpora. In effect, this is another sort of prior, though instead of establishing known semantic connections as the initialization does, the language models provide information about how model output should read in general, in either language (\cite{lample2018unsupervised}).

The last addressed principle is backtranslation, discussed here in the previous subsection. With this method, a problem requiring an unsupervised approach due to lack of data can be transformed into a problem that can be handled with a supervised model. That is, the source-to-target goal system is paired with a target-to-source translation system, which generates translated source data for the target output. These data can be used with the first model, and the coupled system allows the two models to improve one another with iteration (\cite{lample2018unsupervised}).

\cite{lample2018unsupervised} argue that their models using these principles, NMT and PBMT both, are simpler in structure and more accurate in results, and that they outperform state of the art measures in both low-resource and, impressively, even high-resource language pairs.

\subsubsection{Meta-learning}

Another way to handle the disparity in data availability among different language pairs is to not treat it as an obstacle, but leverage it as an asset. \cite{gu2018meta} redefine resource-poor language translation as a ``meta-learning'' problem, and learn from resource-rich language tasks to adapt to resource-poor language pairs. Previous approaches have trained NMT systems on a mix of language pairs, some of which have a large amount of data and some others, much less, in order to take advantage of common universal patterns learned from higher-resource pairs. This work extends that research by using a model-agnostic meta-learning algorithm called MAML (\cite{gu2018meta}).

Meta-learning, or ``learning to learn'', is an attempt to tackle the issue of fast adaptation to new training data. It can be done in a way that entails learning a meta-policy to update model parameters, or it can mean learning a parameter initialization that facilitates quick adaptation. MAML, proposed in \cite{finn2017agnostic}, is an algorithm which aims to generalize from a set of source tasks to an unseen target task which falls in the same category as the source tasks. It learns the parameters with which a newly initialized model would only need a small training set to perform well. Essentially MAML meta-learns those initialization parameters, from which a new model can learn the target task (\cite{finn2017agnostic}).

The application of MAML to neural machine translations means that the source tasks are translation between a variety of high-resource language pairs, and the target task is a low-resource language pair. The goal, then, is to learn an initialization of model parameters so that the target language pair model can learn well with a minimal amount of training examples. A challenge faced during this application is that the input and output space, as assumed by MAML, should be shared among all tasks. This is not naturally the case; the space of word embeddings, as a primary example, would typically be different for each language. To overcome this, \cite{gu2018meta} incorporate a Universal Lexical Representation (ULR): a universal embedding matrix that can dynamically build a vocabulary specific to each language. With this change, evaluation reveals that this method, called MetaNMT, is able to outperform state of the art translation systems in low-resource language pairs which use only up to 160k tokens, and the gap increases as the size of the training set decreases (\cite{gu2018meta}).

\subsection{Attention}

Attention mechanisms are increasingly used with NMT approaches. They were introduced to overcome the handicap inherent in the traditional encoder-decoder system in which the context vector representing the input sentence is always of a fixed length (\cite{bahdanau2015attention}). This characteristic makes handling longer sentences, which carry more information, far more difficult.

The encoder in an NMT sequence-to-sequence (seq2seq) model produces hidden states at every time step; however, only the final states are used for the decoder. Attention mechanisms instead have access to all hidden states of the encoder, and are thereby able to have a view of the source sentence at all steps. The main difference that this ability results in is in the computation of the context vector. Whereas the traditional system is restricted to a fixed-length context vector, a system with attention would encode the input sentence into a sequence of vectors, each of which is a concatenation of its hidden state and the context vector. From this sequence a subset can adaptively be chosen in the decoding phase. The model performs a type of soft search for relevant parts of the input sentence at every generation of a word for the output (\cite{bahdanau2015attention}).

This selective focus on sub-sentential segments can also allow models to learn a soft alignment, and aides in model introspection for these NLP tasks.

Expanding on attentional networks, \cite{vaswani2018attention} goes so far as to propose another type of model, called a transformer, that consists only of attention mechanisms, without the recurrence or convolution. The transformer follows the idea of the overall encoder-decoder architecture, but instead of the recurrent layers, both encoder and decoder are formed by stacks of identical layers, each of which have sub-layers. The encoder's sub-layers include one for the attention mechanism and a second for a simple fully-connected feed-forward network. The decoder has both of those and attentionally a sub-layer which performs attention over the output of the encoder stack. This method additionally has the advantage of allowing more parallelization than a traditional seq2seq model, meaning that training can be completed in less time (\cite{vaswani2018attention}). Transformers are now often considered state of the art in neural machine translation, above LSTM models.

\subsection{OpenNMT} \label{opennmt}

As neural machine translation gained traction and moved towards its current state of being the best-performing approach for translation tasks in most contexts, researchers began to make efforts to rend these systems accessible and easier to test and modify. One result of these efforts has been OpenNMT, an open source project for neural machine translation or sequence learning more generally (\cite{2017opennmt}). Its developers, in introducing it, cite the need for the machine translation community to have open source tools available so that researchers can share benchmarks, extend from others' implementations, and generally build upon a foundational shared framework (\cite{2017opennmt}).

The tool is available via PyTorch or tensorflow, and contains pretrained models as well as various parameters and additional functionalities documented on their website. Tokenization, support for either pretrained or simultaneous training of word embeddings, and various attention mechanisms are a few of the more relevant tools and options that OpenNMT provides (\cite{2017opennmt}).

\section{Sub-sentential Work}

The sequence-to-sequence models described in the previous section may be the state of the art for the task of translating a whole sentence to another, but for the purposes of this thesis it is worthwhile to also explore some research on the sub-sentential level. That is: examining language at the level of the word or phrase rather than the whole sentence. In this section I will discuss some recent experiments relevant to locating and translating specific smaller parts of sentences.

\subsection{Lexical Identification}

There is a lack of prominent literature that is primarily focused on the task of locating, specifically, phrases within a sentence. For words, however, Complex Word Identification exists as a subtask of Lexical Simplification. Still, the identification of words or phrases in sentences has been a subtask in more general problems, and methodologies have been developed that could be used to approach it.

\subsubsection{Complex Word Identification} \label{cwi}

Complex Word Identification refers to finding difficult words to be simplified inside of a sentence. Though the problem has garnered more attention in recent years thanks to shared tasks for conferences, there does not appear to be a standout state of the art model that is agreed on in the field. Ensemble methods have been shown to outperform other algorithms, including neural approaches, but a lack of annotation standards coupled with inconsistent human labeling present a difficulty for those attempting to tackle the problem (\cite{dehertog2018deep}). Engineered features of all kinds are prominent: morphological, syntactic, semantic, psycholinguistic, frequency-based, lexicon-based, readability-based, contextual, and so on. Occasionally they are combined with word and/or character embeddings (\cite{aroyehun2018complex}).

\cite{dehertog2018deep} preselect words to be the focus of the input sentence, making the problem one of binary classification (is this particular word, in this particular sentence, complex?). Experimenting with a model that takes a hybrid of feature engineering and embeddings as input, they note that contextual information does not seem to improve performance, and that complexity is ``best determined by including focused information of the target word itself'' (\cite{dehertog2018deep}).

\cite{aroyehun2018complex} used the same dataset and problem setup (binary classification) as \cite{dehertog2018deep}, but experiment with different approaches. They compare two approaches: one with extensive feature engineering and a tree ensemble classifier for the model, and another taking word embeddings and a deep convolutional neural network (CNN). While the two approaches performed similarly on the data in English, when applied to Spanish the CNN outperformed the tree ensemble classifier, suggesting that the developed architecture contained a flexibility to be used with other languages that could not be said about a feature engineering approach.

\subsubsection{Named Entity Recognition}

One of the more obvious tasks that requires lexical identification is Named Entity Recognition (NER), in which named entities (people, locations, organizations, and so on) are located or extracted from a text. \cite{li2018ner} give a comprehensive survey of the literature in the field so far. Important takeaways are that a hybrid of word and character embeddings has been used in the best-performing models so far, and that one of the more common ways to format input and output is binary (or more) classification on the word level. That is, for each word, a model decides whether that word is a named entity or not (\cite{li2018ner}). These models have found success and can be used as references when approaching similar tasks of extracting certain kinds of words or phrases from sentences.

\subsubsection{Pointer Networks} \label{pointer}

The traditional binary classification approach used for NER approaches has drawbacks: a class imbalance might create difficulty for models in training, for example. Another option could be a neural architecture developed in \cite{vinyals2015pointer} called the Pointer Net.

Similar to the proposal of attention networks, the Pointer Net came from another limitation of recurrent neural networks (RNNs) more generally. Specifically, they require the size of the output dictionary to be fixed. This means that the algorithm cannot handle approaches in which the size of the output dictionary must depend on the length of its input (which would apply if you wanted to ``point'' to an element in your input sequence) (\cite{vinyals2015pointer}).

Pointer Nets use attention mechanisms to create pointers to a location in their input, solving the problem of representing variable length dictionaries in a model's output. It does this by using a recurrent neural network to encode an input sequence into a context vector, as in an NMT encoder-decoder system. However, instead of passing this context vector to a decoder as the next step, the Pointer Net uses softmax over the context vector to produce a distribution over the dictionary of inputs. This relatively simple change of the attention mechanism turns the context vector into a series of pointers to the input elements (\cite{vinyals2015pointer}).

This architecture is meant for tasks whose outputs are discrete and must be pointers to a location in the input; it forces a constraint upon the output that the target must map back to the source sequence. Although the paper which introduces the Pointer Net (\cite{vinyals2015pointer}) uses geometrical problems to test its architecture and not natural language processing (NLP) tasks, its integration with sequence-to-sequence models makes it an easy possible complement to NMT systems. In fact some of the literature has already benefited from the uses of Pointer Nets. The aforementioned survey of NER models, \cite{li2018ner}, also refers to the use of Pointer Nets for their purposes. \cite{ma2018stack} describes a new neural architecture for dependency parsing making heavy use of Pointer Nets, providing further support for this architecture as a useful tool for NLP tasks.

\subsection{Lexical Replacement}

Given, then, a word, and perhaps also the sentence it belongs in, another task would be to replace it with a different word without compromising the sentence. A known problem of this description is Lexical Simplification, a task which would have many applications if solved: replacing a difficult word with a simpler equivalent. Lately neural networks have been utilized in approaching this topic.

\subsubsection{Lexical Simplification}

Generally, the pipeline for Lexical Simplification is already well-established. It starts with Complex Word Identification, as discussed in \ref{cwi}, to choose the words to replace, is followed by Substitution Generation to produce options for replacement words, moves to Substitution Selection to extract the suitable choices from this list, and ends with Substitution Ranking to order them from most to least appropriate for the given input (\cite{paetzold2017lexical}).

\cite{melamud2015simple} offer a word embedding model developed specifically for the this task, but potentially applicable to a wide range of other NLP problems. Their work operates under the assumption that a substitute for a word is good if it is both similar to the word it replaces in semantics, and also fits in the context where the word was originally found. Based off of the commonly used skip-gram word embeddings (which are behind, for example, word2vec embeddings), their proposed model differs from the original in that it is context-aware. Within the skip-gram model, embeddings representing the context of an input word are already generated --- but are considered only as an internal aspect of the model and its training. \cite{melamud2015simple} use these learned context embeddings externally, in addition with the original word embeddings, to model the instance of the word inclusive of its context. Then, a possible substitute word is identified not only through the similarities of the source and target word vectors, but also through the similarities of their contexts (\cite{melamud2015simple}).

\cite{paetzold2017lexical} used a similar context-aware model for word embeddings in their work on neural substitution ranking. After extracting a dataset of complex to simple substitutions by aligning and filtering through aligned pairs in the Newsela dataset of text difficulty, and going through the initial steps of Complex Word Identification, Substitution Generation and Substitution Selection in a standard manner, they focus on a new model for Substitution Ranking. Their approach consits first of pairwise regression, a model that takes two words and evaluates how much simpler the first is than the second, an ordering of the words based on those scores, and a confidence check using a language model to ensure that simplest candidate is likely to appear in the context of the word it would replace. Notably, their method tends to make significantly fewer errors in grammatically or meaning than state of the art simplifiers (\cite{paetzold2017lexical}).

\section{Specific Approaches of Interest}

While the previous sections have focused on theoretical concepts and commonly used strategies for various topics, here I will discuss more specific experiments which have particular relevance to or have influenced the work for this thesis, and which connect to multiple sections described in this chapter. Overall, there have not been a large number of attempts to generate formal from informal or informal from formal language. But two approaches, which are also connected to one another, need to be addressed.

\subsection{Copy Attention}

\cite{jhamtani2017copy} introduces a novel problem: translating modern English to Shakespearean English. Variations in writing styles like this one had previously been met with attempts using language models, phrase tables and lexicons, but these approaches had only a limited scope. The question of automating these stylistic changes is still in large part unanswered. Though this would perhaps at first thought better fit into the category of style transfer, rather than machine translation, the authors here made some modifications to a traditional seq2seq NMT model in order to achieve results matching the state of the art (\cite{jhamtani2017copy}). 

\cite{jhamtani2017copy} posit that sequence-to-sequence neural machine translation is suited for their task as it provides a mechanism to share word representation information between source and target through word embeddings, and to include constraints between words into their representations. Though the difference in Shakespearean and modern English is stylistic, it is also more than that: the two ``languages'' have large differences in vocabulary as well as some differing grammatical constructions and semantic senses of the same words. The limited parallel data also meant a difference in domain --- one as a dramatic play, and the other as material for high school classes (\cite{jhamtani2017copy}).

One way to mitigate this lack of a large dataset was to pretrain the word embeddings, making use of already-existing dictionaries which translate Shakespearean words to modern English, as well as other texts in Shakespearean English. This serves as an initialization of model parameters, as described in \cite{lample2018unsupervised}'s work on low-resource language translation. Despite all the discussed differences, though, there would be plenty of words and phrases which would be the same in both versions of the same language. To facilitate these similarities, pointer networks (see \ref{pointer}) were added to the traditional model, a bidirectional LSTM encoder-decoder with attention. One notable aspect of this architecture was that addition was used rather than concatenation to combine the forward and backward encoder states output by the bidirectional LSTM; this avoids the addition of extraneous parameters, which is important to avoid overfitting for this resource-poor language pair. In this case, the decoder is a mix of an RNN and a pointer network, which share the attention weights produced by the encoder. The pointer network, in predicting a probability distribution over the words in the input, can enable copying of input to output, signaling where the words should remain the same (\cite{jhamtani2017copy}).

\subsection{Grammarly's Yahoo Answers Formality Corpus} \label{gyafc}

Up until recently, the previously mentioned Shakespeare parallel dataset was the only available dataset related to the formality dimension of style. As discussed, this dataset presented its own problems, and the lack of anything else was a clear obstacle in attempts to generate formal from informal text, or the other way around. To present a new dataset and encourage the field of formality translation to be further explored, \cite{rao2018gyafc} presented Grammarly's Yahoo Answers Formality Corpus (GYAFC).

The GYAFC includes approximately 110,000 parallel formal and informal sentence pairs, collected at the informal level from Yahoo! Answers in two categories: Entertainment \& Music and Family \& Relationships. The sentences were then translated using Amazon's Mechanical Turk to a formal style. The dataset as a whole is split up into train, tune and test subsets, the latter two of which were especially focused on to ensure that they be of high quality (\cite{rao2018gyafc}).

Beyond only introducing the corpus, \cite{rao2018gyafc} also provide benchmark models and results for future experiments to compare to. In following a recent movement to approach style transfer problems like this one with sequence-to-sequence NMT methods, they develop a series of models to evaluate. These approaches focus specifically on the informal to formal direction, although some experimentation in the opposite direction is described as an addendum. The main types of models focused on in this work are a rule-based approach, PBMT and NMT. Additional changes are made to the standard machine translation models, as influenced by work in low resource MT. Notably, all NMT models are trained using OpenNMT (discussed in \ref{opennmt}), and one of those takes advantage of the tool's functionality to easily add copy attention, as described by \cite{jhamtani2017copy}, introduced earlier in this section. Finally, various methods of data augmentation are carried out, most prominently upsampling and backtranslation (\cite{rao2018gyafc}).

In order to draw a conclusion as to which of these models performs the best, naturally some standard metrics for evaluation are necessary. \cite{rao2018gyafc} focus on three main criteria: formality, fluency and meaning preservation, as well as overall rankings. They enlist help from Amazon's Mechanical Turk in order to have human evaluation to compare to automatic metrics, in order to also evaluate how appropriate the automatic metrics are for this task. For formality, fluency and meaning preservation, they reimplement classification models put forward in other reports, and for the overall ranking they refer to BLEU scores, among others. With this experimentation, they found that human judgments did not correspond very well to the automatic metrics, and that humans had an overall preference for the results produced by PBMT approaches. They note that two of the neural approaches often made larger changes that other models were not capable of, but that these changes often came with a change in meaning of the sentence (\cite{rao2018gyafc}).

\section{Acrolinx} \label{acrolinx}

Since the use case of this thesis, and the practical choices made thereof, were developed with Acrolinx in mind, it is necessary to provide a general overview of the capabilities of the software as it stands and where it is relevant to this report. Acrolinx provides guidance for written content (marketing texts, business documents, technical guides, and so on) in a variety of different areas. Specifically, I will describe here what measures Acrolinx uses to investigate formality.

\subsection{Readability}

Using a series of linguistic rules, Acrolinx makes a variety of recommendations to a writer. Readability features which are monitored include paragraph and sentence length (Acrolinx notifies the writer when they are too long), sentence length variation (such that not all sentences in a paragraph are of the same length), and certain occurrences of punctuation (semi-colons and comma splices).

\subsection{Grammar and Syntax}

Part of speech tags play a large role in Acrolinx's recommendations for conversational tone: it checks to ensure the number of adjectives in a text is not too high, and that the amount of pronouns is not too low. Acrolinx also can recommend to omit complementizers (``that''), to use more contractions, to simplify complex noun phrases, to avoid nominalization, and to avoid ``it is [...] to'' constructions (for example, ``it is recommended that...'').

Acrolinx also keeps track of the F-score by Heylighen and Dewaele, as described in \ref{fscore}.

\subsection{Repetitive Structures}

Occurrences in which there are repetitions of a similar sentence structure within a paragraph are flagged for change. The list of structures which can trigger this flag include: the same modal verb (``can'', ``could'', ``may'', ``might'', ``shall'', ``should'') followed by a verb at the start of multiple sentences, starting too many sentences with the same personal pronoun, starting too many sentences with ``this'', ``that'', ``if'', ``when'', a gerund, an adverb, ``in order to'', ``there is'' or ``there are'', or a generic introductory phrase.

\subsection{Lexicons}

In addition to the rules, there are various gazetteers which are used to flag or make exceptions for certain words. Archaic words, Latin expressions, cliches, and slang terms are all included lexicons, from which Acrolinx draws attention to words that it recommends not to use.

\subsection{Formal Words} \label{lf}

Formal words are flagged using a layered system of rules and lexicons. The previously mentioned gazetteers, as well as other lists of explicitly declared formal words, label words as formal. Additionally, most words have their own ``Lexical Formality'' score, which is used to calculate an overall score for each paragraph. Should that score exceed a defined threshold, then the most formal words in the paragraph as determined by the Lexical Formality score are flagged (\cite{acro20147lf}).

These Lexical Formality scores for individual words were pre-calculated and stored, but originally they were generated using a word2vec model (\cite{mikolov2013word2vec}). The model in question was trained on approximately 960 million tokens, to a 400-dimensional representation with a window size of 15 (\cite{acro20147lf}).

After this model was trained, a ``formality vector'' was generated, using a seed set of 39 formal words and informal substitutes. In taking the difference vectors between each pair and then averaging them over the set, the formality dimension would theoretically be isolated in this resulting vector. From that point, any word can be compared to the formality vector with a score, simply by taking the dot product of its own embedding vector and the formality vector. This way, informal words would be expected to have a negative dot product/score, and formal words would have positive scores. Neutral words would hover around zero (\cite{acro20147lf}).

\begin{table}[h]
\centering
 \begin{tabular}{|| c | c ||} 
 \hline
 Most Informal & Most Formal\\ [0.3ex] 
 \hline\hline
 really & governing \\ 
 \hline
 anyways & constituted \\ 
 \hline
 anyway & facilitated \\ 
 \hline
 hey & provisions \\ 
 \hline
 get & provision \\ 
 \hline
 like & consequent \\ 
 \hline
 just & hitherto \\ 
 \hline
 heck & whereby \\ 
 \hline
 you & undertaken \\ 
 \hline
 haha & facilitate \\ 
 \hline
\end{tabular}
\caption{Most informal and most formal words, according to the Acrolinx Lexical Formality scores (\cite{acro20147lf}). ``governing'' has a score of 0.3726, and ``really'' finishes at -0.486.}
\label{acrolinxlf}
\end{table}

As can be seen in Table \ref{acrolinxlf}, according to this metric, the most informal and most formal words appear to be accurately labeled. Interestingly, the most neutral words according to this same metric (that is, the ones with a score closest to zero) are ``venomous'', ``czarina'' and ``gastroesophageal'' (\cite{acro20147lf}).